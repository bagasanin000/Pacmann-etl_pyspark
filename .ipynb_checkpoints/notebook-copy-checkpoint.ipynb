{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f9b6cd-7c63-4d7c-a963-25952b0ca934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eace4aee-ea80-400b-af40-bbf677b5cb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check pyspark version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027ca5ff-3f8d-4fc2-b52a-7da84338ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1d6a71-4fac-4f99-9ea4-f2f7e2d73fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Final Project PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4d9054-9107-42d6-a0a0-e049b9319e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Final Project PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f478b9d10d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add16a43-963c-420d-8b5e-e8c18d934667",
   "metadata": {},
   "source": [
    "## Load and Handle Failure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d94b859-9d4d-483a-b6af-5dfcdf168376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54467c7-078d-466a-91d8-57af1aac710c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d67139-17b1-4fdc-aaa1-a5f9a5fc15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sqlalchemy import create_engine, Table, MetaData\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab0c850-780c-4a8f-ab6d-a4cc089f31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Connection to db logger\n",
    "\n",
    "DB_LOGGER_URL = os.getenv(\"DB_LOGGER_URL\")\n",
    "DB_LOGGER_USER = os.getenv(\"DB_LOGGER_USER\")\n",
    "DB_LOGGER_PASS = os.getenv(\"DB_LOGGER_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4906f7-e369-4abe-9160-6c2434100fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Table, MetaData\n",
    "from datetime import datetime\n",
    "\n",
    "def log_to_db(log_msg: dict):\n",
    "    try:\n",
    "        # Setup connection to PostgreSQL\n",
    "        engine = create_engine(f\"postgresql://{DB_LOGGER_USER}:{DB_LOGGER_PASS}@host.docker.internal:5432/pyspark_task_logger\")\n",
    "        metadata = MetaData()\n",
    "        metadata.reflect(bind=engine)\n",
    "\n",
    "        # Take table object\n",
    "        log_table = metadata.tables.get(\"etl_logs\")\n",
    "        if log_table is None:\n",
    "            raise Exception(\"Table 'etl_logs' not found in database\")\n",
    "\n",
    "        # Add timestamp if not exists\n",
    "        if \"etl_date\" not in log_msg:\n",
    "            log_msg[\"etl_date\"] = datetime.now()\n",
    "\n",
    "        # Use transaction (auto commit)\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(log_table.insert().values(**log_msg))\n",
    "\n",
    "        print(\"Log successfully written to database\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing log to database: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e50797d5-9c81-4adc-ba0f-adaddd265451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740,)\n"
     ]
    }
   ],
   "source": [
    "# test connection\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(f\"postgresql://{DB_LOGGER_USER}:{DB_LOGGER_PASS}@host.docker.internal:5432/pyspark_task_logger\")\n",
    "conn = engine.connect()\n",
    "\n",
    "result = conn.execute(text(\"SELECT COUNT(*) FROM etl_logs\"))\n",
    "print(result.fetchone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64f33f6d-db9f-4587-8194-42325005f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def save_invalid_ids(invalid_ids, table_name):\n",
    "    if not invalid_ids:\n",
    "        print(f\"No invalid IDs to save from table '{table_name}'.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Normalize jumlah kolom menjadi maksimal 6 kolom (3 pasang)\n",
    "        max_cols = 6  # 3 pasang: entity/object\n",
    "        if len(invalid_ids[0]) > max_cols:\n",
    "            raise ValueError(\"Too many columns in invalid_ids input\")\n",
    "\n",
    "        # Buat DataFrame dengan nama kolom fleksibel\n",
    "        base_columns = [\n",
    "            \"entity_type\", \"object_id\",\n",
    "            \"extra_entity_type_1\", \"extra_object_id_1\",\n",
    "        ]\n",
    "        df = pd.DataFrame(invalid_ids, columns=base_columns[:len(invalid_ids[0])])\n",
    "        df[\"table_name\"] = table_name\n",
    "        df[\"logged_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Save ke DB\n",
    "        engine = create_engine(f\"postgresql://{DB_LOGGER_USER}:{DB_LOGGER_PASS}@host.docker.internal:5432/pyspark_task_logger\")\n",
    "        with engine.begin() as conn:\n",
    "            df.to_sql(\"invalid_ids\", con=conn, index=False, if_exists=\"append\")\n",
    "            print(f\"{len(df)} invalid IDs from table '{table_name}' saved to logger DB.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving invalid IDs to logger DB: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091d0a6-b0fc-4a07-96a0-5ea4288035fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8994a6-a66d-4dd2-87a7-1e29133144f9",
   "metadata": {},
   "source": [
    "### CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124d8ff-228b-4ecf-b8b2-9f7adb03a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(file_path, table_name):\n",
    "    try:\n",
    "        # Read CSV using Spark\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "        # Show extracted data\n",
    "        df.show()\n",
    "\n",
    "        # Log success\n",
    "        log_to_db({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"CSV\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_to_db({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"CSV\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        print(f\"Error extracting {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6396ca-a091-4d35-aa82-21826baf95e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|people_id|object_id|first_name| last_name|          birthplace|    affiliation_name|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|        1|      p:2|       Ben|   Elowitz|                NULL|           Blue Nile|\n",
      "|        2|      p:3|     Kevin|  Flaherty|                NULL|            Wetpaint|\n",
      "|        3|      p:4|      Raju|   Vegesna|                NULL|                Zoho|\n",
      "|        4|      p:5|       Ian|     Wenig|                NULL|                Zoho|\n",
      "|        5|      p:6|     Kevin|      Rose|         Redding, CA|        i/o Ventures|\n",
      "|        6|      p:7|       Jay|   Adelson|         Detroit, MI|                Digg|\n",
      "|        7|      p:8|      Owen|     Byrne|                NULL|                Digg|\n",
      "|        8|      p:9|       Ron|Gorodetzky|                NULL|                Digg|\n",
      "|        9|     p:10|      Mark|Zuckerberg|                NULL|            Facebook|\n",
      "|       10|     p:11|    Dustin| Moskovitz|     Gainesville, FL|            Facebook|\n",
      "|       11|     p:12|      Owen| Van Natta|                NULL|               Asana|\n",
      "|       12|     p:13|      Matt|    Cohler|                NULL|            LinkedIn|\n",
      "|       13|     p:14|     Chris|    Hughes|Hickery, North Ca...|General Catalyst ...|\n",
      "|       14|     p:16|      Alex|     Welch|                NULL|            C7 Group|\n",
      "|       15|     p:17|    Darren|   Crystal|                NULL|         Photobucket|\n",
      "|       16|     p:18|   Michael|     Clark|                NULL|   Photobucket (Old)|\n",
      "|       17|     p:19|      Greg|    Wimmer|                NULL|         Photobucket|\n",
      "|       18|     p:20|     Peter|    Foster|                NULL|         Photobucket|\n",
      "|       19|     p:21|   Heather|      Dana|                NULL|         Photobucket|\n",
      "|       20|     p:22|     Peter|      Pham|                NULL|   Photobucket, Inc.|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|people_id|object_id|first_name| last_name|          birthplace|    affiliation_name|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|        1|      p:2|       Ben|   Elowitz|                NULL|           Blue Nile|\n",
      "|        2|      p:3|     Kevin|  Flaherty|                NULL|            Wetpaint|\n",
      "|        3|      p:4|      Raju|   Vegesna|                NULL|                Zoho|\n",
      "|        4|      p:5|       Ian|     Wenig|                NULL|                Zoho|\n",
      "|        5|      p:6|     Kevin|      Rose|         Redding, CA|        i/o Ventures|\n",
      "|        6|      p:7|       Jay|   Adelson|         Detroit, MI|                Digg|\n",
      "|        7|      p:8|      Owen|     Byrne|                NULL|                Digg|\n",
      "|        8|      p:9|       Ron|Gorodetzky|                NULL|                Digg|\n",
      "|        9|     p:10|      Mark|Zuckerberg|                NULL|            Facebook|\n",
      "|       10|     p:11|    Dustin| Moskovitz|     Gainesville, FL|            Facebook|\n",
      "|       11|     p:12|      Owen| Van Natta|                NULL|               Asana|\n",
      "|       12|     p:13|      Matt|    Cohler|                NULL|            LinkedIn|\n",
      "|       13|     p:14|     Chris|    Hughes|Hickery, North Ca...|General Catalyst ...|\n",
      "|       14|     p:16|      Alex|     Welch|                NULL|            C7 Group|\n",
      "|       15|     p:17|    Darren|   Crystal|                NULL|         Photobucket|\n",
      "|       16|     p:18|   Michael|     Clark|                NULL|   Photobucket (Old)|\n",
      "|       17|     p:19|      Greg|    Wimmer|                NULL|         Photobucket|\n",
      "|       18|     p:20|     Peter|    Foster|                NULL|         Photobucket|\n",
      "|       19|     p:21|   Heather|      Dana|                NULL|         Photobucket|\n",
      "|       20|     p:22|     Peter|      Pham|                NULL|   Photobucket, Inc.|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract from CSV\n",
    "\n",
    "df_people = extract_csv(\"data/people.csv\", \"people_data\")\n",
    "df_people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "683f3aa9-4a83-429a-96b3-9f6fe53d47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|            start_at|              end_at|is_past|sequence|               title|          created_at|          updated_at|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|              1|             p:2|                   c:1|                NULL|                NULL|  false|       8|Co-Founder/CEO/Bo...|2007-05-25 07:03:...|2013-06-03 09:58:...|\n",
      "|              2|             p:3|                   c:1|                NULL|                NULL|  false|  279242|        VP Marketing|2007-05-25 07:04:...|2010-05-21 16:31:...|\n",
      "|              3|             p:4|                   c:3|                NULL|                NULL|  false|       4|          Evangelist|2007-05-25 19:33:...|2013-06-29 13:36:...|\n",
      "|              4|             p:5|                   c:3|2006-03-01 00:00:...|2009-12-01 00:00:...|  false|       4|Senior Director S...|2007-05-25 19:34:...|2013-06-29 10:25:...|\n",
      "|              6|             p:7|                   c:4|2005-07-01 00:00:...|2010-04-05 00:00:...|  false|       1|Chief Executive O...|2007-05-25 20:05:...|2010-04-05 18:41:...|\n",
      "|              7|             p:8|                   c:4|                NULL|                NULL|  false|       1|Senior Software E...|2007-05-25 20:06:...|2010-01-12 01:13:...|\n",
      "|              8|             p:9|                   c:4|                NULL|                NULL|  false|       1|Systems Engineeri...|2007-05-25 20:07:...|2010-08-03 22:00:...|\n",
      "|              9|            p:10|                   c:5|                NULL|                NULL|  false|       1|Founder and CEO, ...|2007-05-25 21:51:...|2010-01-25 21:49:...|\n",
      "|             10|            p:11|                   c:5|                NULL|                NULL|  false|       1|          Co-Founder|2007-05-25 22:15:...|2011-08-11 23:48:...|\n",
      "|             11|            p:12|                   c:5|                NULL|                NULL|  false|       3|Chief Revenue Off...|2007-05-25 22:15:...|2010-01-25 21:49:...|\n",
      "|             12|            p:13|                   c:5|                NULL|                NULL|  false|       1|VP of Product Man...|2007-05-25 22:16:...|2010-01-25 21:49:...|\n",
      "|             13|            p:14|                   c:5|1993-07-04 00:00:...|1982-02-28 00:00:...|  false|       1|          Co-founder|2007-05-25 22:17:...|2010-09-01 18:15:...|\n",
      "|             14|            p:16|                c:7299|                NULL|                NULL|  false|       2|     Founder and CEO|2007-05-26 12:44:...|2011-07-19 00:05:...|\n",
      "|             15|            p:17|                c:7299|                NULL|                NULL|  false|       2|  Co-founder and CTO|2007-05-26 12:45:...|2011-07-19 00:05:...|\n",
      "|             16|            p:18|                c:7299|2005-09-01 00:00:...|2009-10-01 00:00:...|  false|       1|SVP, Technology a...|2007-05-26 12:47:...|2011-07-19 00:05:...|\n",
      "|             17|            p:19|                c:7299|2006-08-01 00:00:...|2009-08-01 00:00:...|  false|       8|VP, Finance & Adm...|2007-05-26 12:47:...|2011-10-28 03:08:...|\n",
      "|             18|            p:20|                c:7299|                NULL|                NULL|  false|       1|           VP, Sales|2007-05-26 12:48:...|2011-07-19 00:05:...|\n",
      "|             19|            p:21|                c:7299|                NULL|                NULL|  false|       1|VP, Customer Service|2007-05-26 12:49:...|2011-07-19 00:05:...|\n",
      "|             20|            p:22|                c:7299|2005-10-01 00:00:...|2008-03-01 00:00:...|  false|       1|VP, Business Deve...|2007-05-26 12:49:...|2013-08-07 08:51:...|\n",
      "|             21|            p:23|                c:7299|                NULL|                NULL|  false|       1|     VP, Engineering|2007-05-26 12:50:...|2011-07-19 00:05:...|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|            start_at|              end_at|is_past|sequence|               title|          created_at|          updated_at|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|              1|             p:2|                   c:1|                NULL|                NULL|  false|       8|Co-Founder/CEO/Bo...|2007-05-25 07:03:...|2013-06-03 09:58:...|\n",
      "|              2|             p:3|                   c:1|                NULL|                NULL|  false|  279242|        VP Marketing|2007-05-25 07:04:...|2010-05-21 16:31:...|\n",
      "|              3|             p:4|                   c:3|                NULL|                NULL|  false|       4|          Evangelist|2007-05-25 19:33:...|2013-06-29 13:36:...|\n",
      "|              4|             p:5|                   c:3|2006-03-01 00:00:...|2009-12-01 00:00:...|  false|       4|Senior Director S...|2007-05-25 19:34:...|2013-06-29 10:25:...|\n",
      "|              6|             p:7|                   c:4|2005-07-01 00:00:...|2010-04-05 00:00:...|  false|       1|Chief Executive O...|2007-05-25 20:05:...|2010-04-05 18:41:...|\n",
      "|              7|             p:8|                   c:4|                NULL|                NULL|  false|       1|Senior Software E...|2007-05-25 20:06:...|2010-01-12 01:13:...|\n",
      "|              8|             p:9|                   c:4|                NULL|                NULL|  false|       1|Systems Engineeri...|2007-05-25 20:07:...|2010-08-03 22:00:...|\n",
      "|              9|            p:10|                   c:5|                NULL|                NULL|  false|       1|Founder and CEO, ...|2007-05-25 21:51:...|2010-01-25 21:49:...|\n",
      "|             10|            p:11|                   c:5|                NULL|                NULL|  false|       1|          Co-Founder|2007-05-25 22:15:...|2011-08-11 23:48:...|\n",
      "|             11|            p:12|                   c:5|                NULL|                NULL|  false|       3|Chief Revenue Off...|2007-05-25 22:15:...|2010-01-25 21:49:...|\n",
      "|             12|            p:13|                   c:5|                NULL|                NULL|  false|       1|VP of Product Man...|2007-05-25 22:16:...|2010-01-25 21:49:...|\n",
      "|             13|            p:14|                   c:5|1993-07-04 00:00:...|1982-02-28 00:00:...|  false|       1|          Co-founder|2007-05-25 22:17:...|2010-09-01 18:15:...|\n",
      "|             14|            p:16|                c:7299|                NULL|                NULL|  false|       2|     Founder and CEO|2007-05-26 12:44:...|2011-07-19 00:05:...|\n",
      "|             15|            p:17|                c:7299|                NULL|                NULL|  false|       2|  Co-founder and CTO|2007-05-26 12:45:...|2011-07-19 00:05:...|\n",
      "|             16|            p:18|                c:7299|2005-09-01 00:00:...|2009-10-01 00:00:...|  false|       1|SVP, Technology a...|2007-05-26 12:47:...|2011-07-19 00:05:...|\n",
      "|             17|            p:19|                c:7299|2006-08-01 00:00:...|2009-08-01 00:00:...|  false|       8|VP, Finance & Adm...|2007-05-26 12:47:...|2011-10-28 03:08:...|\n",
      "|             18|            p:20|                c:7299|                NULL|                NULL|  false|       1|           VP, Sales|2007-05-26 12:48:...|2011-07-19 00:05:...|\n",
      "|             19|            p:21|                c:7299|                NULL|                NULL|  false|       1|VP, Customer Service|2007-05-26 12:49:...|2011-07-19 00:05:...|\n",
      "|             20|            p:22|                c:7299|2005-10-01 00:00:...|2008-03-01 00:00:...|  false|       1|VP, Business Deve...|2007-05-26 12:49:...|2013-08-07 08:51:...|\n",
      "|             21|            p:23|                c:7299|                NULL|                NULL|  false|       1|     VP, Engineering|2007-05-26 12:50:...|2011-07-19 00:05:...|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract from CSV\n",
    "\n",
    "df_relations = extract_csv(\"data/relationships.csv\", \"relationships_data\")\n",
    "df_relations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73f9ea-4216-46a4-91e4-cd4bdb943e39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58919bd4-a86e-46ed-a849-bf236d617ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variable for Database (data source)\n",
    "\n",
    "DB_URL = os.getenv(\"DB_URL\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c35a1-6309-440c-a1d1-6427bfe1eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_db():\n",
    "    try:\n",
    "        # Get list of tables from the database\n",
    "        table_list = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DB_URL) \\\n",
    "            .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "            .option(\"user\", DB_USER) \\\n",
    "            .option(\"password\", DB_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .filter(\"table_schema = 'public'\") \\\n",
    "            .select(\"table_name\") \\\n",
    "            .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print(f\"Found tables: {table_list}\")\n",
    "\n",
    "        tables = {}\n",
    "        for table in table_list:\n",
    "            try:\n",
    "                # Read each table into a DataFrame\n",
    "                df = spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", DB_URL) \\\n",
    "                    .option(\"dbtable\", table) \\\n",
    "                    .option(\"user\", DB_USER) \\\n",
    "                    .option(\"password\", DB_PASS) \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()\n",
    "\n",
    "                tables[table] = df\n",
    "\n",
    "                # Log success for each table\n",
    "                log_to_db({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": \"Success\",\n",
    "                    \"source\": \"PostgreSQL\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "\n",
    "                print(f\"Successfully extracted table: {table}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log failure for specific table\n",
    "                log_to_db({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": f\"Failed: {e}\",\n",
    "                    \"source\": \"PostgreSQL\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "                print(f\"Failed to extract table: {table} - Error: {e}\")\n",
    "\n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure for the whole extraction process\n",
    "        log_to_db({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"PostgreSQL\",\n",
    "            \"table_name\": \"N/A\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        print(f\"Failed to extract tables: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7557a91-4ad6-4d9e-ba84-3482d3b77027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables: ['company', 'acquisition', 'funding_rounds', 'funds', 'investments', 'ipos']\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: company\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: acquisition\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funding_rounds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: investments\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: ipos\n",
      "Extracted tables: ['company', 'acquisition', 'funding_rounds', 'funds', 'investments', 'ipos']\n"
     ]
    }
   ],
   "source": [
    "# Extract from db\n",
    "\n",
    "tables = extract_from_db()\n",
    "print(f\"Extracted tables: {list(tables.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fe3be2a-e7cf-4d7d-978d-b9135c7bf937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|office_id|object_id|      description|              region|            address1|  address2|          city|  zip_code|state_code|country_code| latitude|  longitude|         created_at|         updated_at|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|        8|      c:8|                 |              SF Bay|959 Skyway Road, ...|          |    San Carlos|     94070|        CA|         USA|37.506885|-122.247573|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|        9|      c:9|     Headquarters|         Los Angeles|9229 W. Sunset Blvd.|          |West Hollywood|     90069|        CA|         USA|34.090368|-118.393064|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       10|     c:10|                 |         Los Angeles|8536 National Blv...|          |   Culver City|     90232|        CA|         USA|34.025958|-118.379768|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       11|     c:11|                 |         Los Angeles|      407 N Maple Dr|          | Beverly Hills|     90210|        CA|         USA|34.076179|-118.394170|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       12|     c:12|                 |              SF Bay|     1355 Market St.|          | San Francisco|     94103|        CA|         USA|37.776805|-122.416924|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       14|     c:14|                 |              SF Bay|                    |          |    Menlo Park|          |        CA|         USA|37.484130|-122.169472|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       15|     c:15|               HQ|              SF Bay|   539 Bryant Street|          | San Francisco|     94107|        CA|         USA|37.789634|-122.404052|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       16|     c:16|                 |           San Diego|16935 W. Bernardo...|          |     San Diego|     92127|        CA|         USA|33.022176|-117.081406|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       17|     c:18|Lala Headquarters|              SF Bay|    209 Hamilton Ave|Suite #200|     Palo Alto|     94301|        CA|         USA|37.451151|-122.154369|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       18|     c:19|                 |         Los Angeles|10960 Wilshire Blvd.| Suite 700|   Los Angeles|     90024|        CA|         USA|34.057498|-118.446596|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       19|     c:20|     Headquarters|              SF Bay|2145 Hamilton Avenue|          |      San Jose|     95125|        CA|         USA|37.295005|-121.930035|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       20|     c:21|                 |United States - O...|                    |          |              |          |          |         USA|37.090240| -95.712891|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       21|     c:22|                 |            New York|                    |          | New York City|          |        NY|         USA|40.757929| -73.985506|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       22|     c:23|                 |            New York|    100 5th Ave Fl 6|          |      New York|10011-6903|        NY|         USA|40.746497| -74.009447|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       23|     c:24|                 |  California - Other|                    |          |              |          |        CA|         USA|37.269175|-119.306607|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       24|     c:25|               HQ|            New York|       1515 Broadway|          |      New York|     10036|        NY|         USA|40.757725| -73.986011|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       25|     c:26|                 |              London|                    |          |        London|          |          |         GBR|53.344104|  -6.267494|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       26|     c:27|               HQ|              SF Bay|1050 Enterprise W...|          |     Sunnyvale|     94089|        CA|         USA|37.387845|-122.055197|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       27|     c:28|     Headquarters|              SF Bay| 170 West Tasman Dr.|          |      San Jose|     95134|        CA|         USA|37.408802|-121.953770|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       28|     c:29|                 |              SF Bay|    701 First Avenue|          |     Sunnyvale|     94089|        CA|         USA|37.418531|-122.025485|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all table\n",
    "\n",
    "df_acquisition = tables[\"acquisition\"]\n",
    "df_company = tables[\"company\"]\n",
    "df_funding_rounds = tables[\"funding_rounds\"]\n",
    "df_funds = tables[\"funds\"]\n",
    "df_investments = tables[\"investments\"]\n",
    "df_ipos = tables[\"ipos\"]\n",
    "\n",
    "# check\n",
    "df_company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd8521-1d76-4405-858b-3fbf4a99f1d2",
   "metadata": {},
   "source": [
    "### From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e095748-b6b8-4e62-b2f4-980a2db88053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba254169-f554-44c9-ad58-88b2a6b19ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(link_api: str, list_parameter: dict, data_name: str):\n",
    "    try:\n",
    "        # Establish connection to API\n",
    "        resp = requests.get(link_api, params=list_parameter)\n",
    "        resp.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the response JSON\n",
    "        raw_response = resp.json()\n",
    "\n",
    "        # Convert JSON data to pandas DataFrame\n",
    "        df_api = pd.DataFrame(raw_response)\n",
    "\n",
    "        if df_api.empty:\n",
    "            raise ValueError(\"Empty response from API\")\n",
    "\n",
    "        # Convert pandas DataFrame to PySpark DataFrame\n",
    "        spark_df = spark.createDataFrame(df_api)\n",
    "\n",
    "        # Log success\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_db(log_msg)\n",
    "\n",
    "        print(f\"Successfully extracted data from API: {data_name}\")\n",
    "        return spark_df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Log request failure\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_db(log_msg)\n",
    "        print(f\"Request failed: {e}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        # Log parsing failure\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_db(log_msg)\n",
    "        print(f\"Parsing error: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other errors\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_db(log_msg)\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c186f5f1-1954-4c9d-b19a-9f6e97e1a860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted data from API: milestones\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "|          created_at|         description|milestone_at|milestone_code|milestone_id|object_id|  source_description|          source_url|          updated_at|\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "|2008-06-18 08:14:...|Survives iPhone 3...|  2008-06-09|         other|           1|     c:12|Twitter Fails To ...|http://www.techcr...|2008-06-18 08:14:...|\n",
      "|2008-06-18 08:50:...|More than 4 Billi...|  2008-06-18|         other|           3|     c:59|11 Billion Videos...|http://www.comsco...|2008-06-18 08:50:...|\n",
      "|2008-06-19 04:14:...|Reddit goes Open ...|  2008-06-18|         other|           4|    c:314|reddit goes open ...|http://blog.reddi...|2008-06-19 04:14:...|\n",
      "|2008-06-19 04:15:...|Adds the ability ...|  2008-01-22|         other|           5|    c:314|Reddit Adds Abili...|http://www.techcr...|2008-06-19 04:15:...|\n",
      "|2008-06-19 04:39:...|Adobe announced n...|  2008-06-18|         other|           6|    c:283|                 NaN|                 NaN|2008-06-19 04:39:...|\n",
      "|2008-06-19 08:29:...|Closed release of...|  2008-02-01|         other|           7|   c:6816|                 NaN|                 NaN|2008-06-19 16:55:...|\n",
      "|2008-06-19 08:29:...|Closed release of...|  2008-05-01|         other|           8|   c:6816|Diary of a Mobile...|http://mobverge.b...|2008-06-19 16:55:...|\n",
      "|2008-06-19 15:29:...|Scoofers, a new s...|  2008-06-19|         other|           9|   c:5874|Official launch o...|http://scoofers.b...|2008-06-19 19:34:...|\n",
      "|2008-06-19 17:48:...|Jeremy Zawodny le...|  2008-07-01|         other|          10|   c:2034|I'm Joining Craig...|http://jeremy.zaw...|2008-06-19 17:48:...|\n",
      "|2008-06-19 18:54:...|SnapLogic release...|  2008-04-23|         other|          11|   c:5085|SnapLogic release...|http://venturebea...|2008-06-19 18:54:...|\n",
      "|2008-06-20 02:06:...|Joshua Schachter ...|  2008-06-19|         other|          13|     c:75|It Gets Worse: Jo...|http://www.techcr...|2008-06-20 02:06:...|\n",
      "|2008-06-20 21:19:...|       Bevy Launches|  2008-05-15|         other|          14|   c:6947|            Facebook|http://apps.faceb...|2008-07-18 01:29:...|\n",
      "|2008-06-22 19:10:...|Twitter partners ...|  2008-06-09|         other|          15|     c:12|Twitter Partners ...|http://www.techcr...|2008-06-22 19:10:...|\n",
      "|2008-06-22 19:15:...|Launches a status...|  2008-05-28|         other|          16|     c:12|    What's Going On?|http://blog.twitt...|2008-06-22 19:15:...|\n",
      "|2008-06-22 19:15:...|Twitter Starts Bl...|  2008-05-07|         other|          17|     c:12|Twitter Starts Bl...|http://www.techcr...|2008-06-22 19:15:...|\n",
      "|2008-06-22 19:17:...|VP Lee Mighdoll L...|  2008-04-24|         other|          18|     c:12|VP Lee Mighdoll O...|http://www.techcr...|2008-06-22 19:17:...|\n",
      "|2008-06-22 19:19:...|Chief Architect B...|  2008-04-23|         other|          19|     c:12|Amateur Hour Over...|http://www.techcr...|2008-06-22 19:19:...|\n",
      "|2008-06-22 19:20:...|Twitter Japan Lau...|  2008-04-22|         other|          20|     c:12|Twitter! Japan! Ads!|http://www.techcr...|2008-06-22 19:20:...|\n",
      "|2008-06-22 19:21:...|Twitter helps bus...|  2008-04-16|         other|          21|     c:12|Twitter Saves Man...|http://www.techcr...|2008-06-23 19:40:...|\n",
      "|2008-06-23 01:49:...|Sometrics has int...|  2008-06-03|         other|          24|   c:5411|Sometrics Launche...|http://publicatio...|2008-06-23 19:16:...|\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract from API (year 2008 - 2010)\n",
    "\n",
    "link_api = \"https://api-milestones.vercel.app/api/data\"\n",
    "list_parameter = {\n",
    "    \"start_date\": \"2008-01-01\",\n",
    "    \"end_date\": \"2010-12-31\"\n",
    "}\n",
    "\n",
    "df_milestones = extract_api(link_api, list_parameter, \"milestones\")\n",
    "df_milestones.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35082a-3994-4b7f-bcf2-37ec93ff032c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load - Staging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ac65e0-9c28-4a03-a51e-96fe5b481438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84df23db-7cc2-4706-9c7e-467bf86376b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pangres in /opt/conda/lib/python3.11/site-packages (4.2.1)\n",
      "Requirement already satisfied: pandas>=0.25.3 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.12 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.22)\n",
      "Requirement already satisfied: alembic>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from pangres) (1.12.0)\n",
      "Requirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.11/site-packages (from pangres) (23.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (1.24.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.3.12->pangres) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->pangres) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.3.1->pangres) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pangres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b33ca6-54a8-45e6-96ea-241d6f62e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variable for Staging\n",
    "\n",
    "DB_STAGING_URL = os.getenv(\"DB_STAGING_URL\")\n",
    "DB_STAGING_USER = os.getenv(\"DB_STAGING_USER\")\n",
    "DB_STAGING_PASS = os.getenv(\"DB_STAGING_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968be5ca-e796-4b3b-b539-8939450416f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "from pangres import upsert\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_staging2(df, table_name, mode=\"overwrite\", use_upsert=False, idx_name=None, schema=None, source=None):\n",
    "    try:\n",
    "        if use_upsert:\n",
    "            # Convert Spark DataFrame to Pandas DataFrame\n",
    "            data = df.toPandas()\n",
    "\n",
    "            # Create connection to PostgreSQL\n",
    "            conn = create_engine(f\"postgresql://{DB_STAGING_USER}:{DB_STAGING_PASS}@host.docker.internal:5432/pyspark_task_staging\")\n",
    "\n",
    "            # Set index for upsert\n",
    "            if idx_name is None:\n",
    "                raise ValueError(\"Index name is required for upsert mode\")\n",
    "\n",
    "            data = data.set_index(idx_name)\n",
    "\n",
    "            # Upsert\n",
    "            upsert(\n",
    "                con=conn,\n",
    "                df=data,\n",
    "                table_name=table_name,\n",
    "                schema=schema,\n",
    "                if_row_exists=\"update\"\n",
    "            )\n",
    "            print(f\"Data upserted to table '{table_name}' successfully!\")\n",
    "        else:\n",
    "            # Load using Spark\n",
    "            df.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", \"jdbc:postgresql://host.docker.internal:5432/pyspark_task_staging\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", DB_STAGING_USER) \\\n",
    "                .option(\"password\", DB_STAGING_PASS) \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .mode(mode) \\\n",
    "                .save()\n",
    "\n",
    "            print(f\"Data loaded to table '{table_name}' successfully!\")\n",
    "\n",
    "        # Success log\n",
    "        log_msg = {\n",
    "            \"step\": \"Load Staging\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to table '{table_name}': {e}\")\n",
    "\n",
    "        # Failed DataFrame\n",
    "        failed_data = df.toPandas() if not use_upsert else data\n",
    "        failed_data['error_message'] = str(e)\n",
    "        failed_data['etl_date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Fail log\n",
    "        log_msg = {\n",
    "            \"step\": \"Load Staging\",\n",
    "            \"status\": \"Failed\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_message\": str(e)\n",
    "        }\n",
    "\n",
    "        # Save failed data to CSV\n",
    "        failed_log_path = f'logs/failed_{table_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        failed_data.to_csv(failed_log_path, index=False)\n",
    "        print(f\"Failed data saved to: {failed_log_path}\")\n",
    "\n",
    "    finally:\n",
    "        # Delete error_message before save it to log\n",
    "        if 'error_message' in log_msg:\n",
    "            del log_msg['error_message']\n",
    "\n",
    "        # Simpan log ke CSV\n",
    "        log_to_db(log_msg, 'etl_log.csv')\n",
    "\n",
    "    return df if not use_upsert else data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d6a2305-57a6-49b1-b57f-cf37f11a2a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'milestones' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: string, description: string, milestone_at: string, milestone_code: string, milestone_id: bigint, object_id: string, source_description: string, source_url: string, updated_at: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from API\n",
    "\n",
    "load_staging2(df_milestones, \"milestones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03e436bf-6909-4dbe-8a4f-9ad66ee5e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'relationship' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'people' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[people_id: string, object_id: string, first_name: string, last_name: string, birthplace: string, affiliation_name: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from CSV\n",
    "\n",
    "load_staging2(df_relations, \"relationship\")\n",
    "load_staging2(df_people, \"people\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "121a8a94-b4c4-4957-aaa4-2fb5dba19e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'acquisition' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'funding_rounds' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'funds' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'investments' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'ipo' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'company' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[office_id: int, object_id: string, description: string, region: string, address1: string, address2: string, city: string, zip_code: string, state_code: string, country_code: string, latitude: decimal(9,6), longitude: decimal(9,6), created_at: timestamp, updated_at: timestamp]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from database\n",
    "\n",
    "load_staging2(df_acquisition, \"acquisition\") \n",
    "load_staging2(df_funding_rounds, \"funding_rounds\") \n",
    "load_staging2(df_funds, \"funds\")\n",
    "load_staging2(df_investments, \"investments\")\n",
    "load_staging2(df_ipos, \"ipo\")\n",
    "load_staging2(df_company, \"company\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771dc6c-3890-4437-8cbb-215f4e0aab11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract Data from Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f576786a-b8bb-43b0-9740-3c34d1311910",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f7905f-e399-4aa4-ae59-7a0b1713abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variable for Staging\n",
    "\n",
    "DB_STAGING_URL = os.getenv(\"DB_STAGING_URL\")\n",
    "DB_STAGING_USER = os.getenv(\"DB_STAGING_USER\")\n",
    "DB_STAGING_PASS = os.getenv(\"DB_STAGING_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebcd5088-00c6-40a8-b4f2-5ddedf46c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_staging():\n",
    "    try:\n",
    "        # Get list of tables from staging\n",
    "        table_list = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DB_STAGING_URL) \\\n",
    "            .option(\"dbtable\", \"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public') AS tbl\") \\\n",
    "            .option(\"user\", DB_STAGING_USER) \\\n",
    "            .option(\"password\", DB_STAGING_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"table_name\") \\\n",
    "            .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print(f\"Found tables in staging: {table_list}\")\n",
    "\n",
    "        tables = {}\n",
    "        for table in table_list:\n",
    "            try:\n",
    "                # Read each table into a DataFrame\n",
    "                df = spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", DB_STAGING_URL) \\\n",
    "                    .option(\"dbtable\", table) \\\n",
    "                    .option(\"user\", DB_STAGING_USER) \\\n",
    "                    .option(\"password\", DB_STAGING_PASS) \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()\n",
    "\n",
    "                tables[table] = df\n",
    "\n",
    "                # Log success for each table\n",
    "                log_to_db({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": \"Success\",\n",
    "                    \"source\": \"PostgreSQL (Staging)\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "\n",
    "                print(f\"Successfully extracted table: {table}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log failure for specific table\n",
    "                log_to_db({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": f\"Failed: {e}\",\n",
    "                    \"source\": \"PostgreSQL (Staging)\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "                print(f\"Failed to extract table: {table} - Error: {e}\")\n",
    "\n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure for the whole extraction process\n",
    "        log_to_db({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"PostgreSQL (Staging)\",\n",
    "            \"table_name\": \"N/A\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        print(f\"Failed to extract tables: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e0eee4a-955a-482b-bec9-eb7fa9ab5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables in staging: ['investments', 'relationship', 'people', 'ipo', 'company', 'acquisition', 'funding_rounds', 'milestones', 'funds']\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: investments\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: relationship\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: people\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: ipo\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: company\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: acquisition\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funding_rounds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: milestones\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funds\n",
      "Extracted tables: ['investments', 'relationship', 'people', 'ipo', 'company', 'acquisition', 'funding_rounds', 'milestones', 'funds']\n"
     ]
    }
   ],
   "source": [
    "# Extract All Tables from Staging\n",
    "\n",
    "data = extract_from_staging()\n",
    "print(f\"Extracted tables: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "21bb0c38-f907-470a-8242-80a13d02ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|office_id|object_id|      description|              region|            address1|  address2|          city|  zip_code|state_code|country_code| latitude|  longitude|         created_at|         updated_at|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|        8|      c:8|                 |              SF Bay|959 Skyway Road, ...|          |    San Carlos|     94070|        CA|         USA|37.506885|-122.247573|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|        9|      c:9|     Headquarters|         Los Angeles|9229 W. Sunset Blvd.|          |West Hollywood|     90069|        CA|         USA|34.090368|-118.393064|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       10|     c:10|                 |         Los Angeles|8536 National Blv...|          |   Culver City|     90232|        CA|         USA|34.025958|-118.379768|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       11|     c:11|                 |         Los Angeles|      407 N Maple Dr|          | Beverly Hills|     90210|        CA|         USA|34.076179|-118.394170|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       12|     c:12|                 |              SF Bay|     1355 Market St.|          | San Francisco|     94103|        CA|         USA|37.776805|-122.416924|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       14|     c:14|                 |              SF Bay|                    |          |    Menlo Park|          |        CA|         USA|37.484130|-122.169472|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       15|     c:15|               HQ|              SF Bay|   539 Bryant Street|          | San Francisco|     94107|        CA|         USA|37.789634|-122.404052|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       16|     c:16|                 |           San Diego|16935 W. Bernardo...|          |     San Diego|     92127|        CA|         USA|33.022176|-117.081406|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       17|     c:18|Lala Headquarters|              SF Bay|    209 Hamilton Ave|Suite #200|     Palo Alto|     94301|        CA|         USA|37.451151|-122.154369|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       18|     c:19|                 |         Los Angeles|10960 Wilshire Blvd.| Suite 700|   Los Angeles|     90024|        CA|         USA|34.057498|-118.446596|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       19|     c:20|     Headquarters|              SF Bay|2145 Hamilton Avenue|          |      San Jose|     95125|        CA|         USA|37.295005|-121.930035|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       20|     c:21|                 |United States - O...|                    |          |              |          |          |         USA|37.090240| -95.712891|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       21|     c:22|                 |            New York|                    |          | New York City|          |        NY|         USA|40.757929| -73.985506|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       22|     c:23|                 |            New York|    100 5th Ave Fl 6|          |      New York|10011-6903|        NY|         USA|40.746497| -74.009447|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       23|     c:24|                 |  California - Other|                    |          |              |          |        CA|         USA|37.269175|-119.306607|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       24|     c:25|               HQ|            New York|       1515 Broadway|          |      New York|     10036|        NY|         USA|40.757725| -73.986011|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       25|     c:26|                 |              London|                    |          |        London|          |          |         GBR|53.344104|  -6.267494|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       26|     c:27|               HQ|              SF Bay|1050 Enterprise W...|          |     Sunnyvale|     94089|        CA|         USA|37.387845|-122.055197|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       27|     c:28|     Headquarters|              SF Bay| 170 West Tasman Dr.|          |      San Jose|     95134|        CA|         USA|37.408802|-121.953770|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       28|     c:29|                 |              SF Bay|    701 First Avenue|          |     Sunnyvale|     94089|        CA|         USA|37.418531|-122.025485|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read All Data from Staging\n",
    "\n",
    "acquisition = data[\"acquisition\"]\n",
    "company = data[\"company\"]\n",
    "funding_rounds = data[\"funding_rounds\"]\n",
    "funds = data[\"funds\"]\n",
    "investments = data[\"investments\"]\n",
    "ipos = data[\"ipo\"]\n",
    "milestones = data[\"milestones\"]\n",
    "people = data[\"people\"]\n",
    "relationship = data[\"relationship\"]\n",
    "\n",
    "# check\n",
    "company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d6181-6ec3-4c5d-8dd7-83623e0ebd1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e38fdd-8ff5-4ca5-9d28-385f85826723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "from decimal import Decimal\n",
    "\n",
    "# Helper function to convert values to JSON format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        return obj.isoformat()\n",
    "    return obj\n",
    "\n",
    "def profile_data(person, df, table_name, format_file):\n",
    "    try:\n",
    "        n_rows = df.count()\n",
    "        n_cols = len(df.columns)\n",
    "        \n",
    "        column_info = {}\n",
    "        for col in df.columns:\n",
    "            data_type = df.schema[col].dataType.simpleString()\n",
    "            sample_values = df.select(col).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "            null_count = df.filter(df[col].isNull()).count()\n",
    "            unique_count = df.select(col).distinct().count()\n",
    "            \n",
    "            # Min and max values (if numeric or date type)\n",
    "            try:\n",
    "                min_value = df.agg({col: \"min\"}).collect()[0][0]\n",
    "                max_value = df.agg({col: \"max\"}).collect()[0][0]\n",
    "            except:\n",
    "                min_value = None\n",
    "                max_value = None\n",
    "            \n",
    "            # Persentase missing value\n",
    "            percentage_missing = round((null_count / n_rows) * 100, 2) if n_rows > 0 else 0.0\n",
    "            \n",
    "            # Ambil 5 nilai unik sebagai sampel\n",
    "            unique_values = df.select(col).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "            \n",
    "            # Persentase valid date (khusus untuk tipe date dan datetime)\n",
    "            percentage_valid_date = None\n",
    "            if data_type in ['date', 'timestamp']:\n",
    "                valid_date_count = df.filter(df[col].isNotNull()).count()\n",
    "                percentage_valid_date = round((valid_date_count / n_rows) * 100, 2) if n_rows > 0 else 0.0\n",
    "\n",
    "            column_info[col] = {\n",
    "                \"data_type\": data_type,\n",
    "                \"sample_values\": [convert_to_serializable(v) for v in sample_values] if sample_values else None,\n",
    "                \"unique_count\": unique_count,\n",
    "                \"unique_value\": [convert_to_serializable(v) for v in unique_values] if unique_values else None,\n",
    "                \"null_count\": null_count,\n",
    "                \"percentage_missing_value\": percentage_missing,\n",
    "                \"min_value\": convert_to_serializable(min_value),\n",
    "                \"max_value\": convert_to_serializable(max_value),\n",
    "                \"percentage_valid_date\": percentage_valid_date\n",
    "            }\n",
    "        \n",
    "        dict_profiling = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"person_in_charge\": person,\n",
    "            \"profiling_result\": {\n",
    "                \"table_name\": table_name,\n",
    "                \"format_file\": format_file,\n",
    "                \"n_rows\": n_rows,\n",
    "                \"n_cols\": n_cols,\n",
    "                \"report\": column_info\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save profiling result to JSON\n",
    "        folder_path = \"data_profiling\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        file_path = os.path.join(folder_path, f\"{table_name}_profiling.json\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(dict_profiling, f, indent=4, default=convert_to_serializable)\n",
    "\n",
    "        print(f\"Profiling saved to: {file_path}\")\n",
    "\n",
    "        # Create success log message\n",
    "        log_msg = {\n",
    "            \"step\": \"Profiling\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": format_file,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error profiling table {table_name}: {e}\")\n",
    "\n",
    "        # Create fail log message\n",
    "        log_msg = {\n",
    "            \"step\": \"Profiling\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": format_file,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        # Save log to CSV\n",
    "        log_to_db(log_msg)\n",
    "\n",
    "    return dict_profiling if 'dict_profiling' in locals() else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc1ec5e0-6c82-4871-9bd0-1e89098e1663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling saved to: data_profiling/people_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "{\n",
      "  \"created_at\": \"2025-03-22T09:50:39.336805\",\n",
      "  \"person_in_charge\": \"Mr. A\",\n",
      "  \"profiling_result\": {\n",
      "    \"table_name\": \"people_data\",\n",
      "    \"format_file\": \"from Staging\",\n",
      "    \"n_rows\": 226709,\n",
      "    \"n_cols\": 6,\n",
      "    \"report\": {\n",
      "      \"people_id\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"177264\",\n",
      "          \"296\",\n",
      "          \"91421\",\n",
      "          \"467\",\n",
      "          \"177595\"\n",
      "        ],\n",
      "        \"unique_count\": 226709,\n",
      "        \"unique_value\": [\n",
      "          \"177264\",\n",
      "          \"296\",\n",
      "          \"91421\",\n",
      "          \"467\",\n",
      "          \"177595\"\n",
      "        ],\n",
      "        \"null_count\": 0,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"1\",\n",
      "        \"max_value\": \"99999\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"object_id\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"p:105829\",\n",
      "          \"p:73\",\n",
      "          \"p:171\",\n",
      "          \"p:214\",\n",
      "          \"p:214744\"\n",
      "        ],\n",
      "        \"unique_count\": 226709,\n",
      "        \"unique_value\": [\n",
      "          \"p:105829\",\n",
      "          \"p:73\",\n",
      "          \"p:171\",\n",
      "          \"p:214\",\n",
      "          \"p:214744\"\n",
      "        ],\n",
      "        \"null_count\": 0,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"p:10\",\n",
      "        \"max_value\": \"p:99999\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"first_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Hideki\",\n",
      "          \"Tyler\",\n",
      "          \"Nell\",\n",
      "          \"K\",\n",
      "          \"Rony\"\n",
      "        ],\n",
      "        \"unique_count\": 28423,\n",
      "        \"unique_value\": [\n",
      "          \"Hideki\",\n",
      "          \"Tyler\",\n",
      "          \"Nell\",\n",
      "          \"K\",\n",
      "          \"Rony\"\n",
      "        ],\n",
      "        \"null_count\": 5,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"\\\"\\\"\\\"DJ\\\"\",\n",
      "        \"max_value\": \"\\u00de\\u00f3rlindur\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"last_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Broukhim\",\n",
      "          \"Francois\",\n",
      "          \"Nerst\",\n",
      "          \"Banerjee\",\n",
      "          \"Cancel\"\n",
      "        ],\n",
      "        \"unique_count\": 107774,\n",
      "        \"unique_value\": [\n",
      "          \"Broukhim\",\n",
      "          \"Francois\",\n",
      "          \"Nerst\",\n",
      "          \"Banerjee\",\n",
      "          \"Cancel\"\n",
      "        ],\n",
      "        \"null_count\": 1,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"!llmind\",\n",
      "        \"max_value\": \"\\u00fcz\\u00fcm\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"birthplace\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Gainesville, FL\",\n",
      "          \"Bangalore\",\n",
      "          \"jacksonville , FL\",\n",
      "          \"Brisbane, Australia\",\n",
      "          \"North Adams, MA\"\n",
      "        ],\n",
      "        \"unique_count\": 8273,\n",
      "        \"unique_value\": [\n",
      "          \"Gainesville, FL\",\n",
      "          \"Bangalore\",\n",
      "          \"jacksonville , FL\",\n",
      "          \"Brisbane, Australia\",\n",
      "          \"North Adams, MA\"\n",
      "        ],\n",
      "        \"null_count\": 198622,\n",
      "        \"percentage_missing_value\": 87.61,\n",
      "        \"min_value\": \" Jr.\\\"\",\n",
      "        \"max_value\": \"\\u00e4\\u00b8\\u00ad\\u00e5\\u009b\\u00bd\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"affiliation_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Blue Nile\",\n",
      "          \"Pandora Media\",\n",
      "          \"YouSendIt\",\n",
      "          \"Diet TV\",\n",
      "          \"SomethingSimpler\"\n",
      "        ],\n",
      "        \"unique_count\": 27250,\n",
      "        \"unique_value\": [\n",
      "          \"Blue Nile\",\n",
      "          \"Pandora Media\",\n",
      "          \"YouSendIt\",\n",
      "          \"Diet TV\",\n",
      "          \"SomethingSimpler\"\n",
      "        ],\n",
      "        \"null_count\": 21,\n",
      "        \"percentage_missing_value\": 0.01,\n",
      "        \"min_value\": \"! Haz Life\",\n",
      "        \"max_value\": \"\\u00d6resundswebb\",\n",
      "        \"percentage_valid_date\": null\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "profiling_result = profile_data(\"Mr. A\", people, \"people_data\", \"from Staging\")\n",
    "print(json.dumps(profiling_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc775e70-e9da-46d1-8042-68d886be2794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling saved to: data_profiling/relationship_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/acquisition_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/company_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/funding_rounds_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/funds_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/investments_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/ipos_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/milestones_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-03-22T09:53:41.820808',\n",
       " 'person_in_charge': 'Mrs. OP',\n",
       " 'profiling_result': {'table_name': 'milestones_data',\n",
       "  'format_file': 'from Staging',\n",
       "  'n_rows': 8152,\n",
       "  'n_cols': 9,\n",
       "  'report': {'created_at': {'data_type': 'string',\n",
       "    'sample_values': ['2010-09-30 04:46:05.000',\n",
       "     '2010-10-04 23:53:31.000',\n",
       "     '2010-05-26 23:08:38.000',\n",
       "     '2010-07-09 05:01:33.000',\n",
       "     '2010-07-10 12:53:28.000'],\n",
       "    'unique_count': 7504,\n",
       "    'unique_value': ['2010-09-30 04:46:05.000',\n",
       "     '2010-10-04 23:53:31.000',\n",
       "     '2010-05-26 23:08:38.000',\n",
       "     '2010-07-09 05:01:33.000',\n",
       "     '2010-07-10 12:53:28.000'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-06-18 08:14:06.000',\n",
       "    'max_value': '2013-12-10 20:15:30.000',\n",
       "    'percentage_valid_date': None},\n",
       "   'description': {'data_type': 'string',\n",
       "    'sample_values': [\"Viewfinity named in 'Hottest Boston Companies' List\",\n",
       "     'Centralway invested in \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n LLC \\x97 the largest US-based mail order portal in the Czech Republic.',\n",
       "     'Eyeblaster files for IPO',\n",
       "     'Arsago Launches UCITS LatAm Fund.',\n",
       "     'Sonicbids launches new venue database.'],\n",
       "    'unique_count': 8012,\n",
       "    'unique_value': [\"Viewfinity named in 'Hottest Boston Companies' List\",\n",
       "     'Centralway invested in \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n LLC \\x97 the largest US-based mail order portal in the Czech Republic.',\n",
       "     'Eyeblaster files for IPO',\n",
       "     'Arsago Launches UCITS LatAm Fund.',\n",
       "     'Sonicbids launches new venue database.'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompleted the raise of StarVest Partners II, a $245 million fund in January 2009',\n",
       "    'max_value': 'â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£ â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_at': {'data_type': 'string',\n",
       "    'sample_values': ['2009-12-04',\n",
       "     '2009-06-23',\n",
       "     '2010-09-24',\n",
       "     '2010-02-12',\n",
       "     '2008-12-03'],\n",
       "    'unique_count': 924,\n",
       "    'unique_value': ['2009-12-04',\n",
       "     '2009-06-23',\n",
       "     '2010-09-24',\n",
       "     '2010-02-12',\n",
       "     '2008-12-03'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-01-01',\n",
       "    'max_value': '2010-12-31',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_code': {'data_type': 'string',\n",
       "    'sample_values': ['other'],\n",
       "    'unique_count': 1,\n",
       "    'unique_value': ['other'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'other',\n",
       "    'max_value': 'other',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_id': {'data_type': 'bigint',\n",
       "    'sample_values': [2453, 5556, 4823, 2509, 2529],\n",
       "    'unique_count': 8152,\n",
       "    'unique_value': [2453, 5556, 4823, 2509, 2529],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 1,\n",
       "    'max_value': 39356,\n",
       "    'percentage_valid_date': None},\n",
       "   'object_id': {'data_type': 'string',\n",
       "    'sample_values': ['c:41922', 'c:58230', 'c:23012', 'c:57411', 'c:44410'],\n",
       "    'unique_count': 4065,\n",
       "    'unique_value': ['c:41922', 'c:58230', 'c:23012', 'c:57411', 'c:44410'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'c:1',\n",
       "    'max_value': 'f:9985',\n",
       "    'percentage_valid_date': None},\n",
       "   'source_description': {'data_type': 'string',\n",
       "    'sample_values': ['Zedira Raises Additional Funding',\n",
       "     'Muecs Ltd. Launches an Online Collaboration Tool for Enterprises With a Social Media Analyzer',\n",
       "     'Mugasha Launches Online Electronic Music Service',\n",
       "     'Datapipe Named Growth Company of the Year',\n",
       "     'Towerstream Launches Wireless Broadband Network in Philadelphia, PA'],\n",
       "    'unique_count': 5452,\n",
       "    'unique_value': ['Zedira Raises Additional Funding',\n",
       "     'Muecs Ltd. Launches an Online Collaboration Tool for Enterprises With a Social Media Analyzer',\n",
       "     'Mugasha Launches Online Electronic Music Service',\n",
       "     'Datapipe Named Growth Company of the Year',\n",
       "     'Towerstream Launches Wireless Broadband Network in Philadelphia, PA'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '\\t  After Whirlwind Year, Everyday Health To Tap Public Markets For $100M',\n",
       "    'max_value': 'â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£ â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90',\n",
       "    'percentage_valid_date': None},\n",
       "   'source_url': {'data_type': 'string',\n",
       "    'sample_values': ['http://techcrunch.com/2010/07/10/google-secretly-invested-100-million-in-zynga-preparing-to-launch-google-games/',\n",
       "     'http://www.tokyocamp.net/',\n",
       "     'http://www.pr.com/press-release/173572',\n",
       "     'http://blog.zooners.com/',\n",
       "     'http://paidcontent.org/article/419-ipg-shifts-initiatives-breen-to-reprise-media-as-global-ceo/'],\n",
       "    'unique_count': 6064,\n",
       "    'unique_value': ['http://techcrunch.com/2010/07/10/google-secretly-invested-100-million-in-zynga-preparing-to-launch-google-games/',\n",
       "     'http://www.tokyocamp.net/',\n",
       "     'http://www.pr.com/press-release/173572',\n",
       "     'http://blog.zooners.com/',\n",
       "     'http://paidcontent.org/article/419-ipg-shifts-initiatives-breen-to-reprise-media-as-global-ceo/'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'NaN',\n",
       "    'max_value': 'www://connectwithcoaches.com/',\n",
       "    'percentage_valid_date': None},\n",
       "   'updated_at': {'data_type': 'string',\n",
       "    'sample_values': ['2010-05-31 01:20:43.000',\n",
       "     '2010-07-10 12:53:28.000',\n",
       "     '2010-07-12 02:15:04.000',\n",
       "     '2010-03-10 18:47:20.000',\n",
       "     '2010-10-12 21:16:38.000'],\n",
       "    'unique_count': 7263,\n",
       "    'unique_value': ['2010-05-31 01:20:43.000',\n",
       "     '2010-07-10 12:53:28.000',\n",
       "     '2010-07-12 02:15:04.000',\n",
       "     '2010-03-10 18:47:20.000',\n",
       "     '2010-10-12 21:16:38.000'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-06-18 08:14:06.000',\n",
       "    'max_value': '2013-12-11 03:24:18.000',\n",
       "    'percentage_valid_date': None}}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Profiling All Data\n",
    "\n",
    "profile_data(\"Mr. CCC\", relationship, \"relationship_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. H\", acquisition, \"acquisition_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. OP\", company, \"company_data\", \"from Staging\")\n",
    "profile_data(\"Mr. CCC\", funding_rounds, \"funding_rounds_data\", \"from Staging\")\n",
    "profile_data(\"Mr. A\", funds, \"funds_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. H\", investments, \"investments_data\", \"from Staging\")\n",
    "profile_data(\"Mr. A\", ipos, \"ipos_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. OP\", milestones, \"milestones_data\", \"from Staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09876a95-9d25-4760-b97f-5c3e3d97974c",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a58de3-97fc-48d0-86cc-1170986a34c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.11/site-packages (1.3.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "624340ac-6bab-450c-bd65-414833ac37ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pangres\n",
      "  Downloading pangres-4.2.1.tar.gz (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m813.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.12 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.22)\n",
      "Requirement already satisfied: alembic>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from pangres) (1.12.0)\n",
      "Requirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.11/site-packages (from pangres) (23.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (1.24.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.3.12->pangres) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->pangres) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.3.1->pangres) (2.1.3)\n",
      "Building wheels for collected packages: pangres\n",
      "  Building wheel for pangres (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pangres: filename=pangres-4.2.1-py3-none-any.whl size=66624 sha256=13f4c919d3ae6110532b6e97d6caa40dd7bc48a6fb0ac7d3800bb4225e1ab570\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/4b/a2/b0/4cc3bc0c120d7b5ba88691fc6773b771c88bd18f2152e1d752\n",
      "Successfully built pangres\n",
      "Installing collected packages: pangres\n",
      "Successfully installed pangres-4.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pangres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9f2c371-b617-4545-8cf4-8ba990ce3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, to_date, when, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from sqlalchemy import create_engine\n",
    "from pangres import upsert\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfb78a6-0390-4e98-8b0f-a0be8874ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Connection to Data Warehouse\n",
    "\n",
    "DWH_URL = os.getenv(\"DWH_URL\")\n",
    "DWH_USER = os.getenv(\"DWH_USER\")\n",
    "DWH_PASS = os.getenv(\"DWH_PASS\")\n",
    "engine = create_engine(f\"postgresql://{DWH_USER}:{DWH_PASS}@host.docker.internal:5432/pyspark_task_dwh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b7b6f-ef77-4b84-882b-77e5cbef9b34",
   "metadata": {},
   "source": [
    "### Helper Function to Analyze Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8b02b5a-afcd-4af1-b603-d594850466b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Values to Make it More Readable\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def clean_integer(value):\n",
    "    if isinstance(value, str):\n",
    "        match = re.match(r\"^[a-zA-Z]:(\\d+)\", value) \n",
    "        if match:\n",
    "            return int(match.group(1))  # Catch value after \":\"\n",
    "        else:\n",
    "            return None \n",
    "    return value\n",
    "    \n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def clean_text(value):\n",
    "    if value:\n",
    "        try:\n",
    "            # Handle encoding issue \n",
    "            value = value.encode('latin1').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            pass\n",
    "        # Normalization\n",
    "        value = unicodedata.normalize(\"NFKD\", value)\n",
    "        # Handle strange character\n",
    "        value = re.sub(r'[^\\x00-\\x7F]+', '', value)\n",
    "        value = value.strip()\n",
    "        value = unidecode(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def normalize_text(value):\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return None  \n",
    "    \n",
    "    # Make it lowercase\n",
    "    value = value.lower()\n",
    "    \n",
    "    # HDelete strange char\n",
    "    value = re.sub(r'[^\\w\\s,&/]', '', value)  # Alphanumeric, space, coma, apersand, slash\n",
    "    \n",
    "    value = re.sub(r'[/,&]', ' ', value) \n",
    "    \n",
    "    # Delete exaggerated space\n",
    "    value = re.sub(r'\\s+', ' ', value).strip()\n",
    "    \n",
    "    return value\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def clean_alpha_text(text):\n",
    "    if text:\n",
    "        # Delete all strange char, except alphanumeric and space\n",
    "        return re.sub(r'[^\\w\\s]', '', text).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def fix_encoding(s):\n",
    "    if s is not None:\n",
    "        try:\n",
    "            return unidecode(s)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663ad514-b2f2-423e-a9ac-1ef40543f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Extracting Prefix and Numeric ID\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_prefix(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[0]\n",
    "    return None\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def extract_id(value):\n",
    "    if value and \":\" in value:\n",
    "        try:\n",
    "            return int(value.split(\":\")[1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4e547c3-3257-47a8-944e-2cd8f5faf2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Handle Stock-related Column\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_stock_market(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[0]\n",
    "    return None\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_stock_symbol(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b194ac-fa94-48c5-8e1d-78eb41b8d816",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Company Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "037320c8-bd0e-46b0-a0e8-dfa5d4c2e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, split\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_company(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"latitude\", col(\"latitude\").cast(\"decimal(9,6)\"))\n",
    "        df = df.withColumn(\"longitude\", col(\"longitude\").cast(\"decimal(9,6)\"))\n",
    "        \n",
    "        # Extract Extract prefix and ID \n",
    "        df = df.withColumn(\"entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "\n",
    "        # Step 3: Encoding\n",
    "        df = df.withColumn(\"description\", clean_text(\"description\"))\n",
    "        df = df.withColumn(\"address1\", clean_text(\"address1\"))\n",
    "        df = df.withColumn(\"zip_code\", clean_text(\"zip_code\"))\n",
    "        df = df.withColumn(\"region\", clean_text(\"region\"))\n",
    "        \n",
    "        log_to_db({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"office_id\").alias(\"company_id\"),\n",
    "            col(\"entity_type\").alias(\"company_entity_type\"),\n",
    "            col(\"object_id\").alias(\"company_object_id\"),  # INT\n",
    "            col(\"description\").alias(\"description\"),\n",
    "            col(\"address1\").alias(\"address\"),\n",
    "            col(\"region\").alias(\"region\"),\n",
    "            col(\"city\").alias(\"city\"),\n",
    "            col(\"zip_code\").alias(\"zip_code\"),\n",
    "            col(\"state_code\").alias(\"state_code\"),\n",
    "            col(\"country_code\").alias(\"country_code\"),\n",
    "            col(\"latitude\").alias(\"latitude\"),\n",
    "            col(\"longitude\").alias(\"longitude\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 5: Data cleansing \n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"description\": \"Unknown\",\n",
    "            \"address\": \"Unknown\",\n",
    "            \"region\": \"Unknown\",\n",
    "            \"city\": \"Unknown\",\n",
    "            \"zip_code\": \"Unknown\",\n",
    "            \"state_code\": \"Unknown\",\n",
    "            \"country_code\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicate data  and latitude/longitude with value = 0\n",
    "        df_transformed = df_transformed.dropDuplicates([\"company_object_id\"])\n",
    "        df_transformed = df_transformed.filter((col(\"latitude\") != 0) & (col(\"longitude\") != 0))\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "        return df_transformed\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef586144-4ba6-4091-a89c-8aa71be742e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "company = spark.read \\\n",
    "       .format(\"jdbc\") \\\n",
    "       .option(\"url\", DB_STAGING_URL) \\\n",
    "       .option(\"dbtable\", \"company\") \\\n",
    "       .option(\"user\", DB_STAGING_USER) \\\n",
    "       .option(\"password\", DB_STAGING_PASS) \\\n",
    "       .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "       .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dbdfdee-7547-48c9-9388-caa2e92802a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "The data is successfully transformed\n"
     ]
    }
   ],
   "source": [
    "transformed_company = transform_company(company)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3153e-6e62-4fb0-b4aa-ef1dadfcb9e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### People Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea816120-0f53-4fc8-b20d-cf668aa60a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, broadcast\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_people(df, enable_company_validation=False): # kalau dibutuhkan validasi ke company, ubah jadi true\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 1: Replace empty strings with NULL\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Create full_name from first_name + last_name\n",
    "        df = df.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "        # Step 3: Extract prefix and ID\n",
    "        df = df.withColumn(\"people_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"people_object_id\", extract_id(col(\"object_id\")))\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 4: Mapping target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"people_id\").alias(\"people_id\"),\n",
    "            col(\"people_entity_type\"),\n",
    "            col(\"people_object_id\"),\n",
    "            col(\"full_name\"),\n",
    "            col(\"birthplace\"),\n",
    "            col(\"affiliation_name\")\n",
    "        )\n",
    "\n",
    "        # Step 5: Cleaning\n",
    "        df_transformed = df_transformed.withColumn(\"people_object_id\", clean_integer(col(\"people_object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"full_name\", clean_alpha_text(col(\"full_name\")))\n",
    "        df_transformed = df_transformed.withColumn(\"birthplace\", fix_encoding(col(\"birthplace\")))\n",
    "        df_transformed = df_transformed.withColumn(\"affiliation_name\", clean_alpha_text(col(\"affiliation_name\")))\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 6: Fillna & dedup\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"full_name\": \"Unknown\",\n",
    "            \"birthplace\": \"Unknown\",\n",
    "            \"affiliation_name\": \"Unknown\"\n",
    "        })\n",
    "        df_transformed = df_transformed.dropDuplicates([\"people_entity_type\", \"people_object_id\"])\n",
    "        df_transformed = df_transformed.filter(col(\"full_name\") != \"Unknown\")\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 7: OPTIONAL validation against dim_company\n",
    "        if enable_company_validation and df_company is not None:\n",
    "            df_company_ids = df_company.select(col(\"company_object_id\"))\n",
    "\n",
    "            df_valid = df_transformed.join(\n",
    "                broadcast(df_company_ids),\n",
    "                on=col(\"people_object_id\") == col(\"company_object_id\"),\n",
    "                how=\"inner\"\n",
    "            ).drop(\"company_object_id\")\n",
    "\n",
    "            df_invalid = df_transformed.join(\n",
    "                broadcast(df_company_ids),\n",
    "                on=col(\"people_object_id\") == col(\"company_object_id\"),\n",
    "                how=\"left_anti\"\n",
    "            )\n",
    "\n",
    "            if df_invalid.count() > 0:\n",
    "                invalid_ids = df_invalid.select(\"people_entity_type\", \"people_object_id\").toPandas().values.tolist()\n",
    "                save_invalid_ids(invalid_ids, table_name=\"people\")\n",
    "\n",
    "                log_to_db({\n",
    "                    \"step\": \"Validation\",\n",
    "                    \"status\": f\"{df_invalid.count()} rows missing object_id\",\n",
    "                    \"source\": \"staging\",\n",
    "                    \"table_name\": \"people\",\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "\n",
    "            return df_valid\n",
    "        else:\n",
    "            return df_transformed\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf49fa4-4565-494d-8d64-2a8011816b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "people = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"people\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5575a930-b015-48f6-9472-1954aa3c7058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n"
     ]
    }
   ],
   "source": [
    "# Transform People Data\n",
    "transformed_people = transform_people(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cff3a7-3d19-4893-babd-43b34e2ea879",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Milestones Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14d710e0-d0b3-474c-b694-57524dd9bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_milestones(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "        df = df.na.replace(\"NaN\", None)\n",
    "        \n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"milestone_date\", to_date(col(\"milestone_at\")))\n",
    "\n",
    "        # Step 3: Extract prefix and ID dari object_id\n",
    "        df = df.withColumn(\"entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "        \n",
    "        log_to_db({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"milestone_id\").alias(\"milestone_id\"),\n",
    "            col(\"entity_type\").alias(\"milestone_entity_type\"),\n",
    "            col(\"object_id\").alias(\"milestone_object_id\"),\n",
    "            col(\"milestone_date\").alias(\"milestone_date\"),\n",
    "            col(\"description\").alias(\"description\"),\n",
    "            col(\"source_url\").alias(\"source_url\"),\n",
    "            col(\"source_description\").alias(\"source_description\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "            \n",
    "        # Step 5: Handle strange values\n",
    "        df_transformed = df_transformed.withColumn(\"milestone_object_id\", clean_integer(col(\"milestone_object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"description\", clean_alpha_text(\"description\"))\n",
    "        df_transformed = df_transformed.withColumn(\"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\"))\n",
    "        df_transformed = df_transformed.withColumn(\"source_description\", clean_alpha_text(\"source_description\"))\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 6: Cleaning data\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"description\": \"No Description\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "        \n",
    "        # Step 7: Drop duplicate data \n",
    "        df_transformed = df_transformed.dropDuplicates([\"milestone_id\"])\n",
    "        df_transformed = df_transformed.dropDuplicates([\"milestone_entity_type\", \"milestone_object_id\"])\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 8: Validation object_id in company\n",
    "        # Take object_id from transform_company\n",
    "        df_company_ids = transformed_company.select(col(\"company_object_id\"))\n",
    "\n",
    "        # Convert object_id to integer (if needed)\n",
    "        df_transformed = df_transformed.withColumn(\"milestone_object_id\", col(\"milestone_object_id\").cast(\"int\"))\n",
    "        \n",
    "        # Filter NULL object_id explicitly\n",
    "        df_transformed = df_transformed.filter(col(\"milestone_object_id\").isNotNull())\n",
    "        \n",
    "        # Valid data (match with `transformed_company`)\n",
    "        df_valid = df_transformed.join(\n",
    "            broadcast(df_company_ids),\n",
    "            on=col(\"milestone_object_id\") == col(\"company_object_id\"),\n",
    "            how=\"inner\"\n",
    "        ).drop(\"company_object_id\")\n",
    "        \n",
    "        # Invalid data\n",
    "        df_invalid = df_transformed.join(\n",
    "            broadcast(df_company_ids),\n",
    "            on=col(\"milestone_object_id\") == col(\"company_object_id\"),\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"milestone_entity_type\", \"milestone_object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"milestones\")\n",
    "        \n",
    "            log_to_db({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"milestones\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "        return df_valid\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86caff59-f7f6-4d4f-90c1-ab320d95d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "milestones = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"milestones\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f787b43-5541-4409-b0bd-5a8466d89ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "2305 invalid IDs from table 'milestones' saved to logger DB.\n",
      "Log successfully written to database\n",
      "The data is successfully transformed\n"
     ]
    }
   ],
   "source": [
    "# Transform Milestones Data\n",
    "transformed_milestones = transform_milestones(milestones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09597a83-09dd-46d4-89ee-13eea7d7763a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Acquisition Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4df8202b-f651-4601-b2f3-889350e36262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import datetime\n",
    "from src.utils.logger import log_to_db, save_invalid_ids\n",
    "from src.utils.helper import extract_prefix, extract_id\n",
    "\n",
    "def transform_acquisition(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Clean & basic transform\n",
    "        df = df.na.replace(\"\", None)\n",
    "        df = df.withColumn(\"price_amount\", col(\"price_amount\").cast(\"decimal(15,2)\"))\n",
    "        df = df.withColumn(\"acquired_at\", to_date(col(\"acquired_at\")))\n",
    "        df = df.withColumn(\"acquiring_entity_type\", extract_prefix(col(\"acquiring_object_id\")))\n",
    "        df = df.withColumn(\"acquired_entity_type\", extract_prefix(col(\"acquired_object_id\")))\n",
    "        df = df.withColumn(\"acquiring_object_id\", extract_id(col(\"acquiring_object_id\")).cast(IntegerType()))\n",
    "        df = df.withColumn(\"acquired_object_id\", extract_id(col(\"acquired_object_id\")).cast(IntegerType()))\n",
    "\n",
    "        # Select relevant columns\n",
    "        df_transformed = df.select(\n",
    "            \"acquisition_id\", \"acquiring_entity_type\", \"acquiring_object_id\",\n",
    "            \"acquired_entity_type\", \"acquired_object_id\", \"price_amount\",\n",
    "            \"price_currency_code\", \"acquired_at\", \"source_url\", \"created_at\", \"updated_at\"\n",
    "        ).fillna({\n",
    "            \"price_amount\": 0.0,\n",
    "            \"price_currency_code\": \"Unknown\",\n",
    "            \"source_url\": \"Unknown\"\n",
    "        }).dropDuplicates([\"acquisition_id\"]) \\\n",
    "         .filter(col(\"price_amount\") != 0.0) \\\n",
    "         .na.drop(subset=[\"acquired_at\"])\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Validate object_id against dim_company\n",
    "        df_company_ids = transformed_company.select(\"company_object_id\", \"company_id\")\n",
    "\n",
    "        df_validated = (\n",
    "            df_transformed\n",
    "            .join(broadcast(df_company_ids.withColumnRenamed(\"company_object_id\", \"acq_obj_id\")\n",
    "                           .withColumnRenamed(\"company_id\", \"acq_id\")),\n",
    "                  col(\"acquiring_object_id\") == col(\"acq_obj_id\"), \"left\")\n",
    "            .join(broadcast(df_company_ids.withColumnRenamed(\"company_object_id\", \"acqed_obj_id\")\n",
    "                           .withColumnRenamed(\"company_id\", \"acqed_id\")),\n",
    "                  col(\"acquired_object_id\") == col(\"acqed_obj_id\"), \"left\")\n",
    "        )\n",
    "        \n",
    "        df_valid = (\n",
    "            df_validated\n",
    "            .filter(col(\"acq_obj_id\").isNotNull() & col(\"acqed_obj_id\").isNotNull())\n",
    "            .select(\n",
    "                \"acquisition_id\",\n",
    "                \"acquiring_entity_type\",\n",
    "                \"acquired_entity_type\",\n",
    "                \"price_amount\",\n",
    "                \"price_currency_code\",\n",
    "                \"acquired_at\",\n",
    "                \"source_url\",\n",
    "                \"created_at\",\n",
    "                \"updated_at\",\n",
    "                col(\"acq_obj_id\").alias(\"acquiring_object_id\"),\n",
    "                col(\"acqed_obj_id\").alias(\"acquired_object_id\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Invalid records\n",
    "        df_invalid = df_validated.filter(\n",
    "            col(\"acq_obj_id\").isNull() | col(\"acqed_obj_id\").isNull()\n",
    "        )\n",
    "\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\n",
    "                \"acquiring_entity_type\", \"acquiring_object_id\",\n",
    "                \"acquired_entity_type\", \"acquired_object_id\"\n",
    "            ).toPandas().values.tolist()\n",
    "\n",
    "            save_invalid_ids(invalid_ids, table_name=\"acquisition\")\n",
    "\n",
    "            log_to_db({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id in dim_company\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"acquisition\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "        return df_valid\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dee608a3-37bd-4339-bf48-6fb3850ac8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "acquisition = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"acquisition\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bdc7052b-1c4b-4c27-8828-2b392d46bf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "1329 invalid IDs from table 'acquisition' saved to logger DB.\n",
      "Log successfully written to database\n",
      "The data is successfully transformed\n"
     ]
    }
   ],
   "source": [
    "# transform acquisition data\n",
    "\n",
    "transformed_acquisition = transform_acquisition(acquisition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54cf6c4-5a50-47be-9731-6d7768dc379e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Investments Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db3c31b4-f469-4d38-98fb-dac68bcb26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_investments(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Extract prefix and ID\n",
    "        df = df.withColumn(\"funded_entity_type\", extract_prefix(col(\"funded_object_id\")))\n",
    "        df = df.withColumn(\"investor_entity_type\", extract_prefix(col(\"investor_object_id\")))\n",
    "        df = df.withColumn(\"funded_object_id\", extract_id(col(\"funded_object_id\")))\n",
    "        df = df.withColumn(\"investor_object_id\", extract_id(col(\"investor_object_id\")))\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 3: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"investment_id\").alias(\"investment_id\"),\n",
    "            col(\"funding_round_id\").alias(\"funding_round_id\"),\n",
    "            col(\"funded_entity_type\").alias(\"funded_entity_type\"),\n",
    "            col(\"funded_object_id\").alias(\"funded_object_id\"),\n",
    "            col(\"investor_entity_type\").alias(\"investor_entity_type\"),\n",
    "            col(\"investor_object_id\").alias(\"investor_object_id\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Step 4: Clean integer values\n",
    "        df_transformed = df_transformed.withColumn(\"funded_object_id\", clean_integer(col(\"funded_object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"investor_object_id\", clean_integer(col(\"investor_object_id\")))\n",
    "        \n",
    "        log_to_db({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicates\n",
    "        df_transformed = df_transformed.dropDuplicates([\"investment_id\"])\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 7: Validation object_id in company data and people data\n",
    "        # Take object_id from transform_company\n",
    "        df_company_ids = transformed_company.select(\"company_object_id\", \"company_id\")\n",
    "\n",
    "        # Take object_id from transform_people\n",
    "        df_people_ids = transformed_people.select(\"people_object_id\", \"people_id\") \n",
    "\n",
    "        # Join with companies and people tables\n",
    "        df_transformed = df_transformed \\\n",
    "            .join(df_company_ids.alias(\"comp\"), df_transformed[\"investor_object_id\"] == col(\"comp.company_object_id\"), \"left\") \\\n",
    "            .join(df_people_ids.alias(\"peop\"), df_transformed[\"investor_object_id\"] == col(\"peop.people_object_id\"), \"left\") \\\n",
    "            .withColumn(\"investor_object_id\", when(col(\"comp.company_id\").isNotNull(), col(\"comp.company_id\"))\n",
    "                        .otherwise(col(\"peop.people_id\"))) \\\n",
    "            .withColumn(\"investor_entity_type\", when(col(\"comp.company_id\").isNotNull(), lit(\"company\"))\n",
    "                        .otherwise(lit(\"people\")))\n",
    "\n",
    "        df_transformed = df_transformed \\\n",
    "            .join(df_company_ids.alias(\"funded\"), df_transformed[\"funded_object_id\"] == col(\"funded.company_object_id\"), \"left\") \\\n",
    "            .withColumn(\"funded_object_id\", col(\"funded.company_id\"))\n",
    "\n",
    "        df_transformed = df_transformed.select(\n",
    "            \"investment_id\",\n",
    "            \"funding_round_id\",\n",
    "            \"funded_entity_type\",\n",
    "            \"funded_object_id\",  # ini udah di-replace dengan company_id hasil join\n",
    "            \"investor_entity_type\",\n",
    "            \"investor_object_id\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # Validation (check if object_id mapping failed)\n",
    "        df_invalid = df_transformed.filter(col(\"investor_object_id\").isNull() | col(\"funded_object_id\").isNull())\n",
    "        df_valid = df_transformed.filter(col(\"investor_object_id\").isNotNull() & col(\"funded_object_id\").isNotNull())\n",
    "\n",
    "        \n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"investor_entity_type\", \"investor_object_id\", \n",
    "                                            \"funded_entity_type\", \"funded_object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"investments\")\n",
    "            \n",
    "            log_to_db({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"investments\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "        return df_valid\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2b2fb2f-b1c0-4a2f-9dfe-b14ed294a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "investments = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"investments\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f5b87a7-79cf-4feb-be33-15ffcb2d48be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "34648 invalid IDs from table 'investments' saved to logger DB.\n",
      "Log successfully written to database\n",
      "The data is successfully transformed\n"
     ]
    }
   ],
   "source": [
    "# Transform investments data\n",
    "\n",
    "transformed_investments = transform_investments(investments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca3ec2-187a-4d41-a70e-ae7b780910b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Funding Rounds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d60e4442-1ffc-496b-9beb-155febd47037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, when, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_funding_rounds(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Extract prefix and ID\n",
    "        df = df.withColumn(\"funding_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", col(\"object_id\").cast(IntegerType()))\n",
    "\n",
    "        # Step 3: Format data type\n",
    "        df = df.withColumn(\"funding_date\", to_date(col(\"funded_at\")))\n",
    "        df = df.withColumn(\"funding_entity_type\", col(\"funding_entity_type\").cast(StringType()))\n",
    "        df = df.withColumn(\"participants\", col(\"participants\").cast(IntegerType()))\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"funding_round_id\"),\n",
    "            col(\"funding_entity_type\"),\n",
    "            col(\"object_id\").alias(\"funding_object_id\"),\n",
    "            col(\"funding_round_type\").alias(\"round_type\"),\n",
    "            col(\"funding_date\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\"),\n",
    "            col(\"raised_amount_usd\"),\n",
    "            col(\"pre_money_currency_code\").alias(\"pre_money_currency\"),\n",
    "            col(\"pre_money_valuation\"),\n",
    "            col(\"pre_money_valuation_usd\"),\n",
    "            col(\"post_money_currency_code\").alias(\"post_money_currency\"),\n",
    "            col(\"post_money_valuation\"),\n",
    "            col(\"post_money_valuation_usd\"),\n",
    "            col(\"participants\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"source_description\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Step 5: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"round_type\": \"Unknown\",\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"pre_money_currency\": \"USD\",\n",
    "            \"post_money_currency\": \"USD\",\n",
    "            \"raised_amount_usd\": 0.0,\n",
    "            \"source_description\": \"Unknown\",\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicates dan invalid data\n",
    "        df_transformed = df_transformed.dropDuplicates([\"funding_round_id\"])\n",
    "        df_transformed = df_transformed.filter(col(\"round_type\") != \"Unknown\")\n",
    "\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\")\n",
    "        )\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_description\", clean_alpha_text(\"source_description\")\n",
    "        )\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 7: Load dim_company and dim_people for validation\n",
    "        # Take object_id from transform_company\n",
    "        df_company_ids = transformed_company.select(\"company_object_id\", \"company_id\")\n",
    "\n",
    "        # Take object_id from transform_people\n",
    "        df_people_ids = transformed_people.select(\"people_object_id\", \"people_id\") \n",
    "\n",
    "        # Validation on funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(df_company_ids), df_transformed[\"funding_object_id\"] == df_company_ids[\"company_object_id\"], \"left\") \\\n",
    "            .join(broadcast(df_people_ids), df_transformed[\"funding_object_id\"] == df_people_ids[\"people_object_id\"], \"left\") \\\n",
    "            .withColumn(\"funding_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_object_id\").isNull() | col(\"people_object_id\").isNull() | col(\"funding_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_object_id\").isNotNull() & col(\"people_object_id\").isNotNull() & col(\"funding_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"funding_entity_type\", \"funding_object_id\").toPandas().values.tolist()\n",
    "            # invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"funding_rounds\")\n",
    "            \n",
    "            log_to_db({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"funding_rounds\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "        return df_valid\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74aafda1-4af4-4373-9165-d0ef0daa0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "\n",
    "funding_rounds = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"funding_rounds\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa247fe5-1e9c-439a-b13f-94b96d4211d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "34100 invalid IDs from table 'funding_rounds' saved to logger DB.\n",
      "Log successfully written to database\n",
      "The data is successfully transformed\n"
     ]
    }
   ],
   "source": [
    "# Transform funding rounds\n",
    "\n",
    "transformed_funding_rounds = transform_funding_rounds(funding_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81202b76-a08c-4679-acbc-d62308dda2ce",
   "metadata": {},
   "source": [
    "### Relationship Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7792757b-77c5-42dc-a63f-3e4fc135dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp, broadcast, when\n",
    "from datetime import datetime\n",
    "from src.utils.logger import log_to_db, save_invalid_ids\n",
    "from src.utils.helper import extract_prefix, extract_id, normalize_text\n",
    "\n",
    "def transform_relationship(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data types\n",
    "        df = df.withColumn(\"start_at\", to_date(col(\"start_at\")))\n",
    "        df = df.withColumn(\"end_at\", to_date(col(\"end_at\")))\n",
    "        df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\")))\n",
    "        df = df.withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "        # Step 3: Extract IDs & clean\n",
    "        df = df.withColumn(\"people_entity_type\", extract_prefix(col(\"person_object_id\")))\n",
    "        df = df.withColumn(\"relationship_entity_type\", extract_prefix(col(\"relationship_object_id\")))\n",
    "        df = df.withColumn(\"people_object_id\", extract_id(col(\"person_object_id\")))\n",
    "        df = df.withColumn(\"relationship_object_id\", extract_id(col(\"relationship_object_id\")))\n",
    "        df = df.withColumn(\"title\", normalize_text(col(\"title\")))\n",
    "\n",
    "        # Step 4: Select target columns\n",
    "        df_transformed = df.select(\n",
    "            col(\"relationship_id\"),\n",
    "            col(\"people_entity_type\"),\n",
    "            col(\"people_object_id\"),\n",
    "            col(\"relationship_entity_type\"),\n",
    "            col(\"relationship_object_id\"),\n",
    "            col(\"start_at\"),\n",
    "            col(\"end_at\"),\n",
    "            col(\"title\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Step 5: Fill null values\n",
    "        df_transformed = df_transformed.fillna({\"title\": \"Unknown\"})\n",
    "\n",
    "        # Step 6: Drop duplicates by primary key\n",
    "        df_transformed = df_transformed.dropDuplicates([\"relationship_id\"])\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 7: Prepare company & people IDs for validation\n",
    "        df_company_ids = transformed_company.select(\"company_object_id\", \"company_id\")\n",
    "        df_people_ids = transformed_people.select(\"people_object_id\", \"people_id\")\n",
    "\n",
    "        # Step 8: Join to validate relationship_object_id (company/people)\n",
    "        df_validated = df_transformed \\\n",
    "            .join(broadcast(df_company_ids), df_transformed[\"relationship_object_id\"] == df_company_ids[\"company_object_id\"], \"left\") \\\n",
    "            .join(broadcast(df_people_ids), df_transformed[\"relationship_object_id\"] == df_people_ids[\"people_object_id\"], \"left\") \\\n",
    "            .withColumn(\"relationship_object_id\", when(\n",
    "                col(\"company_id\").isNotNull(), col(\"company_id\")\n",
    "            ).otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\", \"company_object_id\", \"people_object_id\")\n",
    "\n",
    "        # Step 9: Join to validate people_object_id (from person)\n",
    "        df_validated = df_validated \\\n",
    "            .join(broadcast(df_people_ids), df_transformed[\"people_object_id\"] == df_people_ids[\"people_object_id\"], \"left\") \\\n",
    "            .withColumn(\"people_object_id\", col(\"people_id\")) \\\n",
    "            .drop(\"people_id\")\n",
    "\n",
    "        # Step 10: Filter invalid rows\n",
    "        df_invalid = df_validated.filter(\n",
    "            col(\"relationship_object_id\").isNull() | col(\"people_object_id\").isNull()\n",
    "        )\n",
    "        df_valid = df_validated.filter(\n",
    "            col(\"relationship_object_id\").isNotNull() & col(\"people_object_id\").isNotNull()\n",
    "        )\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\n",
    "                \"relationship_entity_type\", \"relationship_object_id\"\n",
    "            ).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"relationship\")\n",
    "            log_to_db({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"relationship\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "            \n",
    "        # Rename hasil validasi (overwrite nilai lama)\n",
    "        df_valid = df_valid.drop(\"people_object_id\").withColumnRenamed(\"people_id\", \"people_object_id\")\n",
    "\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "        return df_valid\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5df68fbd-aa7c-4bdd-84f4-475f9bc940ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, to_timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_relationship(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"start_at\", to_date(col(\"start_at\")))\n",
    "        df = df.withColumn(\"end_at\", to_date(col(\"end_at\")))\n",
    "        df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\")))\n",
    "        df = df.withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "        # Step 3: Extract prefix and ID, normalize\n",
    "        df = df.withColumn(\"people_entity_type\", extract_prefix(col(\"person_object_id\")))\n",
    "        df = df.withColumn(\"relationship_entity_type\", extract_prefix(col(\"relationship_object_id\")))\n",
    "        df = df.withColumn(\"people_object_id\", extract_id(col(\"person_object_id\")))\n",
    "        df = df.withColumn(\"relationship_object_id\", extract_id(col(\"relationship_object_id\")))\n",
    "        df = df.withColumn(\"title\", normalize_text(col(\"title\")))\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 3: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"relationship_id\"),\n",
    "            col(\"people_entity_type\").alias(\"people_entity_type\"),\n",
    "            col(\"people_object_id\").alias(\"people_object_id\"),\n",
    "            col(\"relationship_entity_type\").alias(\"relationship_entity_type\"),\n",
    "            col(\"relationship_object_id\").alias(\"relationship_object_id\"),\n",
    "            col(\"start_at\").alias(\"start_at\"),\n",
    "            col(\"end_at\").alias(\"end_at\"),\n",
    "            col(\"title\").alias(\"title\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "        \n",
    "        log_to_db({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 4: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"title\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 5: Drop duplicates and null values\n",
    "        df_transformed = df_transformed.dropDuplicates([\"relationship_id\"])\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 6: Load dim_company dan dim_people for validation\n",
    "        # Take object_id from transform_company\n",
    "        df_company_ids = transformed_company.select(\"company_object_id\", \"company_id\")\n",
    "\n",
    "        # Take object_id from transform_people\n",
    "        df_people_ids = transformed_people.select(\"people_object_id\", \"people_id\") \n",
    "\n",
    "        # Validation on funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(df_company_ids), df_transformed[\"relationship_object_id\"] == df_company_ids[\"company_object_id\"], \"left\") \\\n",
    "            .join(broadcast(df_people_ids), df_transformed[\"relationship_object_id\"] == df_people_ids[\"people_object_id\"], \"left\") \\\n",
    "            .withColumn(\"relationship_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"relationship_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"relationship_object_id\").isNotNull())\n",
    "        \n",
    "        # DROP yang lama biar gak dobel\n",
    "        df_valid = df_valid.drop(\"people_object_id\")\n",
    "        \n",
    "        # Rename hasil validasi\n",
    "        df_valid = df_valid.withColumnRenamed(\"people_id\", \"people_object_id\")\n",
    "\n",
    "        \n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"relationship_entity_type\", \"relationship_object_id\").toPandas().values.tolist()\n",
    "            # invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"relationship\")\n",
    "\n",
    "            log_to_db({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"relationship\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "        return df_valid\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "56dfd543-2fe0-4fa4-9756-f57596313b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "relationship = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"relationship\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f1aac358-186d-43cb-b883-525e86198512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "85169 invalid IDs from table 'relationship' saved to logger DB.\n",
      "Log successfully written to database\n",
      "The data is successfully transformed\n"
     ]
    }
   ],
   "source": [
    "# Transform relationship data\n",
    "\n",
    "transformed_relationship = transform_relationship(relationship)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bf71d-6769-4ebb-9e1f-e0dd631cfae9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IPO Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7cf0daf2-51e1-4560-9c71-505051d1553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ipo(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"public_at\", to_date(col(\"public_at\")))\n",
    "        df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\")))\n",
    "        df = df.withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "        # Step 3: Extract and normalize\n",
    "        df = df.withColumn(\"ipo_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"ipo_object_id\", extract_id(col(\"object_id\")))\n",
    "        df = df.withColumn(\"stock_market\", extract_stock_market(col(\"stock_symbol\")))\n",
    "        df = df.withColumn(\"stock_symbol\", extract_stock_symbol(col(\"stock_symbol\")))\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 4: Select target columns\n",
    "        df_transformed = df.select(\n",
    "            col(\"ipo_id\"),\n",
    "            col(\"ipo_entity_type\"),\n",
    "            col(\"ipo_object_id\"),\n",
    "            col(\"valuation_currency_code\").alias(\"valuation_currency\"),\n",
    "            col(\"valuation_amount\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\"),\n",
    "            col(\"public_at\"),\n",
    "            col(\"stock_market\"),\n",
    "            col(\"stock_symbol\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"source_description\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Step 5: Fill null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"valuation_amount\": 0.0,\n",
    "            \"valuation_currency\": \"USD\",\n",
    "            \"raised_amount\": 0.0,\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"stock_market\": \"N/A\",\n",
    "            \"stock_symbol\": \"N/A\",\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicate IPOs\n",
    "        df_transformed = df_transformed.dropDuplicates([\"ipo_id\"])\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        # Step 7: Validasi ke dim_company dan dim_people\n",
    "        df_company_ids = transformed_company.select(\"company_object_id\", \"company_id\")\n",
    "        df_people_ids = transformed_people.select(\"people_object_id\", \"people_id\")\n",
    "\n",
    "        df_valid = (\n",
    "            df_transformed\n",
    "            .join(broadcast(df_company_ids), df_transformed[\"ipo_object_id\"] == df_company_ids[\"company_object_id\"], \"left\")\n",
    "            .join(broadcast(df_people_ids), df_transformed[\"ipo_object_id\"] == df_people_ids[\"people_object_id\"], \"left\")\n",
    "            .withColumn(\"ipo_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\")))\n",
    "            .drop(\"company_id\", \"people_id\", \"company_object_id\", \"people_object_id\")\n",
    "        )\n",
    "\n",
    "        # Step 8: Filter invalid dan logging\n",
    "        df_invalid = df_valid.filter(col(\"ipo_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"ipo_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"ipo_entity_type\", \"ipo_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"ipo\")\n",
    "            log_to_db({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"ipo\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "\n",
    "        # Validasi: pisahkan berdasarkan entitas\n",
    "        df_company_ipo = df_valid.filter(col(\"ipo_entity_type\") == \"c\") \\\n",
    "            .join(transformed_company.select(\"company_id\"), col(\"ipo_object_id\") == col(\"company_id\"), \"inner\") \\\n",
    "            .drop(\"company_id\")\n",
    "        \n",
    "        df_people_ipo = df_valid.filter(col(\"ipo_entity_type\") == \"p\") \\\n",
    "            .join(transformed_people.select(\"people_id\"), col(\"ipo_object_id\") == col(\"people_id\"), \"inner\") \\\n",
    "            .drop(\"people_id\")\n",
    "        \n",
    "        # Gabungkan kembali\n",
    "        df_final = df_company_ipo.unionByName(df_people_ipo)\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7293badc-afd4-4689-96bb-cf05e820c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "\n",
    "ipo = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"ipo\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae0ec69a-609a-4966-ad3e-f04227328f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "122 invalid IDs from table 'ipo' saved to logger DB.\n",
      "Log successfully written to database\n",
      "The data is successfully transformed\n"
     ]
    }
   ],
   "source": [
    "# Transform IPO Data\n",
    "\n",
    "transformed_ipo = transform_ipo(ipo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e2253-1d98-44c5-b241-a68fc8b5651b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Funds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4756cf26-ecb3-4ac5-acbe-fe3196a1c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, to_date, broadcast, when\n",
    "from src.utils.logger import log_to_db, save_invalid_ids\n",
    "from src.utils.helper import extract_prefix, extract_id, clean_alpha_text\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_funds(df):\n",
    "    try:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        df = df.withColumn(\"fund_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"fund_object_id\", extract_id(col(\"object_id\")).cast(IntegerType()))\n",
    "        df = df.withColumn(\"funding_date\", to_date(col(\"funded_at\")))\n",
    "\n",
    "        df_transformed = df.select(\n",
    "            col(\"fund_id\"),\n",
    "            col(\"fund_entity_type\"),\n",
    "            col(\"fund_object_id\"),\n",
    "            col(\"name\").alias(\"fund_name\"),\n",
    "            col(\"funding_date\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"source_description\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"raised_amount\": 0.0,\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        df_transformed = df_transformed.dropDuplicates([\"fund_id\"])\n",
    "        df_transformed = df_transformed.na.drop(subset=[\"funding_date\"])\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\")\n",
    "        ).withColumn(\n",
    "            \"source_description\", clean_alpha_text(\"source_description\")\n",
    "        )\n",
    "\n",
    "        log_to_db({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "        \n",
    "        return df_transformed\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_db({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86fe07f2-bef5-446c-8ef6-5f60a12ca2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "\n",
    "funds = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"funds\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "263c6a2c-03b2-4d91-b99e-b6675b19c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n",
      "Log successfully written to database\n",
      "The data is successfully transformed\n"
     ]
    }
   ],
   "source": [
    "# Transform funds Data\n",
    "\n",
    "transformed_funds = transform_funds(funds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3af385-e24b-4111-a871-2a0d953ee0c4",
   "metadata": {},
   "source": [
    "## Load to Data Warehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac5285b0-4e49-4316-82c9-ed0ec231d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_dwh(df, table_name, mode=\"overwrite\", use_upsert=False, idx_name=None, schema=None, source=None):\n",
    "\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        if not DWH_USER or not DWH_PASS:\n",
    "            raise EnvironmentError(\"DWH_USER or DWH_PASS is not set\")\n",
    "\n",
    "        if use_upsert:\n",
    "            data = df.toPandas()\n",
    "            if idx_name is None:\n",
    "                raise ValueError(\"Index name is required for upsert mode\")\n",
    "            data = data.set_index(idx_name)\n",
    "\n",
    "            conn = create_engine(f\"postgresql://{DWH_USER}:{DWH_PASS}@host.docker.internal:5432/pyspark_task_dwh\")\n",
    "            upsert(\n",
    "                con=conn,\n",
    "                df=data,\n",
    "                table_name=table_name,\n",
    "                schema=schema,\n",
    "                if_row_exists=\"update\"\n",
    "            )\n",
    "            print(f\"Data upserted to table '{table_name}' successfully!\")\n",
    "        else:\n",
    "            df.write \\\n",
    "              .format(\"jdbc\") \\\n",
    "              .option(\"url\", DWH_URL) \\\n",
    "              .option(\"dbtable\", table_name) \\\n",
    "              .option(\"user\", DWH_USER) \\\n",
    "              .option(\"password\", DWH_PASS) \\\n",
    "              .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "              .mode(mode) \\\n",
    "              .save()\n",
    "            print(f\"Data loaded to table '{table_name}' successfully!\")\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"transformed data\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to table '{table_name}': {e}\")\n",
    "        failed_data = data if use_upsert else df.toPandas()\n",
    "        failed_data['error_message'] = str(e)\n",
    "        failed_data['etl_date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"Failed\",\n",
    "            \"source\": \"transformed data\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_message\": str(e)\n",
    "        }\n",
    "\n",
    "        # failed_log_path = f'logs/failed_{table_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        # failed_data.to_csv(failed_log_path, index=False)\n",
    "        # print(f\"Failed data saved to: {failed_log_path}\")\n",
    "\n",
    "    finally:\n",
    "        log_msg.pop(\"error_message\", None)\n",
    "        log_to_db(log_msg)\n",
    "\n",
    "    return df if not use_upsert else data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5bc13-5e7f-40d6-8e90-4447c2f00e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_company\n",
    "\n",
    "load_to_dwh(transformed_company, table_name=\"dim_company\", use_upsert=True, idx_name=\"company_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c60359-1259-4beb-a787-27494cd29603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_people\n",
    "\n",
    "load_to_dwh(transformed_people, table_name=\"dim_people\", use_upsert=True, idx_name=\"people_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff164de-0af1-4e68-ad6c-382548d77bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_milestones\n",
    "\n",
    "load_to_dwh(transformed_milestones, table_name=\"dim_milestones\", use_upsert=True, idx_name=\"people_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34c5351b-d4b4-4157-94c1-094f38e8e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data upserted to table 'fact_acquisition' successfully!\n",
      "Log successfully written to database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acquiring_entity_type</th>\n",
       "      <th>acquired_entity_type</th>\n",
       "      <th>price_amount</th>\n",
       "      <th>price_currency_code</th>\n",
       "      <th>acquired_at</th>\n",
       "      <th>source_url</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>acquiring_object_id</th>\n",
       "      <th>acquired_object_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquisition_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>20000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2007-05-30</td>\n",
       "      <td>http://venturebeat.com/2007/05/30/fox-interact...</td>\n",
       "      <td>2007-05-31 22:19:54</td>\n",
       "      <td>2008-05-21 19:23:44</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>60000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2007-07-01</td>\n",
       "      <td>http://www.techcrunch.com/2007/07/02/deal-is-c...</td>\n",
       "      <td>2007-07-03 08:14:50</td>\n",
       "      <td>2011-05-06 21:51:05</td>\n",
       "      <td>59</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>280000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2007-05-01</td>\n",
       "      <td>http://www.techcrunch.com/2007/05/30/cbs-acqui...</td>\n",
       "      <td>2007-07-12 04:19:24</td>\n",
       "      <td>2008-05-19 04:48:50</td>\n",
       "      <td>24</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>100000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>http://techcrunch.com/2007/05/23/100-million-p...</td>\n",
       "      <td>2007-07-13 09:52:59</td>\n",
       "      <td>2012-06-05 03:22:17</td>\n",
       "      <td>59</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>25000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2007-07-01</td>\n",
       "      <td>http://blog.seattlepi.nwsource.com/venture/arc...</td>\n",
       "      <td>2007-07-20 05:29:07</td>\n",
       "      <td>2008-02-25 00:23:47</td>\n",
       "      <td>212</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10103</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>31000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-07-26</td>\n",
       "      <td>http://architectpartners.com/ma_alert/netgear-...</td>\n",
       "      <td>2013-10-20 13:58:14</td>\n",
       "      <td>2013-10-20 13:58:14</td>\n",
       "      <td>16675</td>\n",
       "      <td>11888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10275</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>30000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-11-10</td>\n",
       "      <td>http://techcrunch.com/2013/11/10/vox-buys-curb...</td>\n",
       "      <td>2013-11-11 03:26:04</td>\n",
       "      <td>2013-11-11 05:01:18</td>\n",
       "      <td>12906</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10427</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>200000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-12-02</td>\n",
       "      <td>http://techcrunch.com/2013/12/02/apple-buys-to...</td>\n",
       "      <td>2013-12-02 23:44:12</td>\n",
       "      <td>2013-12-02 23:44:12</td>\n",
       "      <td>1654</td>\n",
       "      <td>23588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10486</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>350000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-12-09</td>\n",
       "      <td>http://techcrunch.com/2013/12/09/verizon-confi...</td>\n",
       "      <td>2013-12-09 14:01:38</td>\n",
       "      <td>2013-12-09 14:01:38</td>\n",
       "      <td>4843</td>\n",
       "      <td>1587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10527</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>26000000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2007-07-03</td>\n",
       "      <td>http://www.blackstone.com/news-views/press-rel...</td>\n",
       "      <td>2013-12-12 10:28:16</td>\n",
       "      <td>2013-12-12 10:48:44</td>\n",
       "      <td>223792</td>\n",
       "      <td>220208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>579 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               acquiring_entity_type acquired_entity_type    price_amount  \\\n",
       "acquisition_id                                                              \n",
       "1                                  c                    c     20000000.00   \n",
       "7                                  c                    c     60000000.00   \n",
       "8                                  c                    c    280000000.00   \n",
       "9                                  c                    c    100000000.00   \n",
       "10                                 c                    c     25000000.00   \n",
       "...                              ...                  ...             ...   \n",
       "10103                              c                    c     31000000.00   \n",
       "10275                              c                    c     30000000.00   \n",
       "10427                              c                    c    200000000.00   \n",
       "10486                              c                    c    350000000.00   \n",
       "10527                              c                    c  26000000000.00   \n",
       "\n",
       "               price_currency_code acquired_at  \\\n",
       "acquisition_id                                   \n",
       "1                              USD  2007-05-30   \n",
       "7                              USD  2007-07-01   \n",
       "8                              USD  2007-05-01   \n",
       "9                              USD  2007-06-01   \n",
       "10                             USD  2007-07-01   \n",
       "...                            ...         ...   \n",
       "10103                          USD  2012-07-26   \n",
       "10275                          USD  2013-11-10   \n",
       "10427                          USD  2013-12-02   \n",
       "10486                          USD  2013-12-09   \n",
       "10527                          USD  2007-07-03   \n",
       "\n",
       "                                                       source_url  \\\n",
       "acquisition_id                                                      \n",
       "1               http://venturebeat.com/2007/05/30/fox-interact...   \n",
       "7               http://www.techcrunch.com/2007/07/02/deal-is-c...   \n",
       "8               http://www.techcrunch.com/2007/05/30/cbs-acqui...   \n",
       "9               http://techcrunch.com/2007/05/23/100-million-p...   \n",
       "10              http://blog.seattlepi.nwsource.com/venture/arc...   \n",
       "...                                                           ...   \n",
       "10103           http://architectpartners.com/ma_alert/netgear-...   \n",
       "10275           http://techcrunch.com/2013/11/10/vox-buys-curb...   \n",
       "10427           http://techcrunch.com/2013/12/02/apple-buys-to...   \n",
       "10486           http://techcrunch.com/2013/12/09/verizon-confi...   \n",
       "10527           http://www.blackstone.com/news-views/press-rel...   \n",
       "\n",
       "                        created_at          updated_at  acquiring_object_id  \\\n",
       "acquisition_id                                                                \n",
       "1              2007-05-31 22:19:54 2008-05-21 19:23:44                   11   \n",
       "7              2007-07-03 08:14:50 2011-05-06 21:51:05                   59   \n",
       "8              2007-07-12 04:19:24 2008-05-19 04:48:50                   24   \n",
       "9              2007-07-13 09:52:59 2012-06-05 03:22:17                   59   \n",
       "10             2007-07-20 05:29:07 2008-02-25 00:23:47                  212   \n",
       "...                            ...                 ...                  ...   \n",
       "10103          2013-10-20 13:58:14 2013-10-20 13:58:14                16675   \n",
       "10275          2013-11-11 03:26:04 2013-11-11 05:01:18                12906   \n",
       "10427          2013-12-02 23:44:12 2013-12-02 23:44:12                 1654   \n",
       "10486          2013-12-09 14:01:38 2013-12-09 14:01:38                 4843   \n",
       "10527          2013-12-12 10:28:16 2013-12-12 10:48:44               223792   \n",
       "\n",
       "                acquired_object_id  \n",
       "acquisition_id                      \n",
       "1                               10  \n",
       "7                               72  \n",
       "8                              132  \n",
       "9                              155  \n",
       "10                             215  \n",
       "...                            ...  \n",
       "10103                        11888  \n",
       "10275                         1254  \n",
       "10427                        23588  \n",
       "10486                         1587  \n",
       "10527                       220208  \n",
       "\n",
       "[579 rows x 10 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fact_acquisition\n",
    "\n",
    "load_to_dwh(transformed_acquisition, table_name=\"fact_acquisition\", use_upsert=True, idx_name=\"acquisition_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3816c46-a6dd-491e-9191-e645709ed216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_investments\n",
    "\n",
    "load_to_dwh(transformed_investments, table_name=\"fact_investments\", use_upsert=True, idx_name=\"investment_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "945cc2f3-6542-4c35-be17-4e270f1db2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funding_entity_type</th>\n",
       "      <th>funding_object_id</th>\n",
       "      <th>round_type</th>\n",
       "      <th>funding_date</th>\n",
       "      <th>raised_currency</th>\n",
       "      <th>raised_amount</th>\n",
       "      <th>raised_amount_usd</th>\n",
       "      <th>pre_money_currency</th>\n",
       "      <th>pre_money_valuation</th>\n",
       "      <th>pre_money_valuation_usd</th>\n",
       "      <th>...</th>\n",
       "      <th>post_money_valuation_usd</th>\n",
       "      <th>participants</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>company_object_id</th>\n",
       "      <th>people_object_id</th>\n",
       "      <th>error_message</th>\n",
       "      <th>etl_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funding_round_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>c</td>\n",
       "      <td>16</td>\n",
       "      <td>series-b</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>USD</td>\n",
       "      <td>40000000.00</td>\n",
       "      <td>40000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.socaltech.com/slacker_raises_4_m/s-...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2007-06-04 08:48:44</td>\n",
       "      <td>2008-11-20 02:59:19</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>c</td>\n",
       "      <td>33</td>\n",
       "      <td>series-a</td>\n",
       "      <td>2006-05-01</td>\n",
       "      <td>USD</td>\n",
       "      <td>2500000.00</td>\n",
       "      <td>2500000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>http://venturebeat.com/2007/03/06/widget-compa...</td>\n",
       "      <td>Widget company Clearspring says it leads marke...</td>\n",
       "      <td>2007-06-13 09:04:50</td>\n",
       "      <td>2013-09-06 11:20:07</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>c</td>\n",
       "      <td>33</td>\n",
       "      <td>series-b</td>\n",
       "      <td>2007-02-01</td>\n",
       "      <td>USD</td>\n",
       "      <td>5500000.00</td>\n",
       "      <td>5500000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>15500000.00</td>\n",
       "      <td>6</td>\n",
       "      <td>http://venturebeat.com/2007/03/06/widget-compa...</td>\n",
       "      <td>Widget company Clearspring says it leads marke...</td>\n",
       "      <td>2007-06-13 09:05:28</td>\n",
       "      <td>2013-09-06 11:20:07</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>c</td>\n",
       "      <td>34</td>\n",
       "      <td>series-a</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>USD</td>\n",
       "      <td>5000000.00</td>\n",
       "      <td>5000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.techcrunch.com/2007/06/13/openads-o...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2007-06-13 18:26:31</td>\n",
       "      <td>2013-07-18 17:28:21</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>c</td>\n",
       "      <td>39</td>\n",
       "      <td>series-c+</td>\n",
       "      <td>2007-08-01</td>\n",
       "      <td>USD</td>\n",
       "      <td>25000000.00</td>\n",
       "      <td>25000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>http://newteevee.com/2007/06/14/veoh-goes-for-...</td>\n",
       "      <td>Veoh Goes for the Big Money</td>\n",
       "      <td>2007-06-15 09:44:28</td>\n",
       "      <td>2008-06-03 19:26:18</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57859</th>\n",
       "      <td>c</td>\n",
       "      <td>19206</td>\n",
       "      <td>venture</td>\n",
       "      <td>2010-08-18</td>\n",
       "      <td>USD</td>\n",
       "      <td>3111261.00</td>\n",
       "      <td>3111261.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.sec.gov/Archives/edgar/data/1418028...</td>\n",
       "      <td>SEC</td>\n",
       "      <td>2013-12-12 07:44:35</td>\n",
       "      <td>2013-12-12 07:44:35</td>\n",
       "      <td>27385</td>\n",
       "      <td>27385</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57862</th>\n",
       "      <td>c</td>\n",
       "      <td>19206</td>\n",
       "      <td>series-b</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>USD</td>\n",
       "      <td>9009000.00</td>\n",
       "      <td>9009000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.sec.gov/Archives/edgar/data/1418028...</td>\n",
       "      <td>SEC</td>\n",
       "      <td>2013-12-12 07:48:10</td>\n",
       "      <td>2013-12-12 07:48:10</td>\n",
       "      <td>27385</td>\n",
       "      <td>27385</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57887</th>\n",
       "      <td>c</td>\n",
       "      <td>109446</td>\n",
       "      <td>other</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>USD</td>\n",
       "      <td>5000000.00</td>\n",
       "      <td>5000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.sec.gov/Archives/edgar/data/1421000...</td>\n",
       "      <td>SEC</td>\n",
       "      <td>2013-12-12 11:01:46</td>\n",
       "      <td>2013-12-12 11:01:46</td>\n",
       "      <td>238878</td>\n",
       "      <td>238878</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57908</th>\n",
       "      <td>c</td>\n",
       "      <td>105384</td>\n",
       "      <td>other</td>\n",
       "      <td>2010-04-14</td>\n",
       "      <td>USD</td>\n",
       "      <td>3475000.00</td>\n",
       "      <td>3475000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.sec.gov/Archives/edgar/data/1083672...</td>\n",
       "      <td>SEC</td>\n",
       "      <td>2013-12-12 11:50:10</td>\n",
       "      <td>2013-12-12 11:50:10</td>\n",
       "      <td>232285</td>\n",
       "      <td>232285</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57911</th>\n",
       "      <td>c</td>\n",
       "      <td>102701</td>\n",
       "      <td>venture</td>\n",
       "      <td>2010-02-02</td>\n",
       "      <td>USD</td>\n",
       "      <td>7250000.00</td>\n",
       "      <td>7250000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.sec.gov/Archives/edgar/data/1472146...</td>\n",
       "      <td>SEC</td>\n",
       "      <td>2013-12-12 12:32:38</td>\n",
       "      <td>2013-12-12 12:32:38</td>\n",
       "      <td>223767</td>\n",
       "      <td>223767</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 03:56:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16082 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 funding_entity_type funding_object_id round_type  \\\n",
       "funding_round_id                                                    \n",
       "12                                 c                16   series-b   \n",
       "27                                 c                33   series-a   \n",
       "28                                 c                33   series-b   \n",
       "31                                 c                34   series-a   \n",
       "34                                 c                39  series-c+   \n",
       "...                              ...               ...        ...   \n",
       "57859                              c             19206    venture   \n",
       "57862                              c             19206   series-b   \n",
       "57887                              c            109446      other   \n",
       "57908                              c            105384      other   \n",
       "57911                              c            102701    venture   \n",
       "\n",
       "                 funding_date raised_currency raised_amount raised_amount_usd  \\\n",
       "funding_round_id                                                                \n",
       "12                 2007-06-01             USD   40000000.00       40000000.00   \n",
       "27                 2006-05-01             USD    2500000.00        2500000.00   \n",
       "28                 2007-02-01             USD    5500000.00        5500000.00   \n",
       "31                 2007-06-01             USD    5000000.00        5000000.00   \n",
       "34                 2007-08-01             USD   25000000.00       25000000.00   \n",
       "...                       ...             ...           ...               ...   \n",
       "57859              2010-08-18             USD    3111261.00        3111261.00   \n",
       "57862              2010-03-30             USD    9009000.00        9009000.00   \n",
       "57887              2010-02-01             USD    5000000.00        5000000.00   \n",
       "57908              2010-04-14             USD    3475000.00        3475000.00   \n",
       "57911              2010-02-02             USD    7250000.00        7250000.00   \n",
       "\n",
       "                 pre_money_currency pre_money_valuation  \\\n",
       "funding_round_id                                          \n",
       "12                              USD                0.00   \n",
       "27                              USD                0.00   \n",
       "28                              USD                0.00   \n",
       "31                              USD                0.00   \n",
       "34                              USD                0.00   \n",
       "...                             ...                 ...   \n",
       "57859                           USD                0.00   \n",
       "57862                           USD                0.00   \n",
       "57887                           USD                0.00   \n",
       "57908                           USD                0.00   \n",
       "57911                           USD                0.00   \n",
       "\n",
       "                 pre_money_valuation_usd  ... post_money_valuation_usd  \\\n",
       "funding_round_id                          ...                            \n",
       "12                                  0.00  ...                     0.00   \n",
       "27                                  0.00  ...                     0.00   \n",
       "28                                  0.00  ...              15500000.00   \n",
       "31                                  0.00  ...                     0.00   \n",
       "34                                  0.00  ...                     0.00   \n",
       "...                                  ...  ...                      ...   \n",
       "57859                               0.00  ...                     0.00   \n",
       "57862                               0.00  ...                     0.00   \n",
       "57887                               0.00  ...                     0.00   \n",
       "57908                               0.00  ...                     0.00   \n",
       "57911                               0.00  ...                     0.00   \n",
       "\n",
       "                 participants  \\\n",
       "funding_round_id                \n",
       "12                          4   \n",
       "27                          2   \n",
       "28                          6   \n",
       "31                          4   \n",
       "34                          5   \n",
       "...                       ...   \n",
       "57859                       0   \n",
       "57862                       0   \n",
       "57887                       0   \n",
       "57908                       0   \n",
       "57911                       0   \n",
       "\n",
       "                                                         source_url  \\\n",
       "funding_round_id                                                      \n",
       "12                http://www.socaltech.com/slacker_raises_4_m/s-...   \n",
       "27                http://venturebeat.com/2007/03/06/widget-compa...   \n",
       "28                http://venturebeat.com/2007/03/06/widget-compa...   \n",
       "31                http://www.techcrunch.com/2007/06/13/openads-o...   \n",
       "34                http://newteevee.com/2007/06/14/veoh-goes-for-...   \n",
       "...                                                             ...   \n",
       "57859             http://www.sec.gov/Archives/edgar/data/1418028...   \n",
       "57862             http://www.sec.gov/Archives/edgar/data/1418028...   \n",
       "57887             http://www.sec.gov/Archives/edgar/data/1421000...   \n",
       "57908             http://www.sec.gov/Archives/edgar/data/1083672...   \n",
       "57911             http://www.sec.gov/Archives/edgar/data/1472146...   \n",
       "\n",
       "                                                 source_description  \\\n",
       "funding_round_id                                                      \n",
       "12                                                          Unknown   \n",
       "27                Widget company Clearspring says it leads marke...   \n",
       "28                Widget company Clearspring says it leads marke...   \n",
       "31                                                          Unknown   \n",
       "34                                      Veoh Goes for the Big Money   \n",
       "...                                                             ...   \n",
       "57859                                                           SEC   \n",
       "57862                                                           SEC   \n",
       "57887                                                           SEC   \n",
       "57908                                                           SEC   \n",
       "57911                                                           SEC   \n",
       "\n",
       "                          created_at          updated_at company_object_id  \\\n",
       "funding_round_id                                                             \n",
       "12               2007-06-04 08:48:44 2008-11-20 02:59:19                16   \n",
       "27               2007-06-13 09:04:50 2013-09-06 11:20:07                34   \n",
       "28               2007-06-13 09:05:28 2013-09-06 11:20:07                34   \n",
       "31               2007-06-13 18:26:31 2013-07-18 17:28:21                35   \n",
       "34               2007-06-15 09:44:28 2008-06-03 19:26:18                40   \n",
       "...                              ...                 ...               ...   \n",
       "57859            2013-12-12 07:44:35 2013-12-12 07:44:35             27385   \n",
       "57862            2013-12-12 07:48:10 2013-12-12 07:48:10             27385   \n",
       "57887            2013-12-12 11:01:46 2013-12-12 11:01:46            238878   \n",
       "57908            2013-12-12 11:50:10 2013-12-12 11:50:10            232285   \n",
       "57911            2013-12-12 12:32:38 2013-12-12 12:32:38            223767   \n",
       "\n",
       "                 people_object_id  \\\n",
       "funding_round_id                    \n",
       "12                             16   \n",
       "27                             34   \n",
       "28                             34   \n",
       "31                             35   \n",
       "34                             40   \n",
       "...                           ...   \n",
       "57859                       27385   \n",
       "57862                       27385   \n",
       "57887                      238878   \n",
       "57908                      232285   \n",
       "57911                      223767   \n",
       "\n",
       "                                                      error_message  \\\n",
       "funding_round_id                                                      \n",
       "12                (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "27                (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "28                (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "31                (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "34                (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "...                                                             ...   \n",
       "57859             (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "57862             (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "57887             (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "57908             (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "57911             (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "\n",
       "                             etl_date  \n",
       "funding_round_id                       \n",
       "12                2025-04-20 03:56:33  \n",
       "27                2025-04-20 03:56:33  \n",
       "28                2025-04-20 03:56:33  \n",
       "31                2025-04-20 03:56:33  \n",
       "34                2025-04-20 03:56:33  \n",
       "...                               ...  \n",
       "57859             2025-04-20 03:56:33  \n",
       "57862             2025-04-20 03:56:33  \n",
       "57887             2025-04-20 03:56:33  \n",
       "57908             2025-04-20 03:56:33  \n",
       "57911             2025-04-20 03:56:33  \n",
       "\n",
       "[16082 rows x 22 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim_funding_rounds\n",
    "\n",
    "load_to_dwh(transformed_funding_rounds, table_name=\"dim_funding_rounds\", use_upsert=True, idx_name=\"funding_round_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92958dfd-2950-466b-ab9c-ae70c8346667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log successfully written to database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_entity_type</th>\n",
       "      <th>relationship_entity_type</th>\n",
       "      <th>relationship_object_id</th>\n",
       "      <th>start_at</th>\n",
       "      <th>end_at</th>\n",
       "      <th>title</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>company_object_id</th>\n",
       "      <th>error_message</th>\n",
       "      <th>etl_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>3043</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>vp product</td>\n",
       "      <td>2008-04-10 03:53:18</td>\n",
       "      <td>2008-04-10 03:53:18</td>\n",
       "      <td>4125</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>3043</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>vp engineering</td>\n",
       "      <td>2008-04-10 03:53:18</td>\n",
       "      <td>2008-04-10 03:53:18</td>\n",
       "      <td>4125</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100262</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>91323</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>board of directors</td>\n",
       "      <td>2011-02-01 19:38:26</td>\n",
       "      <td>2011-07-14 21:14:52</td>\n",
       "      <td>32540</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100263</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>91323</td>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>None</td>\n",
       "      <td>board of directors</td>\n",
       "      <td>2011-02-01 19:38:27</td>\n",
       "      <td>2013-07-04 03:59:15</td>\n",
       "      <td>32540</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100488</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>91990</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>board member</td>\n",
       "      <td>2011-02-03 07:24:59</td>\n",
       "      <td>2013-06-26 08:17:00</td>\n",
       "      <td>67806</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99881</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>29018</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>2012-01-17</td>\n",
       "      <td>advisory board member</td>\n",
       "      <td>2011-01-29 07:45:25</td>\n",
       "      <td>2012-01-16 11:44:14</td>\n",
       "      <td>40335</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99916</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>6534</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cfo executive vp</td>\n",
       "      <td>2011-01-29 16:12:24</td>\n",
       "      <td>2013-06-07 22:42:31</td>\n",
       "      <td>7060</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99950</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>106183</td>\n",
       "      <td>2010-08-10</td>\n",
       "      <td>None</td>\n",
       "      <td>director</td>\n",
       "      <td>2011-01-30 00:03:04</td>\n",
       "      <td>2011-02-02 19:25:41</td>\n",
       "      <td>39162</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99964</th>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>330</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2011-01-30 04:12:05</td>\n",
       "      <td>2011-02-02 19:26:01</td>\n",
       "      <td>336</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>109084</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>advisor</td>\n",
       "      <td>2011-01-30 20:21:59</td>\n",
       "      <td>2011-02-01 06:10:39</td>\n",
       "      <td>64600</td>\n",
       "      <td>(psycopg2.errors.UndefinedColumn) column \"comp...</td>\n",
       "      <td>2025-04-20 04:11:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49557 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                people_entity_type relationship_entity_type  \\\n",
       "relationship_id                                               \n",
       "10009                            p                        c   \n",
       "10010                            p                        c   \n",
       "100262                           p                        c   \n",
       "100263                           p                        c   \n",
       "100488                           p                        c   \n",
       "...                            ...                      ...   \n",
       "99881                            p                        c   \n",
       "99916                            p                        c   \n",
       "99950                            p                        c   \n",
       "99964                            p                        f   \n",
       "99999                            p                        c   \n",
       "\n",
       "                relationship_object_id    start_at      end_at  \\\n",
       "relationship_id                                                  \n",
       "10009                             3043        None        None   \n",
       "10010                             3043        None        None   \n",
       "100262                           91323  2010-01-01        None   \n",
       "100263                           91323  2010-09-01        None   \n",
       "100488                           91990  2010-08-01  2012-09-01   \n",
       "...                                ...         ...         ...   \n",
       "99881                            29018  2011-01-01  2012-01-17   \n",
       "99916                             6534        None        None   \n",
       "99950                           106183  2010-08-10        None   \n",
       "99964                              330        None        None   \n",
       "99999                           109084        None        None   \n",
       "\n",
       "                                 title          created_at  \\\n",
       "relationship_id                                              \n",
       "10009                       vp product 2008-04-10 03:53:18   \n",
       "10010                   vp engineering 2008-04-10 03:53:18   \n",
       "100262              board of directors 2011-02-01 19:38:26   \n",
       "100263              board of directors 2011-02-01 19:38:27   \n",
       "100488                    board member 2011-02-03 07:24:59   \n",
       "...                                ...                 ...   \n",
       "99881            advisory board member 2011-01-29 07:45:25   \n",
       "99916                 cfo executive vp 2011-01-29 16:12:24   \n",
       "99950                         director 2011-01-30 00:03:04   \n",
       "99964                          Unknown 2011-01-30 04:12:05   \n",
       "99999                          advisor 2011-01-30 20:21:59   \n",
       "\n",
       "                         updated_at  company_object_id  \\\n",
       "relationship_id                                          \n",
       "10009           2008-04-10 03:53:18               4125   \n",
       "10010           2008-04-10 03:53:18               4125   \n",
       "100262          2011-07-14 21:14:52              32540   \n",
       "100263          2013-07-04 03:59:15              32540   \n",
       "100488          2013-06-26 08:17:00              67806   \n",
       "...                             ...                ...   \n",
       "99881           2012-01-16 11:44:14              40335   \n",
       "99916           2013-06-07 22:42:31               7060   \n",
       "99950           2011-02-02 19:25:41              39162   \n",
       "99964           2011-02-02 19:26:01                336   \n",
       "99999           2011-02-01 06:10:39              64600   \n",
       "\n",
       "                                                     error_message  \\\n",
       "relationship_id                                                      \n",
       "10009            (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "10010            (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "100262           (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "100263           (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "100488           (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "...                                                            ...   \n",
       "99881            (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "99916            (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "99950            (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "99964            (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "99999            (psycopg2.errors.UndefinedColumn) column \"comp...   \n",
       "\n",
       "                            etl_date  \n",
       "relationship_id                       \n",
       "10009            2025-04-20 04:11:25  \n",
       "10010            2025-04-20 04:11:25  \n",
       "100262           2025-04-20 04:11:25  \n",
       "100263           2025-04-20 04:11:25  \n",
       "100488           2025-04-20 04:11:25  \n",
       "...                              ...  \n",
       "99881            2025-04-20 04:11:25  \n",
       "99916            2025-04-20 04:11:25  \n",
       "99950            2025-04-20 04:11:25  \n",
       "99964            2025-04-20 04:11:25  \n",
       "99999            2025-04-20 04:11:25  \n",
       "\n",
       "[49557 rows x 11 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52852)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fact_relationship\n",
    "\n",
    "load_to_dwh(transformed_relationship, table_name=\"fact_relationship\", use_upsert=True, idx_name=\"relationship_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00193b1-4469-4da6-b8bf-02d98d4f8e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7930a65e-6eda-432e-95a6-88ed64092e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data upserted to table 'fact_ipo' successfully!\n",
      "Log successfully written to database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipo_entity_type</th>\n",
       "      <th>ipo_object_id</th>\n",
       "      <th>valuation_currency</th>\n",
       "      <th>valuation_amount</th>\n",
       "      <th>raised_currency</th>\n",
       "      <th>raised_amount</th>\n",
       "      <th>public_at</th>\n",
       "      <th>stock_market</th>\n",
       "      <th>stock_symbol</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipo_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>1624</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1980-12-19</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2008-02-09 05:17:45</td>\n",
       "      <td>2012-04-12 04:02:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>c</td>\n",
       "      <td>1045</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>65000000.00</td>\n",
       "      <td>2013-07-02</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>YUME</td>\n",
       "      <td>http://techcrunch.com/2013/07/02/yume-ipo/</td>\n",
       "      <td>Main Event Page news Comment 0 inShare9 Anothe...</td>\n",
       "      <td>2013-07-03 00:00:56</td>\n",
       "      <td>2013-07-03 00:00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>c</td>\n",
       "      <td>107162</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>128000000.00</td>\n",
       "      <td>2013-08-08</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>FOXF</td>\n",
       "      <td>http://www.bizjournals.com/sanjose/news/2013/0...</td>\n",
       "      <td>Fox raises $128M in public debut, ends day up 24%</td>\n",
       "      <td>2013-07-09 04:24:09</td>\n",
       "      <td>2013-08-09 03:18:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>c</td>\n",
       "      <td>94033</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2009-04-02</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2009-04-06 19:46:21</td>\n",
       "      <td>2009-04-06 19:46:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>c</td>\n",
       "      <td>107179</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>OTCQB</td>\n",
       "      <td>AVXL</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2013-07-09 06:08:51</td>\n",
       "      <td>2013-07-09 06:08:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>c</td>\n",
       "      <td>53630</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>80000000.00</td>\n",
       "      <td>2013-05-10</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>BIOA</td>\n",
       "      <td>http://www.biofuelsdigest.com/bdigest/2013/05/...</td>\n",
       "      <td>BioAmber completes IPO</td>\n",
       "      <td>2013-06-21 04:22:05</td>\n",
       "      <td>2013-06-21 04:22:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>c</td>\n",
       "      <td>17538</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>947000000.00</td>\n",
       "      <td>2013-05-09</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>Q</td>\n",
       "      <td>http://uk.reuters.com/article/2013/05/09/us-qu...</td>\n",
       "      <td>Quintiles IPO raises more-than-planned $947 mi...</td>\n",
       "      <td>2013-06-21 04:40:55</td>\n",
       "      <td>2013-06-21 04:48:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>c</td>\n",
       "      <td>31572</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>ECTE</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2013-06-21 06:31:30</td>\n",
       "      <td>2013-06-21 06:31:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>c</td>\n",
       "      <td>82649</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>RAD</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2013-06-22 03:55:00</td>\n",
       "      <td>2013-06-22 03:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>c</td>\n",
       "      <td>105879</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>TLV</td>\n",
       "      <td>NSVX</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2013-07-02 03:18:41</td>\n",
       "      <td>2013-07-02 03:18:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ipo_entity_type ipo_object_id valuation_currency valuation_amount  \\\n",
       "ipo_id                                                                     \n",
       "1                    c          1624                USD             0.00   \n",
       "1002                 c          1045                USD             0.00   \n",
       "1009                 c        107162                USD             0.00   \n",
       "101                  c         94033                USD             0.00   \n",
       "1011                 c        107179                USD             0.00   \n",
       "...                ...           ...                ...              ...   \n",
       "981                  c         53630                USD             0.00   \n",
       "982                  c         17538                USD             0.00   \n",
       "984                  c         31572                USD             0.00   \n",
       "988                  c         82649                USD             0.00   \n",
       "999                  c        105879                USD             0.00   \n",
       "\n",
       "       raised_currency raised_amount   public_at stock_market stock_symbol  \\\n",
       "ipo_id                                                                       \n",
       "1                  USD          0.00  1980-12-19       NASDAQ         AAPL   \n",
       "1002               USD   65000000.00  2013-07-02         NYSE         YUME   \n",
       "1009               USD  128000000.00  2013-08-08       NASDAQ         FOXF   \n",
       "101                USD          0.00  2009-04-02          N/A          N/A   \n",
       "1011               USD          0.00        None        OTCQB         AVXL   \n",
       "...                ...           ...         ...          ...          ...   \n",
       "981                USD   80000000.00  2013-05-10         NYSE         BIOA   \n",
       "982                USD  947000000.00  2013-05-09         NYSE            Q   \n",
       "984                USD          0.00        None       NASDAQ         ECTE   \n",
       "988                USD          0.00        None         NYSE          RAD   \n",
       "999                USD          0.00        None          TLV         NSVX   \n",
       "\n",
       "                                               source_url  \\\n",
       "ipo_id                                                      \n",
       "1                                                 Unknown   \n",
       "1002           http://techcrunch.com/2013/07/02/yume-ipo/   \n",
       "1009    http://www.bizjournals.com/sanjose/news/2013/0...   \n",
       "101                                               Unknown   \n",
       "1011                                              Unknown   \n",
       "...                                                   ...   \n",
       "981     http://www.biofuelsdigest.com/bdigest/2013/05/...   \n",
       "982     http://uk.reuters.com/article/2013/05/09/us-qu...   \n",
       "984                                               Unknown   \n",
       "988                                               Unknown   \n",
       "999                                               Unknown   \n",
       "\n",
       "                                       source_description          created_at  \\\n",
       "ipo_id                                                                          \n",
       "1                                                 Unknown 2008-02-09 05:17:45   \n",
       "1002    Main Event Page news Comment 0 inShare9 Anothe... 2013-07-03 00:00:56   \n",
       "1009    Fox raises $128M in public debut, ends day up 24% 2013-07-09 04:24:09   \n",
       "101                                               Unknown 2009-04-06 19:46:21   \n",
       "1011                                              Unknown 2013-07-09 06:08:51   \n",
       "...                                                   ...                 ...   \n",
       "981                                BioAmber completes IPO 2013-06-21 04:22:05   \n",
       "982     Quintiles IPO raises more-than-planned $947 mi... 2013-06-21 04:40:55   \n",
       "984                                               Unknown 2013-06-21 06:31:30   \n",
       "988                                               Unknown 2013-06-22 03:55:00   \n",
       "999                                               Unknown 2013-07-02 03:18:41   \n",
       "\n",
       "                updated_at  \n",
       "ipo_id                      \n",
       "1      2012-04-12 04:02:59  \n",
       "1002   2013-07-03 00:00:56  \n",
       "1009   2013-08-09 03:18:34  \n",
       "101    2009-04-06 19:46:21  \n",
       "1011   2013-07-09 06:08:51  \n",
       "...                    ...  \n",
       "981    2013-06-21 04:22:05  \n",
       "982    2013-06-21 04:48:52  \n",
       "984    2013-06-21 06:31:30  \n",
       "988    2013-06-22 03:55:00  \n",
       "999    2013-07-02 03:18:41  \n",
       "\n",
       "[449 rows x 13 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fact_ipo\n",
    "\n",
    "load_to_dwh(transformed_ipo, table_name=\"fact_ipo\", use_upsert=True, idx_name=\"ipo_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f39522d4-f72a-48ca-8fc9-14559382f12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data upserted to table 'dim_funds' successfully!\n",
      "Log successfully written to database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fund_entity_type</th>\n",
       "      <th>fund_object_id</th>\n",
       "      <th>fund_name</th>\n",
       "      <th>funding_date</th>\n",
       "      <th>raised_currency</th>\n",
       "      <th>raised_amount</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fund_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>371</td>\n",
       "      <td>Second Fund</td>\n",
       "      <td>2008-12-16</td>\n",
       "      <td>USD</td>\n",
       "      <td>300000000.00</td>\n",
       "      <td>http://www.pehub.com/26194/dfj-dragon-raising-...</td>\n",
       "      <td>peHub</td>\n",
       "      <td>2008-12-17 03:07:16</td>\n",
       "      <td>2008-12-17 03:07:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>f</td>\n",
       "      <td>260</td>\n",
       "      <td>TA XI</td>\n",
       "      <td>2009-08-12</td>\n",
       "      <td>USD</td>\n",
       "      <td>4000000000.00</td>\n",
       "      <td>http://www.preqin.com/item/ta-associates-close...</td>\n",
       "      <td>TA Associates Closes 4B Fund</td>\n",
       "      <td>2009-11-02 22:16:07</td>\n",
       "      <td>2009-11-02 22:16:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>f</td>\n",
       "      <td>10437</td>\n",
       "      <td>Lightspeed China Partners I</td>\n",
       "      <td>2013-01-29</td>\n",
       "      <td>USD</td>\n",
       "      <td>168000000.00</td>\n",
       "      <td>http://www.finsmes.com/2013/01/lightspeed-chin...</td>\n",
       "      <td>Lightspeed China Partners Closes 168M Fund</td>\n",
       "      <td>2013-01-29 02:32:22</td>\n",
       "      <td>2013-01-29 02:32:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>f</td>\n",
       "      <td>8154</td>\n",
       "      <td>Serent Capital II</td>\n",
       "      <td>2013-01-30</td>\n",
       "      <td>USD</td>\n",
       "      <td>350000000.00</td>\n",
       "      <td>http://www.finsmes.com/2013/01/serent-capital-...</td>\n",
       "      <td>Serent Capital Closes Second Fund at 350M</td>\n",
       "      <td>2013-01-31 02:50:10</td>\n",
       "      <td>2013-01-31 02:50:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>f</td>\n",
       "      <td>1866</td>\n",
       "      <td>Cleantech Fund II</td>\n",
       "      <td>2013-01-30</td>\n",
       "      <td>USD</td>\n",
       "      <td>74000000.00</td>\n",
       "      <td>http://pevc.dowjones.com/article?an=DJFVW00020...</td>\n",
       "      <td>Israel Cleantech Ventures Closes 74M Second Fund</td>\n",
       "      <td>2013-01-31 03:41:42</td>\n",
       "      <td>2013-01-31 03:41:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>f</td>\n",
       "      <td>13</td>\n",
       "      <td>Redpoint V</td>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>USD</td>\n",
       "      <td>400000000.00</td>\n",
       "      <td>http://techcrunch.com/2013/01/16/redpoint-vent...</td>\n",
       "      <td>Redpoint Ventures Closes On 400 Million For It...</td>\n",
       "      <td>2013-01-17 02:37:25</td>\n",
       "      <td>2013-01-17 02:57:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>f</td>\n",
       "      <td>9467</td>\n",
       "      <td>Disruptive Innovation Fund</td>\n",
       "      <td>2007-08-01</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2013-01-18 06:39:20</td>\n",
       "      <td>2013-01-21 03:14:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>f</td>\n",
       "      <td>9913</td>\n",
       "      <td>Venture capital fund</td>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>USD</td>\n",
       "      <td>100000000.00</td>\n",
       "      <td>http://venturebeat.com/2013/01/22/ribbit-capit...</td>\n",
       "      <td>Ribbit Capital lands 100M fund for financefocu...</td>\n",
       "      <td>2013-01-23 03:38:54</td>\n",
       "      <td>2013-01-23 03:40:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>f</td>\n",
       "      <td>929</td>\n",
       "      <td>Prime Ventures IV</td>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>EUR</td>\n",
       "      <td>100000000.00</td>\n",
       "      <td>http://www.finsmes.com/2013/01/prime-ventures-...</td>\n",
       "      <td>Prime Ventures Holds First Close VC Fund IV at...</td>\n",
       "      <td>2013-01-25 11:01:18</td>\n",
       "      <td>2013-01-25 11:01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>f</td>\n",
       "      <td>6776</td>\n",
       "      <td>TEXO Ventures I, LP</td>\n",
       "      <td>2012-01-24</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2013-01-28 05:12:53</td>\n",
       "      <td>2013-01-28 07:07:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1381 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fund_entity_type  fund_object_id                    fund_name  \\\n",
       "fund_id                                                                 \n",
       "1                      f             371                  Second Fund   \n",
       "100                    f             260                        TA XI   \n",
       "1002                   f           10437  Lightspeed China Partners I   \n",
       "1005                   f            8154            Serent Capital II   \n",
       "1006                   f            1866            Cleantech Fund II   \n",
       "...                  ...             ...                          ...   \n",
       "991                    f              13                   Redpoint V   \n",
       "992                    f            9467   Disruptive Innovation Fund   \n",
       "993                    f            9913         Venture capital fund   \n",
       "994                    f             929            Prime Ventures IV   \n",
       "996                    f            6776          TEXO Ventures I, LP   \n",
       "\n",
       "        funding_date raised_currency  raised_amount  \\\n",
       "fund_id                                               \n",
       "1         2008-12-16             USD   300000000.00   \n",
       "100       2009-08-12             USD  4000000000.00   \n",
       "1002      2013-01-29             USD   168000000.00   \n",
       "1005      2013-01-30             USD   350000000.00   \n",
       "1006      2013-01-30             USD    74000000.00   \n",
       "...              ...             ...            ...   \n",
       "991       2013-01-16             USD   400000000.00   \n",
       "992       2007-08-01             USD           0.00   \n",
       "993       2013-01-23             USD   100000000.00   \n",
       "994       2013-01-24             EUR   100000000.00   \n",
       "996       2012-01-24             USD           0.00   \n",
       "\n",
       "                                                source_url  \\\n",
       "fund_id                                                      \n",
       "1        http://www.pehub.com/26194/dfj-dragon-raising-...   \n",
       "100      http://www.preqin.com/item/ta-associates-close...   \n",
       "1002     http://www.finsmes.com/2013/01/lightspeed-chin...   \n",
       "1005     http://www.finsmes.com/2013/01/serent-capital-...   \n",
       "1006     http://pevc.dowjones.com/article?an=DJFVW00020...   \n",
       "...                                                    ...   \n",
       "991      http://techcrunch.com/2013/01/16/redpoint-vent...   \n",
       "992                                                Unknown   \n",
       "993      http://venturebeat.com/2013/01/22/ribbit-capit...   \n",
       "994      http://www.finsmes.com/2013/01/prime-ventures-...   \n",
       "996                                                Unknown   \n",
       "\n",
       "                                        source_description  \\\n",
       "fund_id                                                      \n",
       "1                                                    peHub   \n",
       "100                           TA Associates Closes 4B Fund   \n",
       "1002            Lightspeed China Partners Closes 168M Fund   \n",
       "1005             Serent Capital Closes Second Fund at 350M   \n",
       "1006      Israel Cleantech Ventures Closes 74M Second Fund   \n",
       "...                                                    ...   \n",
       "991      Redpoint Ventures Closes On 400 Million For It...   \n",
       "992                                                Unknown   \n",
       "993      Ribbit Capital lands 100M fund for financefocu...   \n",
       "994      Prime Ventures Holds First Close VC Fund IV at...   \n",
       "996                                                Unknown   \n",
       "\n",
       "                 created_at          updated_at  \n",
       "fund_id                                          \n",
       "1       2008-12-17 03:07:16 2008-12-17 03:07:16  \n",
       "100     2009-11-02 22:16:07 2009-11-02 22:16:07  \n",
       "1002    2013-01-29 02:32:22 2013-01-29 02:32:22  \n",
       "1005    2013-01-31 02:50:10 2013-01-31 02:50:10  \n",
       "1006    2013-01-31 03:41:42 2013-01-31 03:41:42  \n",
       "...                     ...                 ...  \n",
       "991     2013-01-17 02:37:25 2013-01-17 02:57:59  \n",
       "992     2013-01-18 06:39:20 2013-01-21 03:14:34  \n",
       "993     2013-01-23 03:38:54 2013-01-23 03:40:52  \n",
       "994     2013-01-25 11:01:18 2013-01-25 11:01:23  \n",
       "996     2013-01-28 05:12:53 2013-01-28 07:07:43  \n",
       "\n",
       "[1381 rows x 10 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim_funds\n",
    "\n",
    "load_to_dwh(transformed_funds, table_name=\"dim_funds\", use_upsert=True, idx_name=\"fund_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3b244-e02c-4816-9920-9ef0b9717ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
