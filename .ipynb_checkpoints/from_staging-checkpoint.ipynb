{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f9b6cd-7c63-4d7c-a963-25952b0ca934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eace4aee-ea80-400b-af40-bbf677b5cb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check pyspark version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027ca5ff-3f8d-4fc2-b52a-7da84338ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1d6a71-4fac-4f99-9ea4-f2f7e2d73fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Final Project PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4d9054-9107-42d6-a0a0-e049b9319e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Final Project PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcc11fa6510>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add16a43-963c-420d-8b5e-e8c18d934667",
   "metadata": {},
   "source": [
    "## Load and Handle Failure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d94b859-9d4d-483a-b6af-5dfcdf168376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54467c7-078d-466a-91d8-57af1aac710c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d67139-17b1-4fdc-aaa1-a5f9a5fc15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12c56d6-bb72-4a25-984a-b30b66d7ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT LOGS\n",
    "def log_to_csv(log_msg: dict, filename: str):\n",
    "    # Ensure the 'logs' directory exists\n",
    "    log_dir = os.path.join(os.getcwd(), 'logs')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Create the full file path inside 'logs'\n",
    "    file_path = os.path.join(log_dir, filename)\n",
    "\n",
    "    # Define the column headers\n",
    "    headers = [\"step\", \"status\", \"source\", \"table_name\", \"etl_date\"]\n",
    "\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        file_exists = os.path.isfile(file_path)\n",
    "\n",
    "        with open(file_path, mode='a', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "\n",
    "            # Write the header only if the file doesn't exist\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Append the log message\n",
    "            writer.writerow(log_msg)\n",
    "\n",
    "        print(f\"Log written to {file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing log to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58919bd4-a86e-46ed-a849-bf236d617ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for database\n",
    "\n",
    "DB_URL = os.getenv(\"DB_URL\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e095748-b6b8-4e62-b2f4-980a2db88053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ac65e0-9c28-4a03-a51e-96fe5b481438",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84df23db-7cc2-4706-9c7e-467bf86376b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pangres in /opt/conda/lib/python3.11/site-packages (4.2.1)\n",
      "Requirement already satisfied: pandas>=0.25.3 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.12 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.22)\n",
      "Requirement already satisfied: alembic>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from pangres) (1.12.0)\n",
      "Requirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.11/site-packages (from pangres) (23.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (1.24.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.3.12->pangres) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->pangres) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.3.1->pangres) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pangres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b33ca6-54a8-45e6-96ea-241d6f62e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for database\n",
    "\n",
    "DB_STAGING_URL = os.getenv(\"DB_STAGING_URL\")\n",
    "DB_STAGING_USER = os.getenv(\"DB_STAGING_USER\")\n",
    "DB_STAGING_PASS = os.getenv(\"DB_STAGING_PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771dc6c-3890-4437-8cbb-215f4e0aab11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract Data from Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f576786a-b8bb-43b0-9740-3c34d1311910",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f7905f-e399-4aa4-ae59-7a0b1713abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for staging\n",
    "\n",
    "DB_STAGING_URL = os.getenv(\"DB_STAGING_URL\")\n",
    "DB_STAGING_USER = os.getenv(\"DB_STAGING_USER\")\n",
    "DB_STAGING_PASS = os.getenv(\"DB_STAGING_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebcd5088-00c6-40a8-b4f2-5ddedf46c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_staging():\n",
    "    try:\n",
    "        # Get list of tables from staging\n",
    "        table_list = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DB_STAGING_URL) \\\n",
    "            .option(\"dbtable\", \"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public') AS tbl\") \\\n",
    "            .option(\"user\", DB_STAGING_USER) \\\n",
    "            .option(\"password\", DB_STAGING_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"table_name\") \\\n",
    "            .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print(f\"Found tables in staging: {table_list}\")\n",
    "\n",
    "        tables = {}\n",
    "        for table in table_list:\n",
    "            try:\n",
    "                # Read each table into a DataFrame\n",
    "                df = spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", DB_STAGING_URL) \\\n",
    "                    .option(\"dbtable\", table) \\\n",
    "                    .option(\"user\", DB_STAGING_USER) \\\n",
    "                    .option(\"password\", DB_STAGING_PASS) \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()\n",
    "\n",
    "                tables[table] = df\n",
    "\n",
    "                # Log success for each table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": \"Success\",\n",
    "                    \"source\": \"PostgreSQL (Staging)\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "\n",
    "                print(f\"Successfully extracted table: {table}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log failure for specific table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": f\"Failed: {e}\",\n",
    "                    \"source\": \"PostgreSQL (Staging)\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "                print(f\"Failed to extract table: {table} - Error: {e}\")\n",
    "\n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure for the whole extraction process\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"PostgreSQL (Staging)\",\n",
    "            \"table_name\": \"N/A\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        print(f\"Failed to extract tables: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0eee4a-955a-482b-bec9-eb7fa9ab5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tables from staging\n",
    "data = extract_from_staging()\n",
    "print(f\"Extracted tables: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb0c38-f907-470a-8242-80a13d02ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read All data from staging\n",
    "acquisition = data[\"acquisition\"]\n",
    "company = data[\"company\"]\n",
    "funding_rounds = data[\"funding_rounds\"]\n",
    "funds = data[\"funds\"]\n",
    "investments = data[\"investments\"]\n",
    "ipos = data[\"ipo\"]\n",
    "milestones = data[\"milestones\"]\n",
    "people = data[\"people\"]\n",
    "relationship = data[\"relationship\"]\n",
    "\n",
    "# check\n",
    "company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d6181-6ec3-4c5d-8dd7-83623e0ebd1a",
   "metadata": {},
   "source": [
    "## Data Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845db9a-96db-420a-b2e0-cbdbd30533a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e38fdd-8ff5-4ca5-9d28-385f85826723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "from decimal import Decimal\n",
    "\n",
    "# Helper function buat konversi tipe data ke JSON-compatible\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        return obj.isoformat()\n",
    "    return obj\n",
    "\n",
    "def profile_data(person, df, table_name, format_file):\n",
    "    try:\n",
    "        n_rows = df.count()\n",
    "        n_cols = len(df.columns)\n",
    "        \n",
    "        column_info = {}\n",
    "        for col in df.columns:\n",
    "            data_type = df.schema[col].dataType.simpleString()\n",
    "            sample_values = df.select(col).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "            null_count = df.filter(df[col].isNull()).count()\n",
    "            unique_count = df.select(col).distinct().count()\n",
    "            \n",
    "            # Min and max values (if numeric or date type)\n",
    "            try:\n",
    "                min_value = df.agg({col: \"min\"}).collect()[0][0]\n",
    "                max_value = df.agg({col: \"max\"}).collect()[0][0]\n",
    "            except:\n",
    "                min_value = None\n",
    "                max_value = None\n",
    "            \n",
    "            # Persentase missing value\n",
    "            percentage_missing = round((null_count / n_rows) * 100, 2) if n_rows > 0 else 0.0\n",
    "            \n",
    "            # Ambil 5 nilai unik sebagai sampel\n",
    "            unique_values = df.select(col).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "            \n",
    "            # Persentase valid date (khusus untuk tipe date dan datetime)\n",
    "            percentage_valid_date = None\n",
    "            if data_type in ['date', 'timestamp']:\n",
    "                valid_date_count = df.filter(df[col].isNotNull()).count()\n",
    "                percentage_valid_date = round((valid_date_count / n_rows) * 100, 2) if n_rows > 0 else 0.0\n",
    "\n",
    "            column_info[col] = {\n",
    "                \"data_type\": data_type,\n",
    "                \"sample_values\": [convert_to_serializable(v) for v in sample_values] if sample_values else None,\n",
    "                \"unique_count\": unique_count,\n",
    "                \"unique_value\": [convert_to_serializable(v) for v in unique_values] if unique_values else None,\n",
    "                \"null_count\": null_count,\n",
    "                \"percentage_missing_value\": percentage_missing,\n",
    "                \"min_value\": convert_to_serializable(min_value),\n",
    "                \"max_value\": convert_to_serializable(max_value),\n",
    "                \"percentage_valid_date\": percentage_valid_date\n",
    "            }\n",
    "        \n",
    "        dict_profiling = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"person_in_charge\": person,\n",
    "            \"profiling_result\": {\n",
    "                \"table_name\": table_name,\n",
    "                \"format_file\": format_file,\n",
    "                \"n_rows\": n_rows,\n",
    "                \"n_cols\": n_cols,\n",
    "                \"report\": column_info\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save profiling result to JSON\n",
    "        folder_path = \"data_profiling\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        file_path = os.path.join(folder_path, f\"{table_name}_profiling.json\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(dict_profiling, f, indent=4, default=convert_to_serializable)\n",
    "\n",
    "        print(f\"Profiling saved to: {file_path}\")\n",
    "\n",
    "        # Create success log message\n",
    "        log_msg = {\n",
    "            \"step\": \"Profiling\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": format_file,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error profiling table {table_name}: {e}\")\n",
    "\n",
    "        # Create fail log message\n",
    "        log_msg = {\n",
    "            \"step\": \"Profiling\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": format_file,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        # Save log to CSV\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "\n",
    "    return dict_profiling if 'dict_profiling' in locals() else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ec5e0-6c82-4871-9bd0-1e89098e1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1\n",
    "profiling_result = profile_data(\"Mr. A\", people, \"people_data\", \"from Staging\")\n",
    "print(json.dumps(profiling_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc775e70-e9da-46d1-8042-68d886be2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_data(\"Mr. CCC\", relationship, \"relationship_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. H\", acquisition, \"acquisition_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. OP\", company, \"company_data\", \"from Staging\")\n",
    "profile_data(\"Mr. CCC\", funding_rounds, \"funding_rounds_data\", \"from Staging\")\n",
    "profile_data(\"Mr. A\", funds, \"funds_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. H\", investments, \"investments_data\", \"from Staging\")\n",
    "profile_data(\"Mr. A\", ipos, \"ipos_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. OP\", milestones, \"milestones_data\", \"from Staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30101ee8-ef65-4765-a403-78358ae5848d",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06437275-5a17-41f5-951d-dd3f78cf14d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.11/site-packages (1.3.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e100f02-b3c0-468a-a147-ce0ac528e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, to_date, when, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from sqlalchemy import create_engine\n",
    "from pangres import upsert\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f7c70e4-45d7-4330-802a-3c3d40a0a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection to dwh\n",
    "DWH_URL = os.getenv(\"DWH_URL\")\n",
    "DWH_USER = os.getenv(\"DWH_USER\")\n",
    "DWH_PASS = os.getenv(\"DWH_PASS\")\n",
    "engine = create_engine(f\"postgresql://{DWH_USER}:{DWH_PASS}@host.docker.internal:5432/pyspark_task_dwh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb46c96b-ba1f-4549-ba34-4ea100a26b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting integer value from format \"<letter>:<number>\"\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def clean_integer(value):\n",
    "    if isinstance(value, str):\n",
    "        match = re.match(r\"^[a-zA-Z]:(\\d+)\", value) \n",
    "        if match:\n",
    "            return int(match.group(1))  # Catch value after \":\"\n",
    "        else:\n",
    "            return None \n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c7613ca-6d7e-4cda-ae06-40ef8b3fb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def clean_text(value):\n",
    "    if value:\n",
    "        try:\n",
    "            # Handle encoding issue \n",
    "            value = value.encode('latin1').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            pass\n",
    "        # Normalization\n",
    "        value = unicodedata.normalize(\"NFKD\", value)\n",
    "        # Handle strange character\n",
    "        value = re.sub(r'[^\\x00-\\x7F]+', '', value)\n",
    "        value = value.strip()\n",
    "        value = unidecode(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d41ca7-6850-4efe-97b9-689d34e138f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for extracting prefix and numeric ID\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_prefix(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[0]\n",
    "    return None\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def extract_id(value):\n",
    "    if value and \":\" in value:\n",
    "        try:\n",
    "            return int(value.split(\":\")[1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5caa8ad0-7503-44d3-994f-a385d8367e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def save_invalid_ids(invalid_ids, table_name, folder='logs', filename='invalid_ids.csv'):\n",
    "    if not invalid_ids:\n",
    "        print(f\"No invalid IDs to save from table '{table_name}'.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        \n",
    "        # Konversi list ke DataFrame\n",
    "        df = pd.DataFrame(invalid_ids, columns=['entity_type', 'object_id'])\n",
    "        df['table_name'] = table_name\n",
    "        df['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Kalau file sudah ada, cek duplikasi biar gak nambah terus\n",
    "        if os.path.exists(file_path):\n",
    "            existing_df = pd.read_csv(file_path)\n",
    "            # Gabung dataframe lalu drop duplikat\n",
    "            df = pd.concat([existing_df, df]).drop_duplicates(subset=['entity_type', 'object_id', 'table_name'])\n",
    "        \n",
    "        # Tulis ulang ke file (bukan append) biar data tetap konsisten\n",
    "        df.to_csv(file_path, mode='w', index=False)\n",
    "        \n",
    "        print(f\"{len(invalid_ids)} invalid IDs from table '{table_name}' saved to {file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving invalid IDs from table '{table_name}': {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a7d4086-6b84-489c-be8f-efa1e5a38d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def normalize_text(value):\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return None  # Jika kosong atau bukan string, kembalikan None\n",
    "    \n",
    "    # Ubah jadi lowercase\n",
    "    value = value.lower()\n",
    "    \n",
    "    # Hapus karakter aneh seperti kutip, tanda baca yang tidak diperlukan\n",
    "    value = re.sub(r'[^\\w\\s,&/]', '', value)  # Hanya biarkan huruf, angka, spasi, koma, ampersand, dan slash\n",
    "    \n",
    "    # Ganti beberapa simbol yang sering dipakai sebagai pemisah menjadi spasi\n",
    "    value = re.sub(r'[/,&]', ' ', value)  # Contoh: \"Designer, UX, UI\" → \"designer ux ui\"\n",
    "    \n",
    "    # Hapus spasi berlebih\n",
    "    value = re.sub(r'\\s+', ' ', value).strip()\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed87da76-e46d-4e64-98a7-b067ed85d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def clean_alpha_text(text):\n",
    "    if text:\n",
    "        # Hapus simbol dan karakter aneh, kecuali alfanumerik dan spasi\n",
    "        return re.sub(r'[^\\w\\s]', '', text).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "190d7268-cb5e-450c-843b-6223adc22246",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def fix_encoding(s):\n",
    "    if s is not None:\n",
    "        try:\n",
    "            return unidecode(s)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76001f21-30c6-4632-b814-7706967c6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def extract_stock_market(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[0]\n",
    "    return None\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_stock_symbol(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a70d9-0c03-4e33-abfa-345a0e1fa4ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Company Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e2e66-ba21-46ff-89c9-2cf2e207c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, split\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_company(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"latitude\", col(\"latitude\").cast(\"decimal(9,6)\"))\n",
    "        df = df.withColumn(\"longitude\", col(\"longitude\").cast(\"decimal(9,6)\"))\n",
    "        \n",
    "        # Extract prefix and ID \n",
    "        df = df.withColumn(\"entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "\n",
    "        # Step 3: Encoding\n",
    "        df = df.withColumn(\"description\", clean_text(\"description\"))\n",
    "        df = df.withColumn(\"address1\", clean_text(\"address1\"))\n",
    "        df = df.withColumn(\"zip_code\", clean_text(\"zip_code\"))\n",
    "        df = df.withColumn(\"region\", clean_text(\"region\"))\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping ke target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"office_id\").alias(\"company_id\"),\n",
    "            col(\"entity_type\").alias(\"entity_type\"),\n",
    "            col(\"object_id\").alias(\"object_id\"),  # INT\n",
    "            col(\"description\").alias(\"description\"),\n",
    "            col(\"address1\").alias(\"address\"),\n",
    "            col(\"region\").alias(\"region\"),\n",
    "            col(\"city\").alias(\"city\"),\n",
    "            col(\"zip_code\").alias(\"zip_code\"),\n",
    "            col(\"state_code\").alias(\"state_code\"),\n",
    "            col(\"country_code\").alias(\"country_code\"),\n",
    "            col(\"latitude\").alias(\"latitude\"),\n",
    "            col(\"longitude\").alias(\"longitude\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 5: Data cleansing \n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"description\": \"Unknown\",\n",
    "            \"address\": \"Unknown\",\n",
    "            \"region\": \"Unknown\",\n",
    "            \"city\": \"Unknown\",\n",
    "            \"zip_code\": \"Unknown\",\n",
    "            \"state_code\": \"Unknown\",\n",
    "            \"country_code\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicate data dan value dengan latitude/longitude = 0\n",
    "        df_transformed = df_transformed.dropDuplicates([\"object_id\"])\n",
    "        df_transformed = df_transformed.filter((col(\"latitude\") != 0) & (col(\"longitude\") != 0))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 7: Convert to pandas to upsert to DWH\n",
    "        df_pandas = df_transformed.toPandas().set_index(\"company_id\")\n",
    "        df_pandas.index.name = \"company_id\"\n",
    "\n",
    "        # Step 8: Load to DWH via upsert\n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"dim_company\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"dim_company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"ETL Process\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"dim_company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66430e63-a5aa-4efb-8a6b-1abe69c12111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "company = spark.read \\\n",
    "       .format(\"jdbc\") \\\n",
    "       .option(\"url\", DB_STAGING_URL) \\\n",
    "       .option(\"dbtable\", \"company\") \\\n",
    "       .option(\"user\", DB_STAGING_USER) \\\n",
    "       .option(\"password\", DB_STAGING_PASS) \\\n",
    "       .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "       .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e88e0-b11c-4636-a509-fc0fe35f8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Company data\n",
    "transform_company(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf466ce-b00b-41dc-8b08-ad89a8c75edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dad63-d81a-4511-8f0c-de494cf4ef85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ca2ee1-33d8-41b2-bbc2-63b2e66f2a6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### People Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784dc83-d28e-4fbd-842e-903e0ac6db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, broadcast\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def transform_people(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace empty strings dengan NULL\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Buat kolom full_name dari first_name + last_name\n",
    "        df = df.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "        # Step 3: Ekstraksi prefix dan ID dari object_id\n",
    "        df = df.withColumn(\"entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping ke target kolom\n",
    "        df_transformed = df.select(\n",
    "            col(\"people_id\").alias(\"people_id\"),\n",
    "            col(\"entity_type\").alias(\"entity_type\"), \n",
    "            col(\"object_id\").alias(\"object_id\"),\n",
    "            col(\"full_name\").alias(\"full_name\"),\n",
    "            col(\"birthplace\").alias(\"birthplace\"),\n",
    "            col(\"affiliation_name\").alias(\"affiliation_name\"),\n",
    "        )\n",
    "\n",
    "        # Step 5: Bersihkan data dengan UDF\n",
    "        df_transformed = df_transformed.withColumn(\"object_id\", clean_integer(col(\"object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"full_name\", clean_alpha_text(col(\"full_name\")))\n",
    "        df_transformed = df_transformed.withColumn(\"birthplace\", fix_encoding(col(\"birthplace\")))\n",
    "        df_transformed = df_transformed.withColumn(\"affiliation_name\", clean_alpha_text(col(\"affiliation_name\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 6: Cleaning data\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"full_name\": \"Unknown\",\n",
    "            \"birthplace\": \"Unknown\",\n",
    "            \"affiliation_name\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 7: Drop duplicate data berdasarkan `entity_type` dan `object_id`\n",
    "        df_transformed = df_transformed.dropDuplicates([\"entity_type\", \"object_id\"])\n",
    "\n",
    "        # 🚨 Drop data dengan `full_name` = \"Unknown\"\n",
    "        df_transformed = df_transformed.filter(col(\"full_name\") != \"Unknown\")\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 8: Validasi object_id di `dim_company`\n",
    "        dim_company = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"object_id\")\n",
    "\n",
    "        # 🟢 Data yang valid (match dengan `dim_company`)\n",
    "        df_valid = df_transformed.join(\n",
    "            broadcast(dim_company),\n",
    "            on=\"object_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "        # 🔴 Data yang tidak valid (object_id tidak match)\n",
    "        df_invalid = df_transformed.join(\n",
    "            broadcast(dim_company),\n",
    "            on=\"object_id\",\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            # Convert ke pandas dan simpan dengan `save_invalid_ids`\n",
    "            invalid_ids = df_invalid.select(\"entity_type\", \"object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"people\")\n",
    "\n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"people\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 9: Convert ke pandas untuk upsert ke DWH\n",
    "        df_pandas = df_valid.toPandas().set_index(\"people_id\")\n",
    "        df_pandas.index.name = \"people_id\"\n",
    "\n",
    "        # Step 10: Load to DWH (via upsert)\n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"dim_people\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"dim_people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"ETL Process\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"dim_people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd68c42-2751-4a1c-bd20-df3c7ff9f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "people = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"people\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcddea8b-9ce3-4f17-a843-2c2ac76d1cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform People Data\n",
    "transform_people(people)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ee0d6-5e91-4384-bae9-d75b94212867",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Milestones Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5374d53-131b-4829-9f6e-22631fd8f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_milestones(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "        df = df.na.replace(\"NaN\", None)\n",
    "        \n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"milestone_date\", to_date(col(\"milestone_at\")))\n",
    "\n",
    "        # Step 3: Ekstraksi prefix dan ID dari object_id\n",
    "        df = df.withColumn(\"entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"milestone_id\").alias(\"milestone_id\"),\n",
    "            col(\"entity_type\").alias(\"entity_type\"),\n",
    "            col(\"object_id\").alias(\"object_id\"),\n",
    "            col(\"milestone_date\").alias(\"milestone_date\"),\n",
    "            col(\"description\").alias(\"description\"),\n",
    "            col(\"source_url\").alias(\"source_url\"),\n",
    "            col(\"source_description\").alias(\"source_description\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "            \n",
    "        # Step 5: Handle strange values\n",
    "        df_transformed = df_transformed.withColumn(\"object_id\", clean_integer(col(\"object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"description\", clean_alpha_text(\"description\"))\n",
    "        df_transformed = df_transformed.withColumn(\"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\"))\n",
    "        df_transformed = df_transformed.withColumn(\"source_description\", clean_alpha_text(\"source_description\"))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 6: Data cleansing\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"description\": \"No Description\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "        \n",
    "        # Step 7: Drop duplicate data \n",
    "        df_transformed = df_transformed.dropDuplicates([\"milestone_id\"])\n",
    "        df_transformed = df_transformed.dropDuplicates([\"entity_type\", \"object_id\"])\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 8: Validasi object_id di `dim_company`\n",
    "        dim_company = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"object_id\")\n",
    "\n",
    "        # Convert object_id to integer (if needed)\n",
    "        df_transformed = df_transformed.withColumn(\"object_id\", col(\"object_id\").cast(\"int\"))\n",
    "        \n",
    "        # Filter NULL object_id explicitly\n",
    "        df_transformed = df_transformed.filter(col(\"object_id\").isNotNull())\n",
    "        \n",
    "        # 🟢 Data yang valid (match dengan `dim_company`)\n",
    "        df_valid = df_transformed.join(\n",
    "            broadcast(dim_company),\n",
    "            on=\"object_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        \n",
    "        # 🔴 Data yang tidak valid (object_id tidak match)\n",
    "        df_invalid = df_transformed.join(\n",
    "            broadcast(dim_company),\n",
    "            on=\"object_id\",\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        \n",
    "        if df_invalid.count() > 0:\n",
    "            # Convert ke pandas dan simpan dengan `save_invalid_ids`\n",
    "            invalid_ids = df_invalid.select(\"entity_type\", \"object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"milestone\")\n",
    "        \n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"milestone\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 9: Convert to pandas (hanya data valid)\n",
    "        df_pandas = df_valid.toPandas().set_index(\"milestone_id\")\n",
    "        df_pandas.index.name = \"milestone_id\"\n",
    "        \n",
    "        # Step 10: Load to DWH via upsert\n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"dim_milestones\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"dim_milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"ETL Process\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"dim_milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa2513-e186-4a17-9f97-4cd4223bdbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "milestones = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"milestones\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16081e5c-45a4-461d-abce-92291434d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Milestones Data\n",
    "transform_milestones(milestones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1ea9d-67d6-4cbb-aad2-4ac03074f6fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Acquisition Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f9e73-911c-4468-9303-d48162c0a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_acquisition(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"price_amount\", col(\"price_amount\").cast(\"decimal(15,2)\"))\n",
    "        df = df.withColumn(\"acquired_at\", to_date(col(\"acquired_at\")))\n",
    "\n",
    "        # Extract prefix and ID\n",
    "        df = df.withColumn(\"acquiring_entity_type\", extract_prefix(col(\"acquiring_object_id\")))\n",
    "        df = df.withColumn(\"acquired_entity_type\", extract_prefix(col(\"acquired_object_id\")))\n",
    "        df = df.withColumn(\"acquiring_object_id\", extract_id(col(\"acquiring_object_id\")))\n",
    "        df = df.withColumn(\"acquired_object_id\", extract_id(col(\"acquired_object_id\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 3: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"acquisition_id\"),\n",
    "            col(\"acquiring_entity_type\"),\n",
    "            col(\"acquiring_object_id\"),\n",
    "            col(\"acquired_entity_type\"),\n",
    "            col(\"acquired_object_id\"),\n",
    "            col(\"term_code\"),\n",
    "            col(\"price_amount\"),\n",
    "            col(\"price_currency_code\"),\n",
    "            col(\"acquired_at\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # ✅ Step 4: Clean integer values\n",
    "        df_transformed = df_transformed.withColumn(\"acquiring_object_id\", clean_integer(col(\"acquiring_object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"acquired_object_id\", clean_integer(col(\"acquired_object_id\")))\n",
    "        df_transformed = df_transformed.dropna(subset=\"acquired_at\")\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 5: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"price_amount\": 0.0,\n",
    "            \"price_currency_code\": \"Unknown\",\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"term_code\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # ✅ Step 6: Drop duplicates dan data yang tidak valid\n",
    "        df_transformed = df_transformed.dropDuplicates([\"acquisition_id\"])\n",
    "        df_transformed = df_transformed.filter(col(\"term_code\") != \"Unknown\")\n",
    "        df_transformed = df_transformed.filter(col(\"price_currency_code\") != \"Unknown\")\n",
    "        df_transformed = df_transformed.filter(col(\"price_amount\") != 0.0)\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 7: Validasi object_id di `dim_company`\n",
    "        dim_company = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"object_id\", \"company_id\")\n",
    "\n",
    "        # 🟢 Data yang valid\n",
    "        df_valid = (\n",
    "            df_transformed\n",
    "            .join(broadcast(dim_company.alias(\"acquiring\")), col(\"acquiring_object_id\") == col(\"acquiring.object_id\"), \"left\")\n",
    "            .join(broadcast(dim_company.alias(\"acquired\")), col(\"acquired_object_id\") == col(\"acquired.object_id\"), \"left\")\n",
    "            .select(df_transformed[\"*\"],  \n",
    "                    col(\"acquiring.company_id\").alias(\"acquiring_company_id\"),  \n",
    "                    col(\"acquired.company_id\").alias(\"acquired_company_id\"))\n",
    "        )\n",
    "\n",
    "        \n",
    "        # 🔴 Data yang tidak valid\n",
    "        df_invalid = df_valid.filter(col(\"acquiring_company_id\").isNull() | col(\"acquired_company_id\").isNull())\n",
    "        \n",
    "        # Hanya simpan data yang valid ke DWH\n",
    "        df_valid = df_valid.filter(col(\"acquiring_company_id\").isNotNull() & col(\"acquired_company_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"acquiring_entity_type\", \"acquiring_object_id\", \"acquired_entity_type\", \"acquired_object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"acquisition\")\n",
    "            \n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"acquisition\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 8: Load to DWH (upsert)\n",
    "        df_pandas = df_valid.select([\n",
    "            \"acquisition_id\",\n",
    "            \"acquiring_object_id\",\n",
    "            \"acquired_object_id\",\n",
    "            \"acquiring_entity_type\",\n",
    "            \"acquired_entity_type\",\n",
    "            \"term_code\",\n",
    "            \"price_amount\",\n",
    "            \"price_currency_code\",\n",
    "            \"acquired_at\",\n",
    "            \"source_url\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        ]).toPandas().set_index(\"acquisition_id\")\n",
    "\n",
    "\n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"fact_acquisition\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"fact_acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"ETL Process\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"fact_acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11c10b-46d2-4c08-be88-bca5ceb2ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "acquisition = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"acquisition\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de7d66-f19b-4b8f-b8ca-f6e509c85cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform acquisition data\n",
    "\n",
    "transform_acquisition(acquisition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5d3b5-24e1-4c5a-9d32-e3e214a44f30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Investments Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf37a5c-55e2-4377-9c76-9740b47edc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_investments(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Extract prefix and ID\n",
    "        df = df.withColumn(\"funded_entity_type\", extract_prefix(col(\"funded_object_id\")))\n",
    "        df = df.withColumn(\"investor_entity_type\", extract_prefix(col(\"investor_object_id\")))\n",
    "        df = df.withColumn(\"funded_object_id\", extract_id(col(\"funded_object_id\")))\n",
    "        df = df.withColumn(\"investor_object_id\", extract_id(col(\"investor_object_id\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 3: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"investment_id\").alias(\"investment_id\"),\n",
    "            col(\"funding_round_id\").alias(\"funding_round_id\"),\n",
    "            col(\"funded_entity_type\").alias(\"funded_entity_type\"),\n",
    "            col(\"funded_object_id\").alias(\"funded_object_id\"),\n",
    "            col(\"investor_entity_type\").alias(\"investor_entity_type\"),\n",
    "            col(\"investor_object_id\").alias(\"investor_object_id\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # ✅ Step 4: Clean integer values\n",
    "        df_transformed = df_transformed.withColumn(\"funded_object_id\", clean_integer(col(\"funded_object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"investor_object_id\", clean_integer(col(\"investor_object_id\")))\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 5: Handle null values\n",
    "\n",
    "        # ✅ Step 6: Drop duplicates dan data yang tidak valid\n",
    "        # Fix dropna and dropDuplicates\n",
    "        df_transformed = df_transformed.dropDuplicates([\"investment_id\"])\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 7: Validasi object_id di `dim_company`\n",
    "        companies = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(col(\"object_id\"), col(\"company_id\"))\n",
    "        \n",
    "        people = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(col(\"object_id\"), col(\"people_id\"))\n",
    "\n",
    "        # Join with companies and people tables\n",
    "        df_transformed = df_transformed \\\n",
    "            .join(companies.alias(\"comp\"), df_transformed[\"investor_object_id\"] == col(\"comp.object_id\"), \"left\") \\\n",
    "            .join(people.alias(\"peop\"), df_transformed[\"investor_object_id\"] == col(\"peop.object_id\"), \"left\") \\\n",
    "            .withColumn(\"investor_object_id\", when(col(\"comp.company_id\").isNotNull(), col(\"comp.company_id\"))\n",
    "                        .otherwise(col(\"peop.people_id\"))) \\\n",
    "            .withColumn(\"investor_entity_type\", when(col(\"comp.company_id\").isNotNull(), lit(\"company\"))\n",
    "                        .otherwise(lit(\"people\")))\n",
    "\n",
    "        df_transformed = df_transformed \\\n",
    "            .join(companies.alias(\"funded\"), df_transformed[\"funded_object_id\"] == col(\"funded.object_id\"), \"left\") \\\n",
    "            .withColumn(\"mapped_funded_object_id\", col(\"funded.company_id\"))\n",
    "\n",
    "        df_transformed = df_transformed.select(\n",
    "            df_transformed[\"investment_id\"],\n",
    "            df_transformed[\"funding_round_id\"],\n",
    "            df_transformed[\"funded_entity_type\"],\n",
    "            # df_transformed[\"funded_object_id\"],\n",
    "            df_transformed[\"investor_entity_type\"],\n",
    "            df_transformed[\"investor_object_id\"],\n",
    "            df_transformed[\"created_at\"],\n",
    "            df_transformed[\"updated_at\"],\n",
    "            col(\"mapped_funded_object_id\").alias(\"funded_object_id\")\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        df_valid = (\n",
    "            df_transformed\n",
    "            .join(broadcast(companies.alias(\"investor\")), col(\"investor_object_id\") == col(\"investor.object_id\"), \"left\")\n",
    "            .join(broadcast(companies.alias(\"funded\")), col(\"funded_object_id\") == col(\"funded.object_id\"), \"left\")\n",
    "            .select(df_transformed[\"*\"],  \n",
    "                    col(\"investor.company_id\").alias(\"investor_company_id\"),  \n",
    "                    col(\"funded.company_id\").alias(\"funded_company_id\"))\n",
    "        )\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"investor_object_id\").isNull() | col(\"funded_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"investor_object_id\").isNotNull() & col(\"funded_object_id\").isNotNull())\n",
    "        \n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"investor_entity_type\", \"investor_object_id\", \n",
    "                                            \"funded_entity_type\", \"funded_object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"investments\")\n",
    "        \n",
    "        # Load to DWH\n",
    "        df_pandas = df_valid.select([\n",
    "            \"investment_id\",\n",
    "            \"funding_round_id\",\n",
    "            \"funded_entity_type\",\n",
    "            \"funded_object_id\",\n",
    "            \"investor_entity_type\",\n",
    "            \"investor_object_id\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        ]).toPandas().set_index(\"investment_id\")\n",
    "        \n",
    "        \n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"fact_investments\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"fact_investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"ETL Process\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"fact_investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcff15-77f0-46f4-9a7b-57d55d7d7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "investments = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"investments\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd6ec3-47bd-48a4-b7a7-5961f6aff873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Investments data\n",
    "\n",
    "transform_investments(investments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f801cd-c047-49a8-afd6-7ef8910078ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Funding Rounds Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0e9cdf8b-7d5f-4c29-b1f5-432d3e291b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, when, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_funding_rounds(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Extract prefix and ID (pastikan fungsi ini sudah didefinisikan)\n",
    "        df = df.withColumn(\"funding_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", col(\"object_id\").cast(IntegerType()))\n",
    "\n",
    "        # Format data type\n",
    "        df = df.withColumn(\"funding_date\", to_date(col(\"funded_at\")))\n",
    "        df = df.withColumn(\"funding_entity_type\", col(\"funding_entity_type\").cast(StringType()))\n",
    "        df = df.withColumn(\"participants\", col(\"participants\").cast(IntegerType()))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        df_null_1 = df.filter(col(\"object_id\").isNull())\n",
    "        df_null_1.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 1: {df_null_1.count()}\")\n",
    "\n",
    "        df_notnull_1 = df.filter(col(\"object_id\").isNotNull())\n",
    "        df_notnull_1.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 1: {df_notnull_1.count()}\")\n",
    "\n",
    "        # Step 2: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"funding_round_id\"),\n",
    "            col(\"funding_entity_type\"),\n",
    "            col(\"object_id\").alias(\"funding_object_id\"),\n",
    "            col(\"funding_round_type\").alias(\"round_type\"),\n",
    "            col(\"funding_date\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\"),\n",
    "            col(\"raised_amount_usd\"),\n",
    "            col(\"pre_money_currency_code\").alias(\"pre_money_currency\"),\n",
    "            col(\"pre_money_valuation\"),\n",
    "            col(\"pre_money_valuation_usd\"),\n",
    "            col(\"post_money_currency_code\").alias(\"post_money_currency\"),\n",
    "            col(\"post_money_valuation\"),\n",
    "            col(\"post_money_valuation_usd\"),\n",
    "            col(\"participants\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"source_description\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"round_type\": \"Unknown\",\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"pre_money_currency\": \"USD\",\n",
    "            \"post_money_currency\": \"USD\",\n",
    "            \"raised_amount_usd\": 0.0,\n",
    "            \"source_description\": \"Unknown\",\n",
    "        })\n",
    "\n",
    "        # Drop duplicates dan invalid data\n",
    "        df_transformed = df_transformed.dropDuplicates([\"funding_round_id\"])\n",
    "        df_transformed = df_transformed.filter(col(\"round_type\") != \"Unknown\")\n",
    "\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\")\n",
    "        )\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_description\", clean_alpha_text(\"source_description\")\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Load dim_company dan dim_people untuk validasi\n",
    "        companies = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"company_id\"))\n",
    "\n",
    "        people = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"people_id\"))\n",
    "\n",
    "        # Validasi funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(companies), df_transformed[\"funding_object_id\"] == companies[\"company_id\"], \"left\") \\\n",
    "            .join(broadcast(people), df_transformed[\"funding_object_id\"] == people[\"people_id\"], \"left\") \\\n",
    "            .withColumn(\"funding_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_null_funding = df_valid.filter(col(\"funding_object_id\").isNull())\n",
    "        df_null_funding.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL funding_object_id: {df_null_funding.count()}\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"funding_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"funding_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"funding_rounds\")\n",
    "\n",
    "\n",
    "        # Convert to Pandas & Load to DWH\n",
    "        df_pandas = df_valid.toPandas().set_index(\"funding_round_id\").drop_duplicates().dropna()\n",
    "        #df_pandas = df_pandas.dropna(subset=(\"funding_round_id\"))\n",
    "\n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"dim_funding_rounds\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"dim_funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\"step\": \"ETL Process\", \"status\": f\"FAILED - {str(e)}\", \"source\": \"staging\"}, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b07c8000-1bbd-4b09-a178-ac872b87a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "funding_rounds = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"funding_rounds\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "84ee1dde-fd47-4f28-b681-fd49d91a81d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+----------------+---------+---------+------------------+------------------+-----------------+-------------+--------------------+-----------------------+-------------------+-----------------------+------------------------+--------------------+------------------------+------------+--------------+-------------+----------+------------------+----------+----------+----------+-------------------+------------+\n",
      "|funding_round_id|object_id|funded_at|funding_round_type|funding_round_code|raised_amount_usd|raised_amount|raised_currency_code|pre_money_valuation_usd|pre_money_valuation|pre_money_currency_code|post_money_valuation_usd|post_money_valuation|post_money_currency_code|participants|is_first_round|is_last_round|source_url|source_description|created_by|created_at|updated_at|funding_entity_type|funding_date|\n",
      "+----------------+---------+---------+------------------+------------------+-----------------+-------------+--------------------+-----------------------+-------------------+-----------------------+------------------------+--------------------+------------------------+------------+--------------+-------------+----------+------------------+----------+----------+----------+-------------------+------------+\n",
      "+----------------+---------+---------+------------------+------------------+-----------------+-------------+--------------------+-----------------------+-------------------+-----------------------+------------------------+--------------------+------------------------+------------+--------------+-------------+----------+------------------+----------+----------+----------+-------------------+------------+\n",
      "\n",
      "Total rows with NULL object_id 1: 0\n",
      "+----------------+---------+----------+------------------+------------------+-----------------+-------------+--------------------+-----------------------+-------------------+-----------------------+------------------------+--------------------+------------------------+------------+--------------+-------------+-----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+----------------+-------------------+-------------------+-------------------+------------+\n",
      "|funding_round_id|object_id|funded_at |funding_round_type|funding_round_code|raised_amount_usd|raised_amount|raised_currency_code|pre_money_valuation_usd|pre_money_valuation|pre_money_currency_code|post_money_valuation_usd|post_money_valuation|post_money_currency_code|participants|is_first_round|is_last_round|source_url                                                                                           |source_description                                                 |created_by      |created_at         |updated_at         |funding_entity_type|funding_date|\n",
      "+----------------+---------+----------+------------------+------------------+-----------------+-------------+--------------------+-----------------------+-------------------+-----------------------+------------------------+--------------------+------------------------+------------+--------------+-------------+-----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+----------------+-------------------+-------------------+-------------------+------------+\n",
      "|1               |4        |2006-12-01|series-b          |b                 |8500000.00       |8500000.00   |USD                 |0.00                   |0.00               |NULL                   |0.00                    |0.00                |NULL                    |2           |false         |false        |http://www.marketingvox.com/archives/2006/12/29/digg-gets-85mm-in-funding/                           |NULL                                                               |initial-importer|2007-07-04 04:52:57|2008-02-27 23:14:29|c                  |2006-12-01  |\n",
      "|2               |5        |2004-09-01|angel             |angel             |500000.00        |500000.00    |USD                 |0.00                   |0.00               |USD                    |0.00                    |0.00                |USD                     |2           |false         |false        |NULL                                                                                                 |NULL                                                               |initial-importer|2007-05-27 06:08:18|2013-06-28 20:07:23|c                  |2004-09-01  |\n",
      "|3               |5        |2005-05-01|series-a          |a                 |12700000.00      |12700000.00  |USD                 |115000000.00           |115000000.00       |USD                    |0.00                    |0.00                |USD                     |3           |false         |false        |http://www.techcrunch.com/2007/11/02/jim-breyer-extra-500-million-round-for-facebook-a-total-fiction/|Jim Breyer: Extra $500 Million Round For Facebook A \"Total Fiction\"|initial-importer|2007-05-27 06:09:10|2013-06-28 20:07:23|c                  |2005-05-01  |\n",
      "|4               |5        |2006-04-01|series-b          |b                 |27500000.00      |27500000.00  |USD                 |525000000.00           |525000000.00       |USD                    |0.00                    |0.00                |USD                     |4           |false         |false        |http://www.facebook.com/press/info.php?factsheet                                                     |Facebook Funding                                                   |initial-importer|2007-05-27 06:09:36|2013-06-28 20:07:24|c                  |2006-04-01  |\n",
      "|5               |7299     |2006-05-01|series-b          |b                 |10500000.00      |10500000.00  |USD                 |0.00                   |0.00               |NULL                   |0.00                    |0.00                |NULL                    |2           |false         |false        |http://www.techcrunch.com/2006/05/14/photobucket-closes-105-from-trinity-ventures/                   |PhotoBucket Closes $10.5M From Trinity Ventures                    |initial-importer|2007-05-29 11:05:59|2008-04-16 17:09:12|c                  |2006-05-01  |\n",
      "+----------------+---------+----------+------------------+------------------+-----------------+-------------+--------------------+-----------------------+-------------------+-----------------------+------------------------+--------------------+------------------------+------------+--------------+-------------+-----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+----------------+-------------------+-------------------+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL object_id 1: 50182\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+----------------+-------------------+-----------------+----------+------------+---------------+-------------+-----------------+------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+------------+-------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-------------------+-------------------+\n",
      "|funding_round_id|funding_entity_type|funding_object_id|round_type|funding_date|raised_currency|raised_amount|raised_amount_usd|pre_money_currency|pre_money_valuation|pre_money_valuation_usd|post_money_currency|post_money_valuation|post_money_valuation_usd|participants|source_url                                                                                                   |source_description                                               |created_at         |updated_at         |\n",
      "+----------------+-------------------+-----------------+----------+------------+---------------+-------------+-----------------+------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+------------+-------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-------------------+-------------------+\n",
      "|417             |c                  |NULL             |series-b  |2007-08-01  |USD            |4500000.00   |4500000.00       |USD               |0.00               |0.00                   |USD                |0.00                |0.00                    |1           |http://www.paidcontent.org/entry/419-hip-hop-influenced-personalized-start-page-globalgrind-gets-45-million-/|Unknown                                                          |2007-08-22 09:23:01|2008-06-11 02:08:24|\n",
      "|633             |c                  |NULL             |angel     |2007-07-01  |USD            |0.00         |0.00             |USD               |0.00               |0.00                   |USD                |0.00                |0.00                    |1           |http://www.idgvb.com/newsitem.cfm?id=208                                                                     |Unknown                                                          |2007-09-05 17:58:20|2008-05-20 01:54:35|\n",
      "|833             |c                  |NULL             |series-a  |1999-01-01  |USD            |300000.00    |300000.00        |USD               |0.00               |0.00                   |USD                |0.00                |0.00                    |2           |Unknown                                                                                                      |Unknown                                                          |2007-11-26 12:44:27|2008-05-08 05:28:54|\n",
      "|874             |c                  |NULL             |angel     |2007-06-01  |USD            |0.00         |0.00             |USD               |0.00               |0.00                   |USD                |0.00                |0.00                    |1           |Unknown                                                                                                      |Unknown                                                          |2007-11-03 11:21:13|2011-10-23 04:04:39|\n",
      "|883             |c                  |NULL             |series-a  |2007-10-31  |USD            |3150000.00   |3150000.00       |USD               |0.00               |0.00                   |USD                |0.00                |0.00                    |0           |http://venturebeat.com/2007/10/31/investors-still-betting-on-online-video-mdialouge-snags-315-million/       |Investors still betting on online video mDialog snags 315 million|2007-11-05 21:28:45|2008-06-06 19:49:53|\n",
      "+----------------+-------------------+-----------------+----------+------------+---------------+-------------+-----------------+------------------+-------------------+-----------------------+-------------------+--------------------+------------------------+------------+-------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL funding_object_id: 38223\n",
      "Error saving invalid IDs from table 'funding_rounds': 2 columns passed, passed data had 19 columns\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Transform Funding Rounds\n",
    "transform_funding_rounds(funding_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d827278-73c8-46f6-8298-57b9cefe5037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Relationship Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "592eacc5-e521-4c0e-bbb0-7059a10535e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, to_timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_relationship(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 1: Format data type\n",
    "        df = df.withColumn(\"start_at\", to_date(col(\"start_at\")))\n",
    "        df = df.withColumn(\"end_at\", to_date(col(\"end_at\")))\n",
    "        df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\")))\n",
    "        df = df.withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "        # Extract prefix and ID, normalize\n",
    "        df = df.withColumn(\"people_entity_type\", extract_prefix(col(\"person_object_id\")))\n",
    "        df = df.withColumn(\"relationship_entity_type\", extract_prefix(col(\"relationship_object_id\")))\n",
    "        df = df.withColumn(\"people_object_id\", extract_id(col(\"person_object_id\")))\n",
    "        df = df.withColumn(\"relationship_object_id\", extract_id(col(\"relationship_object_id\")))\n",
    "        df = df.withColumn(\"title\", normalize_text(col(\"title\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        df_null_1 = df.filter(col(\"people_object_id\").isNull())\n",
    "        df_null_1.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 1: {df_null_1.count()}\")\n",
    "\n",
    "        df_null_2 = df.filter(col(\"relationship_object_id\").isNull())\n",
    "        df_null_2.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 2: {df_null_1.count()}\")\n",
    "\n",
    "        df_notnull_1 = df.filter(col(\"people_object_id\").isNotNull())\n",
    "        df_notnull_1.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 1: {df_notnull_1.count()}\")\n",
    "\n",
    "        df_notnull_2 = df.filter(col(\"relationship_object_id\").isNotNull())\n",
    "        df_notnull_2.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 2: {df_notnull_1.count()}\")\n",
    "        \n",
    "\n",
    "        # ✅ Step 3: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"relationship_id\"),\n",
    "            col(\"people_entity_type\").alias(\"people_entity_type\"),\n",
    "            col(\"people_object_id\").alias(\"people_object_id\"),\n",
    "            col(\"relationship_entity_type\").alias(\"relationship_entity_type\"),\n",
    "            col(\"relationship_object_id\").alias(\"relationship_object_id\"),\n",
    "            col(\"start_at\").alias(\"start_at\"),\n",
    "            col(\"end_at\").alias(\"end_at\"),\n",
    "            col(\"title\").alias(\"title\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 5: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"title\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # ✅ Step 6: Drop duplicates dan data yang tidak valid\n",
    "        # Fix dropna and dropDuplicates\n",
    "        df_transformed = df_transformed.dropDuplicates([\"relationship_id\"])\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Load dim_company dan dim_people untuk validasi\n",
    "        companies = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"company_id\"))\n",
    "\n",
    "        people = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"people_id\"))\n",
    "\n",
    "        # Validasi funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(companies), df_transformed[\"relationship_object_id\"] == companies[\"company_id\"], \"left\") \\\n",
    "            .join(broadcast(people), df_transformed[\"people_object_id\"] == people[\"people_id\"], \"left\") \\\n",
    "            .withColumn(\"relationship_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_null_relationship = df_valid.filter(col(\"relationship_object_id\").isNull())\n",
    "        df_null_relationship.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL funding_object_id: {df_null_relationship.count()}\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"people_object_id\").isNull() | col(\"relationship_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"people_object_id\").isNotNull() & col(\"relationship_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"relationship\")\n",
    "\n",
    "\n",
    "        # Convert to Pandas & Load to DWH\n",
    "        df_pandas = df_valid.toPandas().set_index(\"relationship_id\").drop_duplicates().dropna()\n",
    "        \n",
    "        \n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"fact_relationship\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"fact_relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"ETL Process\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"fact_relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n",
    "\n",
    "    print(\"✅ Relationship data successfully loaded to warehouse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "68035ef6-1534-47cb-ae1c-bc68bb3b8240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca data dari staging\n",
    "relationship = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"relationship\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cc96531e-4d82-4d73-b9de-f2f93c696d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+---------------+----------------+----------------------+--------+------+-------+--------+-----+----------+----------+------------------+------------------------+----------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|start_at|end_at|is_past|sequence|title|created_at|updated_at|people_entity_type|relationship_entity_type|people_object_id|\n",
      "+---------------+----------------+----------------------+--------+------+-------+--------+-----+----------+----------+------------------+------------------------+----------------+\n",
      "+---------------+----------------+----------------------+--------+------+-------+--------+-----+----------+----------+------------------+------------------------+----------------+\n",
      "\n",
      "Total rows with NULL object_id 1: 0\n",
      "+---------------+----------------+----------------------+--------+------+-------+--------+-----+----------+----------+------------------+------------------------+----------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|start_at|end_at|is_past|sequence|title|created_at|updated_at|people_entity_type|relationship_entity_type|people_object_id|\n",
      "+---------------+----------------+----------------------+--------+------+-------+--------+-----+----------+----------+------------------+------------------------+----------------+\n",
      "+---------------+----------------+----------------------+--------+------+-------+--------+-----+----------+----------+------------------+------------------------+----------------+\n",
      "\n",
      "Total rows with NULL object_id 2: 0\n",
      "+---------------+----------------+----------------------+----------+----------+-------+--------+-----------------------------------+-------------------+-------------------+------------------+------------------------+----------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|start_at  |end_at    |is_past|sequence|title                              |created_at         |updated_at         |people_entity_type|relationship_entity_type|people_object_id|\n",
      "+---------------+----------------+----------------------+----------+----------+-------+--------+-----------------------------------+-------------------+-------------------+------------------+------------------------+----------------+\n",
      "|1              |p:2             |1                     |NULL      |NULL      |false  |8       |cofounder ceo board of directors   |2007-05-25 07:03:54|2013-06-03 09:58:46|p                 |c                       |2               |\n",
      "|2              |p:3             |1                     |NULL      |NULL      |false  |279242  |vp marketing                       |2007-05-25 07:04:16|2010-05-21 16:31:34|p                 |c                       |3               |\n",
      "|3              |p:4             |3                     |NULL      |NULL      |false  |4       |evangelist                         |2007-05-25 19:33:03|2013-06-29 13:36:58|p                 |c                       |4               |\n",
      "|4              |p:5             |3                     |2006-03-01|2009-12-01|false  |4       |senior director strategic alliances|2007-05-25 19:34:53|2013-06-29 10:25:34|p                 |c                       |5               |\n",
      "|6              |p:7             |4                     |2005-07-01|2010-04-05|false  |1       |chief executive officer            |2007-05-25 20:05:33|2010-04-05 18:41:41|p                 |c                       |7               |\n",
      "+---------------+----------------+----------------------+----------+----------+-------+--------+-----------------------------------+-------------------+-------------------+------------------+------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL object_id 1: 134726\n",
      "+---------------+----------------+----------------------+----------+----------+-------+--------+-----------------------------------+-------------------+-------------------+------------------+------------------------+----------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|start_at  |end_at    |is_past|sequence|title                              |created_at         |updated_at         |people_entity_type|relationship_entity_type|people_object_id|\n",
      "+---------------+----------------+----------------------+----------+----------+-------+--------+-----------------------------------+-------------------+-------------------+------------------+------------------------+----------------+\n",
      "|1              |p:2             |1                     |NULL      |NULL      |false  |8       |cofounder ceo board of directors   |2007-05-25 07:03:54|2013-06-03 09:58:46|p                 |c                       |2               |\n",
      "|2              |p:3             |1                     |NULL      |NULL      |false  |279242  |vp marketing                       |2007-05-25 07:04:16|2010-05-21 16:31:34|p                 |c                       |3               |\n",
      "|3              |p:4             |3                     |NULL      |NULL      |false  |4       |evangelist                         |2007-05-25 19:33:03|2013-06-29 13:36:58|p                 |c                       |4               |\n",
      "|4              |p:5             |3                     |2006-03-01|2009-12-01|false  |4       |senior director strategic alliances|2007-05-25 19:34:53|2013-06-29 10:25:34|p                 |c                       |5               |\n",
      "|6              |p:7             |4                     |2005-07-01|2010-04-05|false  |1       |chief executive officer            |2007-05-25 20:05:33|2010-04-05 18:41:41|p                 |c                       |7               |\n",
      "+---------------+----------------+----------------------+----------+----------+-------+--------+-----------------------------------+-------------------+-------------------+------------------+------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL object_id 2: 134726\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+---------------+------------------+----------------+------------------------+----------------------+----------+------+---------+-------------------+-------------------+\n",
      "|relationship_id|people_entity_type|people_object_id|relationship_entity_type|relationship_object_id|start_at  |end_at|title    |created_at         |updated_at         |\n",
      "+---------------+------------------+----------------+------------------------+----------------------+----------+------+---------+-------------------+-------------------+\n",
      "|100021         |p                 |81401           |c                       |NULL                  |NULL      |NULL  |cofounder|2011-01-30 23:40:42|2011-02-17 01:57:01|\n",
      "|100062         |p                 |81445           |c                       |NULL                  |2010-06-01|NULL  |coo      |2011-01-31 13:20:39|2011-02-01 03:39:19|\n",
      "|100070         |p                 |81456           |c                       |NULL                  |NULL      |NULL  |ceo      |2011-01-31 15:36:25|2011-02-01 03:39:37|\n",
      "|100128         |p                 |81509           |c                       |NULL                  |NULL      |NULL  |cofounder|2011-01-31 21:39:09|2011-02-01 01:49:28|\n",
      "|100129         |p                 |81510           |c                       |NULL                  |NULL      |NULL  |cofounder|2011-01-31 21:39:09|2011-02-01 01:49:28|\n",
      "+---------------+------------------+----------------+------------------------+----------------------+----------+------+---------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL funding_object_id: 87786\n",
      "Error saving invalid IDs from table 'relationship': 2 columns passed, passed data had 10 columns\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "✅ Relationship data successfully loaded to warehouse\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform Relationship Data\n",
    "transform_relationship(relationship)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e85056-501d-46d6-8f85-5b858a7de6dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IPO Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfd70066-c470-41e2-a78c-2b489103674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, to_timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_ipo(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 1: Format data type\n",
    "        df = df.withColumn(\"public_at\", to_date(col(\"public_at\")))\n",
    "\n",
    "        # Extract prefix and ID, normalize\n",
    "        df = df.withColumn(\"ipo_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"ipo_object_id\", extract_id(col(\"object_id\")))\n",
    "        df = df.withColumn(\"stock_market\", extract_stock_market(col(\"stock_symbol\")))\n",
    "        df = df.withColumn(\"stock_symbol\", extract_stock_symbol(col(\"stock_symbol\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        df_null_1 = df.filter(col(\"ipo_object_id\").isNull())\n",
    "        df_null_1.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 1: {df_null_1.count()}\")\n",
    "\n",
    "\n",
    "        df_notnull_1 = df.filter(col(\"ipo_object_id\").isNotNull())\n",
    "        df_notnull_1.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 1: {df_notnull_1.count()}\")\n",
    "\n",
    "        \n",
    "\n",
    "        # ✅ Step 3: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"ipo_id\").alias(\"ipo_id\"),\n",
    "            col(\"ipo_entity_type\").alias(\"ipo_entity_type\"),\n",
    "            col(\"ipo_object_id\").alias(\"ipo_object_id\"),\n",
    "            col(\"valuation_currency_code\").alias(\"valuation_currency\"),\n",
    "            col(\"valuation_amount\").alias(\"valuation_amount\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\").alias(\"raised_amount\"),\n",
    "            col(\"public_at\").alias(\"public_at\"),\n",
    "            col(\"stock_market\").alias(\"stock_market\"),\n",
    "            col(\"stock_symbol\").alias(\"stock_symbol\"),\n",
    "            col(\"source_url\").alias(\"source_url\"),\n",
    "            col(\"source_description\").alias(\"source_description\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # ✅ Step 5: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"valuation_amount\": 0.0,\n",
    "            \"valuation_currency\": \"USD\",\n",
    "            \"raised_amount\": 0.0,\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"stock_market\": \"N/A\",\n",
    "            \"stock_symbol\": \"N/A\",\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # ✅ Step 6: Drop duplicates dan data yang tidak valid\n",
    "        # dropDuplicates\n",
    "        df_transformed = df_transformed.dropDuplicates([\"ipo_id\"])\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipos\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Load dim_company dan dim_people untuk validasi\n",
    "        companies = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"company_id\"))\n",
    "\n",
    "        people = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"people_id\"))\n",
    "\n",
    "        # Validasi funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(companies), df_transformed[\"ipo_object_id\"] == companies[\"company_id\"], \"left\") \\\n",
    "            .join(broadcast(people), df_transformed[\"ipo_object_id\"] == people[\"people_id\"], \"left\") \\\n",
    "            .withColumn(\"ipo_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_null_ipo = df_valid.filter(col(\"ipo_object_id\").isNull())\n",
    "        df_null_ipo.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id: {df_null_ipo.count()}\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"ipo_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"ipo_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"ipo\")\n",
    "\n",
    "\n",
    "        # Convert to Pandas & Load to DWH\n",
    "        df_pandas = df_valid.toPandas().set_index(\"ipo_id\").drop_duplicates().dropna()\n",
    "        \n",
    "        \n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"fact_ipo\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"fact_ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"ETL Process\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"fact_ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n",
    "\n",
    "    print(\"✅ The data is successfully loaded to warehouse\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0f0633e-4911-48e6-bbcd-8a3a993320e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "ipo = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"ipo\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cdf92ae-fca2-464a-b77c-88257dec7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+------+---------+----------------+-----------------------+-------------+--------------------+---------+------------+----------+------------------+----------+----------+---------------+-------------+------------+\n",
      "|ipo_id|object_id|valuation_amount|valuation_currency_code|raised_amount|raised_currency_code|public_at|stock_symbol|source_url|source_description|created_at|updated_at|ipo_entity_type|ipo_object_id|stock_market|\n",
      "+------+---------+----------------+-----------------------+-------------+--------------------+---------+------------+----------+------------------+----------+----------+---------------+-------------+------------+\n",
      "+------+---------+----------------+-----------------------+-------------+--------------------+---------+------------+----------+------------------+----------+----------+---------------+-------------+------------+\n",
      "\n",
      "Total rows with NULL object_id 1: 0\n",
      "+------+---------+----------------+-----------------------+-------------+--------------------+----------+------------+----------+------------------+-------------------+-------------------+---------------+-------------+------------+\n",
      "|ipo_id|object_id|valuation_amount|valuation_currency_code|raised_amount|raised_currency_code|public_at |stock_symbol|source_url|source_description|created_at         |updated_at         |ipo_entity_type|ipo_object_id|stock_market|\n",
      "+------+---------+----------------+-----------------------+-------------+--------------------+----------+------------+----------+------------------+-------------------+-------------------+---------------+-------------+------------+\n",
      "|1     |c:1654   |0.00            |USD                    |0.00         |USD                 |1980-12-19|AAPL        |NULL      |NULL              |2008-02-09 05:17:45|2012-04-12 04:02:59|c              |1654         |NASDAQ      |\n",
      "|2     |c:1242   |0.00            |USD                    |0.00         |NULL                |1986-03-13|MSFT        |NULL      |NULL              |2008-02-09 05:25:18|2010-12-11 12:39:46|c              |1242         |NASDAQ      |\n",
      "|3     |c:342    |0.00            |USD                    |0.00         |NULL                |1969-06-09|DIS         |NULL      |NULL              |2008-02-09 05:40:32|2010-12-23 08:58:16|c              |342          |NYSE        |\n",
      "|4     |c:59     |0.00            |USD                    |0.00         |NULL                |2004-08-25|GOOG        |NULL      |NULL              |2008-02-10 22:51:24|2011-08-01 20:47:08|c              |59           |NASDAQ      |\n",
      "|5     |c:317    |100000000000.00 |USD                    |0.00         |NULL                |1997-05-01|AMZN        |NULL      |NULL              |2008-02-10 23:28:09|2011-08-01 21:11:22|c              |317          |NASDAQ      |\n",
      "+------+---------+----------------+-----------------------+-------------+--------------------+----------+------------+----------+------------------+-------------------+-------------------+---------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL object_id 1: 1194\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+------+---------------+-------------+------------------+----------------+---------------+-------------+----------+------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------+-------------------+-------------------+\n",
      "|ipo_id|ipo_entity_type|ipo_object_id|valuation_currency|valuation_amount|raised_currency|raised_amount|public_at |stock_market|stock_symbol|source_url                                                                                                                                                                                                                                            |source_description                               |created_at         |updated_at         |\n",
      "+------+---------------+-------------+------------------+----------------+---------------+-------------+----------+------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------+-------------------+-------------------+\n",
      "|1     |c              |NULL         |USD               |0.00            |USD            |0.00         |1980-12-19|NASDAQ      |AAPL        |Unknown                                                                                                                                                                                                                                               |Unknown                                          |2008-02-09 05:17:45|2012-04-12 04:02:59|\n",
      "|1001  |c              |NULL         |USD               |0.00            |USD            |60000000.00  |2013-07-01|NASDAQ      |MBII        |http://pevc.dowjones.com/article?an=DJFVW00020130701e971u5v7j&from=alert&pid=32&ReturnUrl=http%3a%2f%2fpevc.dowjones.com%2farticle%3fan%3dDJFVW00020130701e971u5v7j%26from%3dalert%26pid%3d32                                                         |Marrone Bio Innovations Files IPO                |2013-07-02 04:22:10|2013-07-02 04:22:50|\n",
      "|1004  |c              |NULL         |USD               |0.00            |USD            |0.00         |NULL      |OTC         |AVTC        |Unknown                                                                                                                                                                                                                                               |Unknown                                          |2013-07-04 03:34:46|2013-07-04 03:34:46|\n",
      "|1008  |c              |NULL         |USD               |0.00            |USD            |0.00         |2011-01-05|OTC         |ALQA        |Unknown                                                                                                                                                                                                                                               |Unknown                                          |2013-07-08 12:36:54|2013-11-08 03:28:31|\n",
      "|1009  |c              |NULL         |USD               |0.00            |USD            |128000000.00 |2013-08-08|NASDAQ      |FOXF        |http://www.bizjournals.com/sanjose/news/2013/08/08/fox-raises-128m-in-public-market-debut.html?ana=RSS&s=article_search&utm_source=twitterfeed&utm_medium=twitter&utm_campaign=Feed%3A+bizj_sanjose+%28Silicon+Valley+%2F+San+Jose+Business+Journal%29|Fox raises $128M in public debut, ends day up 24%|2013-07-09 04:24:09|2013-08-09 03:18:34|\n",
      "+------+---------------+-------------+------------------+----------------+---------------+-------------+----------+------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL object_id: 841\n",
      "Error saving invalid IDs from table 'ipo': 2 columns passed, passed data had 14 columns\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "✅ The data is successfully loaded to warehouse\n"
     ]
    }
   ],
   "source": [
    "# Transform IPO Data\n",
    "transform_ipo(ipo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b244d-ce54-49ed-a08d-fb298f457ca6",
   "metadata": {},
   "source": [
    "### Funds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac901e65-8390-4d07-a37a-6379d0b5054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, when, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_funds(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Extract prefix and ID (pastikan fungsi ini sudah didefinisikan)\n",
    "        df = df.withColumn(\"fund_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"fund_object_id\", extract_id(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", col(\"object_id\").cast(IntegerType()))\n",
    "        \n",
    "        # Format data type\n",
    "        df = df.withColumn(\"funding_date\", to_date(col(\"funded_at\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        df_null_1 = df.filter(col(\"fund_object_id\").isNull())\n",
    "        df_null_1.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 1: {df_null_1.count()}\")\n",
    "\n",
    "        df_notnull_1 = df.filter(col(\"fund_object_id\").isNotNull())\n",
    "        df_notnull_1.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL object_id 1: {df_notnull_1.count()}\")\n",
    "\n",
    "        # Step 2: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"fund_id\"),\n",
    "            col(\"fund_entity_type\"),\n",
    "            col(\"fund_object_id\").alias(\"fund_object_id\"),\n",
    "            col(\"name\").alias(\"fund_name\"),\n",
    "            col(\"funding_date\").alias(\"funding_date\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"source_description\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"raised_amount\": 0.0,\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Drop duplicates dan invalid data\n",
    "        df_transformed = df_transformed.dropDuplicates([\"fund_id\"])\n",
    "        df_transformed = df_transformed.na.drop(subset=[\"funding_date\"])\n",
    "\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\")\n",
    "        )\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_description\", clean_alpha_text(\"source_description\")\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Load dim_company dan dim_people untuk validasi\n",
    "        companies = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"company_id\"))\n",
    "\n",
    "        people = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"people_id\"))\n",
    "\n",
    "        # Validasi funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(companies), df_transformed[\"fund_object_id\"] == companies[\"company_id\"], \"left\") \\\n",
    "            .join(broadcast(people), df_transformed[\"fund_object_id\"] == people[\"people_id\"], \"left\") \\\n",
    "            .withColumn(\"fund_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_null_fund = df_valid.filter(col(\"fund_object_id\").isNull())\n",
    "        df_null_fund.show(5, truncate=False)  # Lihat beberapa contoh baris yang menyebabkan error\n",
    "        print(f\"Total rows with NULL fund_object_id: {df_null_fund.count()}\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"fund_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"fund_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"funds\")\n",
    "\n",
    "\n",
    "        # Convert to Pandas & Load to DWH\n",
    "        df_pandas = df_valid.toPandas().set_index(\"fund_id\").drop_duplicates().dropna()\n",
    "\n",
    "        upsert(\n",
    "            con=engine,\n",
    "            df=df_pandas,\n",
    "            table_name=\"dim_funds\",\n",
    "            if_row_exists=\"update\"\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"dim_funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\"step\": \"ETL Process\", \"status\": f\"FAILED - {str(e)}\", \"source\": \"staging\"}, \"etl_log.csv\")\n",
    "        raise\n",
    "\n",
    "    print(\"✅ The data is successfully loaded to warehouse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb8a5822-6aa7-4a03-a763-e108dd107b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "funds = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"funds\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f12d418c-1be0-4421-80d6-de61e463c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+-------+---------+----+---------+-------------+--------------------+----------+------------------+----------+----------+----------------+--------------+------------+\n",
      "|fund_id|object_id|name|funded_at|raised_amount|raised_currency_code|source_url|source_description|created_at|updated_at|fund_entity_type|fund_object_id|funding_date|\n",
      "+-------+---------+----+---------+-------------+--------------------+----------+------------------+----------+----------+----------------+--------------+------------+\n",
      "+-------+---------+----+---------+-------------+--------------------+----------+------------------+----------+----------+----------------+--------------+------------+\n",
      "\n",
      "Total rows with NULL object_id 1: 0\n",
      "+-------+---------+--------------------------+----------+-------------+--------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------+-------------------+-------------------+----------------+--------------+------------+\n",
      "|fund_id|object_id|name                      |funded_at |raised_amount|raised_currency_code|source_url                                                                                 |source_description                                         |created_at         |updated_at         |fund_entity_type|fund_object_id|funding_date|\n",
      "+-------+---------+--------------------------+----------+-------------+--------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------+-------------------+-------------------+----------------+--------------+------------+\n",
      "|1      |NULL     |Second Fund               |2008-12-16|300000000.00 |USD                 |http://www.pehub.com/26194/dfj-dragon-raising-second-fund/                                 |peHub                                                      |2008-12-17 03:07:16|2008-12-17 03:07:16|f               |371           |2008-12-16  |\n",
      "|4      |NULL     |Sequoia Israel Fourth Fund|2008-12-17|200750000.00 |USD                 |http://www.pehub.com/26725/sequoia-israel-raises-fourth-fund/                              |Sequoia Israel Raises Fourth Fund                          |2008-12-18 22:04:42|2008-12-18 22:04:42|f               |17            |2008-12-17  |\n",
      "|5      |NULL     |Tenth fund                |2008-08-11|650000000.00 |USD                 |http://venturebeat.com/2008/08/11/interwest-closes-650m-fund/                              |Venture Beat                                               |2008-12-31 09:47:51|2008-12-31 09:47:51|f               |951           |2008-08-11  |\n",
      "|6      |NULL     |New funds acquire         |NULL      |625000000.00 |USD                 |http://venturebeat.com/2008/07/28/us-venture-partners-raises-625m-fund-for-new-investments/|U.S. Venture Partners raises $625M fund for new investments|2009-01-01 18:13:44|2009-01-01 18:16:27|f               |192           |NULL        |\n",
      "|7      |NULL     |Third fund                |2008-05-20|200000000.00 |USD                 |http://venturebeat.com/2008/05/20/disneys-steamboat-ventures-looking-to-raise-200m-fund/   |Venture Beat                                               |2009-01-03 09:51:58|2013-09-03 16:34:54|f               |519           |2008-05-20  |\n",
      "+-------+---------+--------------------------+----------+-------------+--------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------+-------------------+-------------------+----------------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL object_id 1: 1493\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+-------+----------------+--------------+------------------------------+------------+---------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+-------------------+-------------------+\n",
      "|fund_id|fund_entity_type|fund_object_id|fund_name                     |funding_date|raised_currency|raised_amount|source_url                                                                                                                                                                                        |source_description                                 |created_at         |updated_at         |\n",
      "+-------+----------------+--------------+------------------------------+------------+---------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+-------------------+-------------------+\n",
      "|1005   |f               |NULL          |Serent Capital II             |2013-01-30  |USD            |350000000.00 |http://www.finsmes.com/2013/01/serent-capital-closes-fund-350m.html                                                                                                                               |Serent Capital Closes Second Fund at 350M          |2013-01-31 02:50:10|2013-01-31 02:50:10|\n",
      "|101    |f               |NULL          |Emerald Stage2 Ventures, LP   |2008-03-05  |USD            |15000000.00  |http://philadelphia.bizjournals.com/philadelphia/stories/2008/07/07/story7.html                                                                                                                   |Emerald Stage2 Ventures making its mark            |2009-11-03 00:05:11|2009-11-03 01:53:16|\n",
      "|1015   |f               |NULL          |Ambienta II                   |2013-10-29  |EUR            |147200000.00 |http://www.finsmes.com/2013/10/ambienta-holds-closes-fund-e147-2m.html                                                                                                                            |Ambienta Holds First Closes of Second Fund at 1472M|2013-02-08 23:09:32|2013-10-30 06:11:59|\n",
      "|1016   |f               |NULL          |DWHP III                      |2013-02-11  |USD            |265000000.00 |http://www.finsmes.com/2013/02/dw-healthcare-partners-closes-fund-265m.html                                                                                                                       |DW Healthcare Partners Closes Third Fund at 265M   |2013-02-12 04:59:32|2013-02-12 04:59:32|\n",
      "|1030   |f               |NULL          |Accelerator Venture Capital II|2013-02-20  |USD            |25000000.00  |http://pevc.dowjones.com/article?an=DJFVW00020130221e92lt5rh4&from=alert&pid=32&ReturnUrl=http%3a%2f%2fpevc.dowjones.com%3a80%2farticle%3fan%3dDJFVW00020130221e92lt5rh4%26from%3dalert%26pid%3d32|Accelerator Ventures Raising Second Fund           |2013-02-21 03:38:49|2013-02-21 03:38:49|\n",
      "+-------+----------------+--------------+------------------------------+------------+---------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows with NULL fund_object_id: 225\n",
      "Error saving invalid IDs from table 'funds': 2 columns passed, passed data had 11 columns\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "✅ The data is successfully loaded to warehouse\n"
     ]
    }
   ],
   "source": [
    "# Transform funds Data\n",
    "transform_funds(funds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d8be8-e825-4719-b12f-38db7317754e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
