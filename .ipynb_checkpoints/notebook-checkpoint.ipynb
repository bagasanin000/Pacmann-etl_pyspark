{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30f9b6cd-7c63-4d7c-a963-25952b0ca934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eace4aee-ea80-400b-af40-bbf677b5cb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check pyspark version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "027ca5ff-3f8d-4fc2-b52a-7da84338ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb1d6a71-4fac-4f99-9ea4-f2f7e2d73fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Final Project PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f4d9054-9107-42d6-a0a0-e049b9319e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Final Project PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f12b7f3b010>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add16a43-963c-420d-8b5e-e8c18d934667",
   "metadata": {},
   "source": [
    "## Load and Handle Failure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d94b859-9d4d-483a-b6af-5dfcdf168376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a54467c7-078d-466a-91d8-57af1aac710c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8d67139-17b1-4fdc-aaa1-a5f9a5fc15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12c56d6-bb72-4a25-984a-b30b66d7ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT LOGS TO SAVE ALL LOGGING\n",
    "\n",
    "def log_to_csv(log_msg: dict, filename: str):\n",
    "    # Ensure the 'logs' directory exists\n",
    "    log_dir = os.path.join(os.getcwd(), 'logs')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Create the full file path inside 'logs'\n",
    "    file_path = os.path.join(log_dir, filename)\n",
    "\n",
    "    # Define the column headers\n",
    "    headers = [\"step\", \"status\", \"source\", \"table_name\", \"etl_date\"]\n",
    "\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        file_exists = os.path.isfile(file_path)\n",
    "\n",
    "        with open(file_path, mode='a', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "\n",
    "            # Write the header only if the file doesn't exist\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Append the log message\n",
    "            writer.writerow(log_msg)\n",
    "\n",
    "        print(f\"Log written to {file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing log to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091d0a6-b0fc-4a07-96a0-5ea4288035fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8994a6-a66d-4dd2-87a7-1e29133144f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7124d8ff-228b-4ecf-b8b2-9f7adb03a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(file_path, table_name):\n",
    "    try:\n",
    "        # Read CSV using Spark\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "        # Show extracted data\n",
    "        df.show()\n",
    "\n",
    "        # Log success\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"CSV\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"CSV\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        print(f\"Error extracting {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6396ca-a091-4d35-aa82-21826baf95e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|people_id|object_id|first_name| last_name|          birthplace|    affiliation_name|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|        1|      p:2|       Ben|   Elowitz|                NULL|           Blue Nile|\n",
      "|        2|      p:3|     Kevin|  Flaherty|                NULL|            Wetpaint|\n",
      "|        3|      p:4|      Raju|   Vegesna|                NULL|                Zoho|\n",
      "|        4|      p:5|       Ian|     Wenig|                NULL|                Zoho|\n",
      "|        5|      p:6|     Kevin|      Rose|         Redding, CA|        i/o Ventures|\n",
      "|        6|      p:7|       Jay|   Adelson|         Detroit, MI|                Digg|\n",
      "|        7|      p:8|      Owen|     Byrne|                NULL|                Digg|\n",
      "|        8|      p:9|       Ron|Gorodetzky|                NULL|                Digg|\n",
      "|        9|     p:10|      Mark|Zuckerberg|                NULL|            Facebook|\n",
      "|       10|     p:11|    Dustin| Moskovitz|     Gainesville, FL|            Facebook|\n",
      "|       11|     p:12|      Owen| Van Natta|                NULL|               Asana|\n",
      "|       12|     p:13|      Matt|    Cohler|                NULL|            LinkedIn|\n",
      "|       13|     p:14|     Chris|    Hughes|Hickery, North Ca...|General Catalyst ...|\n",
      "|       14|     p:16|      Alex|     Welch|                NULL|            C7 Group|\n",
      "|       15|     p:17|    Darren|   Crystal|                NULL|         Photobucket|\n",
      "|       16|     p:18|   Michael|     Clark|                NULL|   Photobucket (Old)|\n",
      "|       17|     p:19|      Greg|    Wimmer|                NULL|         Photobucket|\n",
      "|       18|     p:20|     Peter|    Foster|                NULL|         Photobucket|\n",
      "|       19|     p:21|   Heather|      Dana|                NULL|         Photobucket|\n",
      "|       20|     p:22|     Peter|      Pham|                NULL|   Photobucket, Inc.|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|people_id|object_id|first_name| last_name|          birthplace|    affiliation_name|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|        1|      p:2|       Ben|   Elowitz|                NULL|           Blue Nile|\n",
      "|        2|      p:3|     Kevin|  Flaherty|                NULL|            Wetpaint|\n",
      "|        3|      p:4|      Raju|   Vegesna|                NULL|                Zoho|\n",
      "|        4|      p:5|       Ian|     Wenig|                NULL|                Zoho|\n",
      "|        5|      p:6|     Kevin|      Rose|         Redding, CA|        i/o Ventures|\n",
      "|        6|      p:7|       Jay|   Adelson|         Detroit, MI|                Digg|\n",
      "|        7|      p:8|      Owen|     Byrne|                NULL|                Digg|\n",
      "|        8|      p:9|       Ron|Gorodetzky|                NULL|                Digg|\n",
      "|        9|     p:10|      Mark|Zuckerberg|                NULL|            Facebook|\n",
      "|       10|     p:11|    Dustin| Moskovitz|     Gainesville, FL|            Facebook|\n",
      "|       11|     p:12|      Owen| Van Natta|                NULL|               Asana|\n",
      "|       12|     p:13|      Matt|    Cohler|                NULL|            LinkedIn|\n",
      "|       13|     p:14|     Chris|    Hughes|Hickery, North Ca...|General Catalyst ...|\n",
      "|       14|     p:16|      Alex|     Welch|                NULL|            C7 Group|\n",
      "|       15|     p:17|    Darren|   Crystal|                NULL|         Photobucket|\n",
      "|       16|     p:18|   Michael|     Clark|                NULL|   Photobucket (Old)|\n",
      "|       17|     p:19|      Greg|    Wimmer|                NULL|         Photobucket|\n",
      "|       18|     p:20|     Peter|    Foster|                NULL|         Photobucket|\n",
      "|       19|     p:21|   Heather|      Dana|                NULL|         Photobucket|\n",
      "|       20|     p:22|     Peter|      Pham|                NULL|   Photobucket, Inc.|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract from CSV\n",
    "\n",
    "df_people = extract_csv(\"data/people.csv\", \"people_data\")\n",
    "df_people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "683f3aa9-4a83-429a-96b3-9f6fe53d47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|            start_at|              end_at|is_past|sequence|               title|          created_at|          updated_at|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|              1|             p:2|                   c:1|                NULL|                NULL|  false|       8|Co-Founder/CEO/Bo...|2007-05-25 07:03:...|2013-06-03 09:58:...|\n",
      "|              2|             p:3|                   c:1|                NULL|                NULL|  false|  279242|        VP Marketing|2007-05-25 07:04:...|2010-05-21 16:31:...|\n",
      "|              3|             p:4|                   c:3|                NULL|                NULL|  false|       4|          Evangelist|2007-05-25 19:33:...|2013-06-29 13:36:...|\n",
      "|              4|             p:5|                   c:3|2006-03-01 00:00:...|2009-12-01 00:00:...|  false|       4|Senior Director S...|2007-05-25 19:34:...|2013-06-29 10:25:...|\n",
      "|              6|             p:7|                   c:4|2005-07-01 00:00:...|2010-04-05 00:00:...|  false|       1|Chief Executive O...|2007-05-25 20:05:...|2010-04-05 18:41:...|\n",
      "|              7|             p:8|                   c:4|                NULL|                NULL|  false|       1|Senior Software E...|2007-05-25 20:06:...|2010-01-12 01:13:...|\n",
      "|              8|             p:9|                   c:4|                NULL|                NULL|  false|       1|Systems Engineeri...|2007-05-25 20:07:...|2010-08-03 22:00:...|\n",
      "|              9|            p:10|                   c:5|                NULL|                NULL|  false|       1|Founder and CEO, ...|2007-05-25 21:51:...|2010-01-25 21:49:...|\n",
      "|             10|            p:11|                   c:5|                NULL|                NULL|  false|       1|          Co-Founder|2007-05-25 22:15:...|2011-08-11 23:48:...|\n",
      "|             11|            p:12|                   c:5|                NULL|                NULL|  false|       3|Chief Revenue Off...|2007-05-25 22:15:...|2010-01-25 21:49:...|\n",
      "|             12|            p:13|                   c:5|                NULL|                NULL|  false|       1|VP of Product Man...|2007-05-25 22:16:...|2010-01-25 21:49:...|\n",
      "|             13|            p:14|                   c:5|1993-07-04 00:00:...|1982-02-28 00:00:...|  false|       1|          Co-founder|2007-05-25 22:17:...|2010-09-01 18:15:...|\n",
      "|             14|            p:16|                c:7299|                NULL|                NULL|  false|       2|     Founder and CEO|2007-05-26 12:44:...|2011-07-19 00:05:...|\n",
      "|             15|            p:17|                c:7299|                NULL|                NULL|  false|       2|  Co-founder and CTO|2007-05-26 12:45:...|2011-07-19 00:05:...|\n",
      "|             16|            p:18|                c:7299|2005-09-01 00:00:...|2009-10-01 00:00:...|  false|       1|SVP, Technology a...|2007-05-26 12:47:...|2011-07-19 00:05:...|\n",
      "|             17|            p:19|                c:7299|2006-08-01 00:00:...|2009-08-01 00:00:...|  false|       8|VP, Finance & Adm...|2007-05-26 12:47:...|2011-10-28 03:08:...|\n",
      "|             18|            p:20|                c:7299|                NULL|                NULL|  false|       1|           VP, Sales|2007-05-26 12:48:...|2011-07-19 00:05:...|\n",
      "|             19|            p:21|                c:7299|                NULL|                NULL|  false|       1|VP, Customer Service|2007-05-26 12:49:...|2011-07-19 00:05:...|\n",
      "|             20|            p:22|                c:7299|2005-10-01 00:00:...|2008-03-01 00:00:...|  false|       1|VP, Business Deve...|2007-05-26 12:49:...|2013-08-07 08:51:...|\n",
      "|             21|            p:23|                c:7299|                NULL|                NULL|  false|       1|     VP, Engineering|2007-05-26 12:50:...|2011-07-19 00:05:...|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|            start_at|              end_at|is_past|sequence|               title|          created_at|          updated_at|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|              1|             p:2|                   c:1|                NULL|                NULL|  false|       8|Co-Founder/CEO/Bo...|2007-05-25 07:03:...|2013-06-03 09:58:...|\n",
      "|              2|             p:3|                   c:1|                NULL|                NULL|  false|  279242|        VP Marketing|2007-05-25 07:04:...|2010-05-21 16:31:...|\n",
      "|              3|             p:4|                   c:3|                NULL|                NULL|  false|       4|          Evangelist|2007-05-25 19:33:...|2013-06-29 13:36:...|\n",
      "|              4|             p:5|                   c:3|2006-03-01 00:00:...|2009-12-01 00:00:...|  false|       4|Senior Director S...|2007-05-25 19:34:...|2013-06-29 10:25:...|\n",
      "|              6|             p:7|                   c:4|2005-07-01 00:00:...|2010-04-05 00:00:...|  false|       1|Chief Executive O...|2007-05-25 20:05:...|2010-04-05 18:41:...|\n",
      "|              7|             p:8|                   c:4|                NULL|                NULL|  false|       1|Senior Software E...|2007-05-25 20:06:...|2010-01-12 01:13:...|\n",
      "|              8|             p:9|                   c:4|                NULL|                NULL|  false|       1|Systems Engineeri...|2007-05-25 20:07:...|2010-08-03 22:00:...|\n",
      "|              9|            p:10|                   c:5|                NULL|                NULL|  false|       1|Founder and CEO, ...|2007-05-25 21:51:...|2010-01-25 21:49:...|\n",
      "|             10|            p:11|                   c:5|                NULL|                NULL|  false|       1|          Co-Founder|2007-05-25 22:15:...|2011-08-11 23:48:...|\n",
      "|             11|            p:12|                   c:5|                NULL|                NULL|  false|       3|Chief Revenue Off...|2007-05-25 22:15:...|2010-01-25 21:49:...|\n",
      "|             12|            p:13|                   c:5|                NULL|                NULL|  false|       1|VP of Product Man...|2007-05-25 22:16:...|2010-01-25 21:49:...|\n",
      "|             13|            p:14|                   c:5|1993-07-04 00:00:...|1982-02-28 00:00:...|  false|       1|          Co-founder|2007-05-25 22:17:...|2010-09-01 18:15:...|\n",
      "|             14|            p:16|                c:7299|                NULL|                NULL|  false|       2|     Founder and CEO|2007-05-26 12:44:...|2011-07-19 00:05:...|\n",
      "|             15|            p:17|                c:7299|                NULL|                NULL|  false|       2|  Co-founder and CTO|2007-05-26 12:45:...|2011-07-19 00:05:...|\n",
      "|             16|            p:18|                c:7299|2005-09-01 00:00:...|2009-10-01 00:00:...|  false|       1|SVP, Technology a...|2007-05-26 12:47:...|2011-07-19 00:05:...|\n",
      "|             17|            p:19|                c:7299|2006-08-01 00:00:...|2009-08-01 00:00:...|  false|       8|VP, Finance & Adm...|2007-05-26 12:47:...|2011-10-28 03:08:...|\n",
      "|             18|            p:20|                c:7299|                NULL|                NULL|  false|       1|           VP, Sales|2007-05-26 12:48:...|2011-07-19 00:05:...|\n",
      "|             19|            p:21|                c:7299|                NULL|                NULL|  false|       1|VP, Customer Service|2007-05-26 12:49:...|2011-07-19 00:05:...|\n",
      "|             20|            p:22|                c:7299|2005-10-01 00:00:...|2008-03-01 00:00:...|  false|       1|VP, Business Deve...|2007-05-26 12:49:...|2013-08-07 08:51:...|\n",
      "|             21|            p:23|                c:7299|                NULL|                NULL|  false|       1|     VP, Engineering|2007-05-26 12:50:...|2011-07-19 00:05:...|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract from CSV\n",
    "\n",
    "df_relations = extract_csv(\"data/relationships.csv\", \"relationships_data\")\n",
    "df_relations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73f9ea-4216-46a4-91e4-cd4bdb943e39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58919bd4-a86e-46ed-a849-bf236d617ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variable for Database (data source)\n",
    "\n",
    "DB_URL = os.getenv(\"DB_URL\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029c35a1-6309-440c-a1d1-6427bfe1eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_db():\n",
    "    try:\n",
    "        # Get list of tables from the database\n",
    "        table_list = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DB_URL) \\\n",
    "            .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "            .option(\"user\", DB_USER) \\\n",
    "            .option(\"password\", DB_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .filter(\"table_schema = 'public'\") \\\n",
    "            .select(\"table_name\") \\\n",
    "            .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print(f\"Found tables: {table_list}\")\n",
    "\n",
    "        tables = {}\n",
    "        for table in table_list:\n",
    "            try:\n",
    "                # Read each table into a DataFrame\n",
    "                df = spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", DB_URL) \\\n",
    "                    .option(\"dbtable\", table) \\\n",
    "                    .option(\"user\", DB_USER) \\\n",
    "                    .option(\"password\", DB_PASS) \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()\n",
    "\n",
    "                tables[table] = df\n",
    "\n",
    "                # Log success for each table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": \"Success\",\n",
    "                    \"source\": \"PostgreSQL\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "\n",
    "                print(f\"Successfully extracted table: {table}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log failure for specific table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": f\"Failed: {e}\",\n",
    "                    \"source\": \"PostgreSQL\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "                print(f\"Failed to extract table: {table} - Error: {e}\")\n",
    "\n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure for the whole extraction process\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"PostgreSQL\",\n",
    "            \"table_name\": \"N/A\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        print(f\"Failed to extract tables: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7557a91-4ad6-4d9e-ba84-3482d3b77027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables: ['company', 'acquisition', 'funding_rounds', 'funds', 'investments', 'ipos']\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: company\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: acquisition\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funding_rounds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: investments\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: ipos\n",
      "Extracted tables: ['company', 'acquisition', 'funding_rounds', 'funds', 'investments', 'ipos']\n"
     ]
    }
   ],
   "source": [
    "# Extract from db\n",
    "\n",
    "tables = extract_from_db()\n",
    "print(f\"Extracted tables: {list(tables.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fe3be2a-e7cf-4d7d-978d-b9135c7bf937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|office_id|object_id|      description|              region|            address1|  address2|          city|  zip_code|state_code|country_code| latitude|  longitude|         created_at|         updated_at|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|        8|      c:8|                 |              SF Bay|959 Skyway Road, ...|          |    San Carlos|     94070|        CA|         USA|37.506885|-122.247573|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|        9|      c:9|     Headquarters|         Los Angeles|9229 W. Sunset Blvd.|          |West Hollywood|     90069|        CA|         USA|34.090368|-118.393064|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       10|     c:10|                 |         Los Angeles|8536 National Blv...|          |   Culver City|     90232|        CA|         USA|34.025958|-118.379768|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       11|     c:11|                 |         Los Angeles|      407 N Maple Dr|          | Beverly Hills|     90210|        CA|         USA|34.076179|-118.394170|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       12|     c:12|                 |              SF Bay|     1355 Market St.|          | San Francisco|     94103|        CA|         USA|37.776805|-122.416924|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       14|     c:14|                 |              SF Bay|                    |          |    Menlo Park|          |        CA|         USA|37.484130|-122.169472|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       15|     c:15|               HQ|              SF Bay|   539 Bryant Street|          | San Francisco|     94107|        CA|         USA|37.789634|-122.404052|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       16|     c:16|                 |           San Diego|16935 W. Bernardo...|          |     San Diego|     92127|        CA|         USA|33.022176|-117.081406|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       17|     c:18|Lala Headquarters|              SF Bay|    209 Hamilton Ave|Suite #200|     Palo Alto|     94301|        CA|         USA|37.451151|-122.154369|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       18|     c:19|                 |         Los Angeles|10960 Wilshire Blvd.| Suite 700|   Los Angeles|     90024|        CA|         USA|34.057498|-118.446596|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       19|     c:20|     Headquarters|              SF Bay|2145 Hamilton Avenue|          |      San Jose|     95125|        CA|         USA|37.295005|-121.930035|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       20|     c:21|                 |United States - O...|                    |          |              |          |          |         USA|37.090240| -95.712891|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       21|     c:22|                 |            New York|                    |          | New York City|          |        NY|         USA|40.757929| -73.985506|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       22|     c:23|                 |            New York|    100 5th Ave Fl 6|          |      New York|10011-6903|        NY|         USA|40.746497| -74.009447|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       23|     c:24|                 |  California - Other|                    |          |              |          |        CA|         USA|37.269175|-119.306607|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       24|     c:25|               HQ|            New York|       1515 Broadway|          |      New York|     10036|        NY|         USA|40.757725| -73.986011|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       25|     c:26|                 |              London|                    |          |        London|          |          |         GBR|53.344104|  -6.267494|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       26|     c:27|               HQ|              SF Bay|1050 Enterprise W...|          |     Sunnyvale|     94089|        CA|         USA|37.387845|-122.055197|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       27|     c:28|     Headquarters|              SF Bay| 170 West Tasman Dr.|          |      San Jose|     95134|        CA|         USA|37.408802|-121.953770|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       28|     c:29|                 |              SF Bay|    701 First Avenue|          |     Sunnyvale|     94089|        CA|         USA|37.418531|-122.025485|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all table\n",
    "\n",
    "df_acquisition = tables[\"acquisition\"]\n",
    "df_company = tables[\"company\"]\n",
    "df_funding_rounds = tables[\"funding_rounds\"]\n",
    "df_funds = tables[\"funds\"]\n",
    "df_investments = tables[\"investments\"]\n",
    "df_ipos = tables[\"ipos\"]\n",
    "\n",
    "# check\n",
    "df_company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd8521-1d76-4405-858b-3fbf4a99f1d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e095748-b6b8-4e62-b2f4-980a2db88053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba254169-f554-44c9-ad58-88b2a6b19ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(link_api: str, list_parameter: dict, data_name: str):\n",
    "    try:\n",
    "        # Establish connection to API\n",
    "        resp = requests.get(link_api, params=list_parameter)\n",
    "        resp.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the response JSON\n",
    "        raw_response = resp.json()\n",
    "\n",
    "        # Convert JSON data to pandas DataFrame\n",
    "        df_api = pd.DataFrame(raw_response)\n",
    "\n",
    "        if df_api.empty:\n",
    "            raise ValueError(\"Empty response from API\")\n",
    "\n",
    "        # Convert pandas DataFrame to PySpark DataFrame\n",
    "        spark_df = spark.createDataFrame(df_api)\n",
    "\n",
    "        # Log success\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "\n",
    "        print(f\"Successfully extracted data from API: {data_name}\")\n",
    "        return spark_df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Log request failure\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "        print(f\"Request failed: {e}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        # Log parsing failure\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "        print(f\"Parsing error: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other errors\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c186f5f1-1954-4c9d-b19a-9f6e97e1a860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted data from API: milestones\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "|          created_at|         description|milestone_at|milestone_code|milestone_id|object_id|  source_description|          source_url|          updated_at|\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "|2008-06-18 08:14:...|Survives iPhone 3...|  2008-06-09|         other|           1|     c:12|Twitter Fails To ...|http://www.techcr...|2008-06-18 08:14:...|\n",
      "|2008-06-18 08:50:...|More than 4 Billi...|  2008-06-18|         other|           3|     c:59|11 Billion Videos...|http://www.comsco...|2008-06-18 08:50:...|\n",
      "|2008-06-19 04:14:...|Reddit goes Open ...|  2008-06-18|         other|           4|    c:314|reddit goes open ...|http://blog.reddi...|2008-06-19 04:14:...|\n",
      "|2008-06-19 04:15:...|Adds the ability ...|  2008-01-22|         other|           5|    c:314|Reddit Adds Abili...|http://www.techcr...|2008-06-19 04:15:...|\n",
      "|2008-06-19 04:39:...|Adobe announced n...|  2008-06-18|         other|           6|    c:283|                 NaN|                 NaN|2008-06-19 04:39:...|\n",
      "|2008-06-19 08:29:...|Closed release of...|  2008-02-01|         other|           7|   c:6816|                 NaN|                 NaN|2008-06-19 16:55:...|\n",
      "|2008-06-19 08:29:...|Closed release of...|  2008-05-01|         other|           8|   c:6816|Diary of a Mobile...|http://mobverge.b...|2008-06-19 16:55:...|\n",
      "|2008-06-19 15:29:...|Scoofers, a new s...|  2008-06-19|         other|           9|   c:5874|Official launch o...|http://scoofers.b...|2008-06-19 19:34:...|\n",
      "|2008-06-19 17:48:...|Jeremy Zawodny le...|  2008-07-01|         other|          10|   c:2034|I'm Joining Craig...|http://jeremy.zaw...|2008-06-19 17:48:...|\n",
      "|2008-06-19 18:54:...|SnapLogic release...|  2008-04-23|         other|          11|   c:5085|SnapLogic release...|http://venturebea...|2008-06-19 18:54:...|\n",
      "|2008-06-20 02:06:...|Joshua Schachter ...|  2008-06-19|         other|          13|     c:75|It Gets Worse: Jo...|http://www.techcr...|2008-06-20 02:06:...|\n",
      "|2008-06-20 21:19:...|       Bevy Launches|  2008-05-15|         other|          14|   c:6947|            Facebook|http://apps.faceb...|2008-07-18 01:29:...|\n",
      "|2008-06-22 19:10:...|Twitter partners ...|  2008-06-09|         other|          15|     c:12|Twitter Partners ...|http://www.techcr...|2008-06-22 19:10:...|\n",
      "|2008-06-22 19:15:...|Launches a status...|  2008-05-28|         other|          16|     c:12|    What's Going On?|http://blog.twitt...|2008-06-22 19:15:...|\n",
      "|2008-06-22 19:15:...|Twitter Starts Bl...|  2008-05-07|         other|          17|     c:12|Twitter Starts Bl...|http://www.techcr...|2008-06-22 19:15:...|\n",
      "|2008-06-22 19:17:...|VP Lee Mighdoll L...|  2008-04-24|         other|          18|     c:12|VP Lee Mighdoll O...|http://www.techcr...|2008-06-22 19:17:...|\n",
      "|2008-06-22 19:19:...|Chief Architect B...|  2008-04-23|         other|          19|     c:12|Amateur Hour Over...|http://www.techcr...|2008-06-22 19:19:...|\n",
      "|2008-06-22 19:20:...|Twitter Japan Lau...|  2008-04-22|         other|          20|     c:12|Twitter! Japan! Ads!|http://www.techcr...|2008-06-22 19:20:...|\n",
      "|2008-06-22 19:21:...|Twitter helps bus...|  2008-04-16|         other|          21|     c:12|Twitter Saves Man...|http://www.techcr...|2008-06-23 19:40:...|\n",
      "|2008-06-23 01:49:...|Sometrics has int...|  2008-06-03|         other|          24|   c:5411|Sometrics Launche...|http://publicatio...|2008-06-23 19:16:...|\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract from API (year 2008 - 2010)\n",
    "\n",
    "link_api = \"https://api-milestones.vercel.app/api/data\"\n",
    "list_parameter = {\n",
    "    \"start_date\": \"2008-01-01\",\n",
    "    \"end_date\": \"2010-12-31\"\n",
    "}\n",
    "\n",
    "df_milestones = extract_api(link_api, list_parameter, \"milestones\")\n",
    "df_milestones.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35082a-3994-4b7f-bcf2-37ec93ff032c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load - Staging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ac65e0-9c28-4a03-a51e-96fe5b481438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84df23db-7cc2-4706-9c7e-467bf86376b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pangres in /opt/conda/lib/python3.11/site-packages (4.2.1)\n",
      "Requirement already satisfied: pandas>=0.25.3 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.12 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.22)\n",
      "Requirement already satisfied: alembic>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from pangres) (1.12.0)\n",
      "Requirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.11/site-packages (from pangres) (23.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (1.24.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.3.12->pangres) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->pangres) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.3.1->pangres) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pangres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45b33ca6-54a8-45e6-96ea-241d6f62e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variable for Staging\n",
    "\n",
    "DB_STAGING_URL = os.getenv(\"DB_STAGING_URL\")\n",
    "DB_STAGING_USER = os.getenv(\"DB_STAGING_USER\")\n",
    "DB_STAGING_PASS = os.getenv(\"DB_STAGING_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "968be5ca-e796-4b3b-b539-8939450416f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "from pangres import upsert\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_staging2(df, table_name, mode=\"overwrite\", use_upsert=False, idx_name=None, schema=None, source=None):\n",
    "    try:\n",
    "        if use_upsert:\n",
    "            # Convert Spark DataFrame to Pandas DataFrame\n",
    "            data = df.toPandas()\n",
    "\n",
    "            # Create connection to PostgreSQL\n",
    "            conn = create_engine(f\"postgresql://{DB_STAGING_USER}:{DB_STAGING_PASS}@host.docker.internal:5432/pyspark_task_staging\")\n",
    "\n",
    "            # Set index for upsert\n",
    "            if idx_name is None:\n",
    "                raise ValueError(\"Index name is required for upsert mode\")\n",
    "\n",
    "            data = data.set_index(idx_name)\n",
    "\n",
    "            # Upsert\n",
    "            upsert(\n",
    "                con=conn,\n",
    "                df=data,\n",
    "                table_name=table_name,\n",
    "                schema=schema,\n",
    "                if_row_exists=\"update\"\n",
    "            )\n",
    "            print(f\"Data upserted to table '{table_name}' successfully!\")\n",
    "        else:\n",
    "            # Load using Spark\n",
    "            df.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", \"jdbc:postgresql://host.docker.internal:5432/pyspark_task_staging\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", DB_STAGING_USER) \\\n",
    "                .option(\"password\", DB_STAGING_PASS) \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .mode(mode) \\\n",
    "                .save()\n",
    "\n",
    "            print(f\"Data loaded to table '{table_name}' successfully!\")\n",
    "\n",
    "        # Success log\n",
    "        log_msg = {\n",
    "            \"step\": \"Load Staging\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to table '{table_name}': {e}\")\n",
    "\n",
    "        # Failed DataFrame\n",
    "        failed_data = df.toPandas() if not use_upsert else data\n",
    "        failed_data['error_message'] = str(e)\n",
    "        failed_data['etl_date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Fail log\n",
    "        log_msg = {\n",
    "            \"step\": \"Load Staging\",\n",
    "            \"status\": \"Failed\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_message\": str(e)\n",
    "        }\n",
    "\n",
    "        # Save failed data to CSV\n",
    "        failed_log_path = f'logs/failed_{table_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        failed_data.to_csv(failed_log_path, index=False)\n",
    "        print(f\"Failed data saved to: {failed_log_path}\")\n",
    "\n",
    "    finally:\n",
    "        # Delete error_message before save it to log\n",
    "        if 'error_message' in log_msg:\n",
    "            del log_msg['error_message']\n",
    "\n",
    "        # Simpan log ke CSV\n",
    "        log_to_csv(log_msg, 'etl_log.csv')\n",
    "\n",
    "    return df if not use_upsert else data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d6a2305-57a6-49b1-b57f-cf37f11a2a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'milestones' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: string, description: string, milestone_at: string, milestone_code: string, milestone_id: bigint, object_id: string, source_description: string, source_url: string, updated_at: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from API\n",
    "\n",
    "load_staging2(df_milestones, \"milestones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03e436bf-6909-4dbe-8a4f-9ad66ee5e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'relationship' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'people' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[people_id: string, object_id: string, first_name: string, last_name: string, birthplace: string, affiliation_name: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from CSV\n",
    "\n",
    "load_staging2(df_relations, \"relationship\")\n",
    "load_staging2(df_people, \"people\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "121a8a94-b4c4-4957-aaa4-2fb5dba19e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'acquisition' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'funding_rounds' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'funds' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'investments' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'ipo' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'company' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[office_id: int, object_id: string, description: string, region: string, address1: string, address2: string, city: string, zip_code: string, state_code: string, country_code: string, latitude: decimal(9,6), longitude: decimal(9,6), created_at: timestamp, updated_at: timestamp]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from database\n",
    "\n",
    "load_staging2(df_acquisition, \"acquisition\") \n",
    "load_staging2(df_funding_rounds, \"funding_rounds\") \n",
    "load_staging2(df_funds, \"funds\")\n",
    "load_staging2(df_investments, \"investments\")\n",
    "load_staging2(df_ipos, \"ipo\")\n",
    "load_staging2(df_company, \"company\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771dc6c-3890-4437-8cbb-215f4e0aab11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract Data from Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f576786a-b8bb-43b0-9740-3c34d1311910",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4f7905f-e399-4aa4-ae59-7a0b1713abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variable for Staging\n",
    "\n",
    "DB_STAGING_URL = os.getenv(\"DB_STAGING_URL\")\n",
    "DB_STAGING_USER = os.getenv(\"DB_STAGING_USER\")\n",
    "DB_STAGING_PASS = os.getenv(\"DB_STAGING_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebcd5088-00c6-40a8-b4f2-5ddedf46c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_staging():\n",
    "    try:\n",
    "        # Get list of tables from staging\n",
    "        table_list = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DB_STAGING_URL) \\\n",
    "            .option(\"dbtable\", \"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public') AS tbl\") \\\n",
    "            .option(\"user\", DB_STAGING_USER) \\\n",
    "            .option(\"password\", DB_STAGING_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"table_name\") \\\n",
    "            .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print(f\"Found tables in staging: {table_list}\")\n",
    "\n",
    "        tables = {}\n",
    "        for table in table_list:\n",
    "            try:\n",
    "                # Read each table into a DataFrame\n",
    "                df = spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", DB_STAGING_URL) \\\n",
    "                    .option(\"dbtable\", table) \\\n",
    "                    .option(\"user\", DB_STAGING_USER) \\\n",
    "                    .option(\"password\", DB_STAGING_PASS) \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()\n",
    "\n",
    "                tables[table] = df\n",
    "\n",
    "                # Log success for each table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": \"Success\",\n",
    "                    \"source\": \"PostgreSQL (Staging)\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "\n",
    "                print(f\"Successfully extracted table: {table}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log failure for specific table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": f\"Failed: {e}\",\n",
    "                    \"source\": \"PostgreSQL (Staging)\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "                print(f\"Failed to extract table: {table} - Error: {e}\")\n",
    "\n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure for the whole extraction process\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"PostgreSQL (Staging)\",\n",
    "            \"table_name\": \"N/A\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        print(f\"Failed to extract tables: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e0eee4a-955a-482b-bec9-eb7fa9ab5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables in staging: ['investments', 'relationship', 'people', 'ipo', 'company', 'acquisition', 'funding_rounds', 'milestones', 'funds']\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: investments\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: relationship\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: people\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: ipo\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: company\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: acquisition\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funding_rounds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: milestones\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funds\n",
      "Extracted tables: ['investments', 'relationship', 'people', 'ipo', 'company', 'acquisition', 'funding_rounds', 'milestones', 'funds']\n"
     ]
    }
   ],
   "source": [
    "# Extract All Tables from Staging\n",
    "\n",
    "data = extract_from_staging()\n",
    "print(f\"Extracted tables: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "21bb0c38-f907-470a-8242-80a13d02ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|office_id|object_id|      description|              region|            address1|  address2|          city|  zip_code|state_code|country_code| latitude|  longitude|         created_at|         updated_at|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|        8|      c:8|                 |              SF Bay|959 Skyway Road, ...|          |    San Carlos|     94070|        CA|         USA|37.506885|-122.247573|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|        9|      c:9|     Headquarters|         Los Angeles|9229 W. Sunset Blvd.|          |West Hollywood|     90069|        CA|         USA|34.090368|-118.393064|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       10|     c:10|                 |         Los Angeles|8536 National Blv...|          |   Culver City|     90232|        CA|         USA|34.025958|-118.379768|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       11|     c:11|                 |         Los Angeles|      407 N Maple Dr|          | Beverly Hills|     90210|        CA|         USA|34.076179|-118.394170|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       12|     c:12|                 |              SF Bay|     1355 Market St.|          | San Francisco|     94103|        CA|         USA|37.776805|-122.416924|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       14|     c:14|                 |              SF Bay|                    |          |    Menlo Park|          |        CA|         USA|37.484130|-122.169472|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       15|     c:15|               HQ|              SF Bay|   539 Bryant Street|          | San Francisco|     94107|        CA|         USA|37.789634|-122.404052|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       16|     c:16|                 |           San Diego|16935 W. Bernardo...|          |     San Diego|     92127|        CA|         USA|33.022176|-117.081406|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       17|     c:18|Lala Headquarters|              SF Bay|    209 Hamilton Ave|Suite #200|     Palo Alto|     94301|        CA|         USA|37.451151|-122.154369|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       18|     c:19|                 |         Los Angeles|10960 Wilshire Blvd.| Suite 700|   Los Angeles|     90024|        CA|         USA|34.057498|-118.446596|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       19|     c:20|     Headquarters|              SF Bay|2145 Hamilton Avenue|          |      San Jose|     95125|        CA|         USA|37.295005|-121.930035|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       20|     c:21|                 |United States - O...|                    |          |              |          |          |         USA|37.090240| -95.712891|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       21|     c:22|                 |            New York|                    |          | New York City|          |        NY|         USA|40.757929| -73.985506|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       22|     c:23|                 |            New York|    100 5th Ave Fl 6|          |      New York|10011-6903|        NY|         USA|40.746497| -74.009447|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       23|     c:24|                 |  California - Other|                    |          |              |          |        CA|         USA|37.269175|-119.306607|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       24|     c:25|               HQ|            New York|       1515 Broadway|          |      New York|     10036|        NY|         USA|40.757725| -73.986011|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       25|     c:26|                 |              London|                    |          |        London|          |          |         GBR|53.344104|  -6.267494|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       26|     c:27|               HQ|              SF Bay|1050 Enterprise W...|          |     Sunnyvale|     94089|        CA|         USA|37.387845|-122.055197|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       27|     c:28|     Headquarters|              SF Bay| 170 West Tasman Dr.|          |      San Jose|     95134|        CA|         USA|37.408802|-121.953770|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       28|     c:29|                 |              SF Bay|    701 First Avenue|          |     Sunnyvale|     94089|        CA|         USA|37.418531|-122.025485|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read All Data from Staging\n",
    "\n",
    "acquisition = data[\"acquisition\"]\n",
    "company = data[\"company\"]\n",
    "funding_rounds = data[\"funding_rounds\"]\n",
    "funds = data[\"funds\"]\n",
    "investments = data[\"investments\"]\n",
    "ipos = data[\"ipo\"]\n",
    "milestones = data[\"milestones\"]\n",
    "people = data[\"people\"]\n",
    "relationship = data[\"relationship\"]\n",
    "\n",
    "# check\n",
    "company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d6181-6ec3-4c5d-8dd7-83623e0ebd1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0e38fdd-8ff5-4ca5-9d28-385f85826723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "from decimal import Decimal\n",
    "\n",
    "# Helper function to convert values to JSON format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        return obj.isoformat()\n",
    "    return obj\n",
    "\n",
    "def profile_data(person, df, table_name, format_file):\n",
    "    try:\n",
    "        n_rows = df.count()\n",
    "        n_cols = len(df.columns)\n",
    "        \n",
    "        column_info = {}\n",
    "        for col in df.columns:\n",
    "            data_type = df.schema[col].dataType.simpleString()\n",
    "            sample_values = df.select(col).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "            null_count = df.filter(df[col].isNull()).count()\n",
    "            unique_count = df.select(col).distinct().count()\n",
    "            \n",
    "            # Min and max values (if numeric or date type)\n",
    "            try:\n",
    "                min_value = df.agg({col: \"min\"}).collect()[0][0]\n",
    "                max_value = df.agg({col: \"max\"}).collect()[0][0]\n",
    "            except:\n",
    "                min_value = None\n",
    "                max_value = None\n",
    "            \n",
    "            # Persentase missing value\n",
    "            percentage_missing = round((null_count / n_rows) * 100, 2) if n_rows > 0 else 0.0\n",
    "            \n",
    "            # Ambil 5 nilai unik sebagai sampel\n",
    "            unique_values = df.select(col).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "            \n",
    "            # Persentase valid date (khusus untuk tipe date dan datetime)\n",
    "            percentage_valid_date = None\n",
    "            if data_type in ['date', 'timestamp']:\n",
    "                valid_date_count = df.filter(df[col].isNotNull()).count()\n",
    "                percentage_valid_date = round((valid_date_count / n_rows) * 100, 2) if n_rows > 0 else 0.0\n",
    "\n",
    "            column_info[col] = {\n",
    "                \"data_type\": data_type,\n",
    "                \"sample_values\": [convert_to_serializable(v) for v in sample_values] if sample_values else None,\n",
    "                \"unique_count\": unique_count,\n",
    "                \"unique_value\": [convert_to_serializable(v) for v in unique_values] if unique_values else None,\n",
    "                \"null_count\": null_count,\n",
    "                \"percentage_missing_value\": percentage_missing,\n",
    "                \"min_value\": convert_to_serializable(min_value),\n",
    "                \"max_value\": convert_to_serializable(max_value),\n",
    "                \"percentage_valid_date\": percentage_valid_date\n",
    "            }\n",
    "        \n",
    "        dict_profiling = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"person_in_charge\": person,\n",
    "            \"profiling_result\": {\n",
    "                \"table_name\": table_name,\n",
    "                \"format_file\": format_file,\n",
    "                \"n_rows\": n_rows,\n",
    "                \"n_cols\": n_cols,\n",
    "                \"report\": column_info\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save profiling result to JSON\n",
    "        folder_path = \"data_profiling\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        file_path = os.path.join(folder_path, f\"{table_name}_profiling.json\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(dict_profiling, f, indent=4, default=convert_to_serializable)\n",
    "\n",
    "        print(f\"Profiling saved to: {file_path}\")\n",
    "\n",
    "        # Create success log message\n",
    "        log_msg = {\n",
    "            \"step\": \"Profiling\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": format_file,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error profiling table {table_name}: {e}\")\n",
    "\n",
    "        # Create fail log message\n",
    "        log_msg = {\n",
    "            \"step\": \"Profiling\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": format_file,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        # Save log to CSV\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "\n",
    "    return dict_profiling if 'dict_profiling' in locals() else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc1ec5e0-6c82-4871-9bd0-1e89098e1663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling saved to: data_profiling/people_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "{\n",
      "  \"created_at\": \"2025-03-22T09:50:39.336805\",\n",
      "  \"person_in_charge\": \"Mr. A\",\n",
      "  \"profiling_result\": {\n",
      "    \"table_name\": \"people_data\",\n",
      "    \"format_file\": \"from Staging\",\n",
      "    \"n_rows\": 226709,\n",
      "    \"n_cols\": 6,\n",
      "    \"report\": {\n",
      "      \"people_id\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"177264\",\n",
      "          \"296\",\n",
      "          \"91421\",\n",
      "          \"467\",\n",
      "          \"177595\"\n",
      "        ],\n",
      "        \"unique_count\": 226709,\n",
      "        \"unique_value\": [\n",
      "          \"177264\",\n",
      "          \"296\",\n",
      "          \"91421\",\n",
      "          \"467\",\n",
      "          \"177595\"\n",
      "        ],\n",
      "        \"null_count\": 0,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"1\",\n",
      "        \"max_value\": \"99999\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"object_id\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"p:105829\",\n",
      "          \"p:73\",\n",
      "          \"p:171\",\n",
      "          \"p:214\",\n",
      "          \"p:214744\"\n",
      "        ],\n",
      "        \"unique_count\": 226709,\n",
      "        \"unique_value\": [\n",
      "          \"p:105829\",\n",
      "          \"p:73\",\n",
      "          \"p:171\",\n",
      "          \"p:214\",\n",
      "          \"p:214744\"\n",
      "        ],\n",
      "        \"null_count\": 0,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"p:10\",\n",
      "        \"max_value\": \"p:99999\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"first_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Hideki\",\n",
      "          \"Tyler\",\n",
      "          \"Nell\",\n",
      "          \"K\",\n",
      "          \"Rony\"\n",
      "        ],\n",
      "        \"unique_count\": 28423,\n",
      "        \"unique_value\": [\n",
      "          \"Hideki\",\n",
      "          \"Tyler\",\n",
      "          \"Nell\",\n",
      "          \"K\",\n",
      "          \"Rony\"\n",
      "        ],\n",
      "        \"null_count\": 5,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"\\\"\\\"\\\"DJ\\\"\",\n",
      "        \"max_value\": \"\\u00de\\u00f3rlindur\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"last_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Broukhim\",\n",
      "          \"Francois\",\n",
      "          \"Nerst\",\n",
      "          \"Banerjee\",\n",
      "          \"Cancel\"\n",
      "        ],\n",
      "        \"unique_count\": 107774,\n",
      "        \"unique_value\": [\n",
      "          \"Broukhim\",\n",
      "          \"Francois\",\n",
      "          \"Nerst\",\n",
      "          \"Banerjee\",\n",
      "          \"Cancel\"\n",
      "        ],\n",
      "        \"null_count\": 1,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"!llmind\",\n",
      "        \"max_value\": \"\\u00fcz\\u00fcm\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"birthplace\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Gainesville, FL\",\n",
      "          \"Bangalore\",\n",
      "          \"jacksonville , FL\",\n",
      "          \"Brisbane, Australia\",\n",
      "          \"North Adams, MA\"\n",
      "        ],\n",
      "        \"unique_count\": 8273,\n",
      "        \"unique_value\": [\n",
      "          \"Gainesville, FL\",\n",
      "          \"Bangalore\",\n",
      "          \"jacksonville , FL\",\n",
      "          \"Brisbane, Australia\",\n",
      "          \"North Adams, MA\"\n",
      "        ],\n",
      "        \"null_count\": 198622,\n",
      "        \"percentage_missing_value\": 87.61,\n",
      "        \"min_value\": \" Jr.\\\"\",\n",
      "        \"max_value\": \"\\u00e4\\u00b8\\u00ad\\u00e5\\u009b\\u00bd\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"affiliation_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Blue Nile\",\n",
      "          \"Pandora Media\",\n",
      "          \"YouSendIt\",\n",
      "          \"Diet TV\",\n",
      "          \"SomethingSimpler\"\n",
      "        ],\n",
      "        \"unique_count\": 27250,\n",
      "        \"unique_value\": [\n",
      "          \"Blue Nile\",\n",
      "          \"Pandora Media\",\n",
      "          \"YouSendIt\",\n",
      "          \"Diet TV\",\n",
      "          \"SomethingSimpler\"\n",
      "        ],\n",
      "        \"null_count\": 21,\n",
      "        \"percentage_missing_value\": 0.01,\n",
      "        \"min_value\": \"! Haz Life\",\n",
      "        \"max_value\": \"\\u00d6resundswebb\",\n",
      "        \"percentage_valid_date\": null\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "profiling_result = profile_data(\"Mr. A\", people, \"people_data\", \"from Staging\")\n",
    "print(json.dumps(profiling_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc775e70-e9da-46d1-8042-68d886be2794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling saved to: data_profiling/relationship_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/acquisition_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/company_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/funding_rounds_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/funds_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/investments_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/ipos_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/milestones_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-03-22T09:53:41.820808',\n",
       " 'person_in_charge': 'Mrs. OP',\n",
       " 'profiling_result': {'table_name': 'milestones_data',\n",
       "  'format_file': 'from Staging',\n",
       "  'n_rows': 8152,\n",
       "  'n_cols': 9,\n",
       "  'report': {'created_at': {'data_type': 'string',\n",
       "    'sample_values': ['2010-09-30 04:46:05.000',\n",
       "     '2010-10-04 23:53:31.000',\n",
       "     '2010-05-26 23:08:38.000',\n",
       "     '2010-07-09 05:01:33.000',\n",
       "     '2010-07-10 12:53:28.000'],\n",
       "    'unique_count': 7504,\n",
       "    'unique_value': ['2010-09-30 04:46:05.000',\n",
       "     '2010-10-04 23:53:31.000',\n",
       "     '2010-05-26 23:08:38.000',\n",
       "     '2010-07-09 05:01:33.000',\n",
       "     '2010-07-10 12:53:28.000'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-06-18 08:14:06.000',\n",
       "    'max_value': '2013-12-10 20:15:30.000',\n",
       "    'percentage_valid_date': None},\n",
       "   'description': {'data_type': 'string',\n",
       "    'sample_values': [\"Viewfinity named in 'Hottest Boston Companies' List\",\n",
       "     'Centralway invested in \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n LLC \\x97 the largest US-based mail order portal in the Czech Republic.',\n",
       "     'Eyeblaster files for IPO',\n",
       "     'Arsago Launches UCITS LatAm Fund.',\n",
       "     'Sonicbids launches new venue database.'],\n",
       "    'unique_count': 8012,\n",
       "    'unique_value': [\"Viewfinity named in 'Hottest Boston Companies' List\",\n",
       "     'Centralway invested in \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n LLC \\x97 the largest US-based mail order portal in the Czech Republic.',\n",
       "     'Eyeblaster files for IPO',\n",
       "     'Arsago Launches UCITS LatAm Fund.',\n",
       "     'Sonicbids launches new venue database.'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompleted the raise of StarVest Partners II, a $245 million fund in January 2009',\n",
       "    'max_value': 'â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£ â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_at': {'data_type': 'string',\n",
       "    'sample_values': ['2009-12-04',\n",
       "     '2009-06-23',\n",
       "     '2010-09-24',\n",
       "     '2010-02-12',\n",
       "     '2008-12-03'],\n",
       "    'unique_count': 924,\n",
       "    'unique_value': ['2009-12-04',\n",
       "     '2009-06-23',\n",
       "     '2010-09-24',\n",
       "     '2010-02-12',\n",
       "     '2008-12-03'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-01-01',\n",
       "    'max_value': '2010-12-31',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_code': {'data_type': 'string',\n",
       "    'sample_values': ['other'],\n",
       "    'unique_count': 1,\n",
       "    'unique_value': ['other'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'other',\n",
       "    'max_value': 'other',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_id': {'data_type': 'bigint',\n",
       "    'sample_values': [2453, 5556, 4823, 2509, 2529],\n",
       "    'unique_count': 8152,\n",
       "    'unique_value': [2453, 5556, 4823, 2509, 2529],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 1,\n",
       "    'max_value': 39356,\n",
       "    'percentage_valid_date': None},\n",
       "   'object_id': {'data_type': 'string',\n",
       "    'sample_values': ['c:41922', 'c:58230', 'c:23012', 'c:57411', 'c:44410'],\n",
       "    'unique_count': 4065,\n",
       "    'unique_value': ['c:41922', 'c:58230', 'c:23012', 'c:57411', 'c:44410'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'c:1',\n",
       "    'max_value': 'f:9985',\n",
       "    'percentage_valid_date': None},\n",
       "   'source_description': {'data_type': 'string',\n",
       "    'sample_values': ['Zedira Raises Additional Funding',\n",
       "     'Muecs Ltd. Launches an Online Collaboration Tool for Enterprises With a Social Media Analyzer',\n",
       "     'Mugasha Launches Online Electronic Music Service',\n",
       "     'Datapipe Named Growth Company of the Year',\n",
       "     'Towerstream Launches Wireless Broadband Network in Philadelphia, PA'],\n",
       "    'unique_count': 5452,\n",
       "    'unique_value': ['Zedira Raises Additional Funding',\n",
       "     'Muecs Ltd. Launches an Online Collaboration Tool for Enterprises With a Social Media Analyzer',\n",
       "     'Mugasha Launches Online Electronic Music Service',\n",
       "     'Datapipe Named Growth Company of the Year',\n",
       "     'Towerstream Launches Wireless Broadband Network in Philadelphia, PA'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '\\t  After Whirlwind Year, Everyday Health To Tap Public Markets For $100M',\n",
       "    'max_value': 'â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£ â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90',\n",
       "    'percentage_valid_date': None},\n",
       "   'source_url': {'data_type': 'string',\n",
       "    'sample_values': ['http://techcrunch.com/2010/07/10/google-secretly-invested-100-million-in-zynga-preparing-to-launch-google-games/',\n",
       "     'http://www.tokyocamp.net/',\n",
       "     'http://www.pr.com/press-release/173572',\n",
       "     'http://blog.zooners.com/',\n",
       "     'http://paidcontent.org/article/419-ipg-shifts-initiatives-breen-to-reprise-media-as-global-ceo/'],\n",
       "    'unique_count': 6064,\n",
       "    'unique_value': ['http://techcrunch.com/2010/07/10/google-secretly-invested-100-million-in-zynga-preparing-to-launch-google-games/',\n",
       "     'http://www.tokyocamp.net/',\n",
       "     'http://www.pr.com/press-release/173572',\n",
       "     'http://blog.zooners.com/',\n",
       "     'http://paidcontent.org/article/419-ipg-shifts-initiatives-breen-to-reprise-media-as-global-ceo/'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'NaN',\n",
       "    'max_value': 'www://connectwithcoaches.com/',\n",
       "    'percentage_valid_date': None},\n",
       "   'updated_at': {'data_type': 'string',\n",
       "    'sample_values': ['2010-05-31 01:20:43.000',\n",
       "     '2010-07-10 12:53:28.000',\n",
       "     '2010-07-12 02:15:04.000',\n",
       "     '2010-03-10 18:47:20.000',\n",
       "     '2010-10-12 21:16:38.000'],\n",
       "    'unique_count': 7263,\n",
       "    'unique_value': ['2010-05-31 01:20:43.000',\n",
       "     '2010-07-10 12:53:28.000',\n",
       "     '2010-07-12 02:15:04.000',\n",
       "     '2010-03-10 18:47:20.000',\n",
       "     '2010-10-12 21:16:38.000'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-06-18 08:14:06.000',\n",
       "    'max_value': '2013-12-11 03:24:18.000',\n",
       "    'percentage_valid_date': None}}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Profiling All Data\n",
    "\n",
    "profile_data(\"Mr. CCC\", relationship, \"relationship_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. H\", acquisition, \"acquisition_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. OP\", company, \"company_data\", \"from Staging\")\n",
    "profile_data(\"Mr. CCC\", funding_rounds, \"funding_rounds_data\", \"from Staging\")\n",
    "profile_data(\"Mr. A\", funds, \"funds_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. H\", investments, \"investments_data\", \"from Staging\")\n",
    "profile_data(\"Mr. A\", ipos, \"ipos_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. OP\", milestones, \"milestones_data\", \"from Staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09876a95-9d25-4760-b97f-5c3e3d97974c",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18a58de3-97fc-48d0-86cc-1170986a34c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.11/site-packages (1.3.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9f2c371-b617-4545-8cf4-8ba990ce3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, to_date, when, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from sqlalchemy import create_engine\n",
    "from pangres import upsert\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dfb78a6-0390-4e98-8b0f-a0be8874ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Connection to Data Warehouse\n",
    "\n",
    "DWH_URL = os.getenv(\"DWH_URL\")\n",
    "DWH_USER = os.getenv(\"DWH_USER\")\n",
    "DWH_PASS = os.getenv(\"DWH_PASS\")\n",
    "engine = create_engine(f\"postgresql://{DWH_USER}:{DWH_PASS}@host.docker.internal:5432/pyspark_task_dwh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "642c3e18-0fa0-459c-818f-f76d57b49c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Save and Track Invalid IDs\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def save_invalid_ids(invalid_ids, table_name, folder='logs', filename='invalid_ids.csv'):\n",
    "    if not invalid_ids:\n",
    "        print(f\"No invalid IDs to save from table '{table_name}'.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        \n",
    "        # Konversi list ke DataFrame\n",
    "        df = pd.DataFrame(invalid_ids, columns=['entity_type', 'object_id'])\n",
    "        df['table_name'] = table_name\n",
    "        df['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Kalau file sudah ada, cek duplikasi biar gak nambah terus\n",
    "        if os.path.exists(file_path):\n",
    "            existing_df = pd.read_csv(file_path)\n",
    "            # Gabung dataframe lalu drop duplikat\n",
    "            df = pd.concat([existing_df, df]).drop_duplicates(subset=['entity_type', 'object_id', 'table_name'])\n",
    "        \n",
    "        # Tulis ulang ke file (bukan append) biar data tetap konsisten\n",
    "        df.to_csv(file_path, mode='w', index=False)\n",
    "        \n",
    "        print(f\"{len(invalid_ids)} invalid IDs from table '{table_name}' saved to {file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving invalid IDs from table '{table_name}': {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b7b6f-ef77-4b84-882b-77e5cbef9b34",
   "metadata": {},
   "source": [
    "### Helper Function to Analyze Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8b02b5a-afcd-4af1-b603-d594850466b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Values to Make it More Readable\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def clean_integer(value):\n",
    "    if isinstance(value, str):\n",
    "        match = re.match(r\"^[a-zA-Z]:(\\d+)\", value) \n",
    "        if match:\n",
    "            return int(match.group(1))  # Catch value after \":\"\n",
    "        else:\n",
    "            return None \n",
    "    return value\n",
    "    \n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def clean_text(value):\n",
    "    if value:\n",
    "        try:\n",
    "            # Handle encoding issue \n",
    "            value = value.encode('latin1').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            pass\n",
    "        # Normalization\n",
    "        value = unicodedata.normalize(\"NFKD\", value)\n",
    "        # Handle strange character\n",
    "        value = re.sub(r'[^\\x00-\\x7F]+', '', value)\n",
    "        value = value.strip()\n",
    "        value = unidecode(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def normalize_text(value):\n",
    "    if not isinstance(value, str) or not value.strip():\n",
    "        return None  \n",
    "    \n",
    "    # Make it lowercase\n",
    "    value = value.lower()\n",
    "    \n",
    "    # HDelete strange char\n",
    "    value = re.sub(r'[^\\w\\s,&/]', '', value)  # Alphanumeric, space, coma, apersand, slash\n",
    "    \n",
    "    value = re.sub(r'[/,&]', ' ', value) \n",
    "    \n",
    "    # Delete exaggerated space\n",
    "    value = re.sub(r'\\s+', ' ', value).strip()\n",
    "    \n",
    "    return value\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def clean_alpha_text(text):\n",
    "    if text:\n",
    "        # Delete all strange char, except alphanumeric and space\n",
    "        return re.sub(r'[^\\w\\s]', '', text).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def fix_encoding(s):\n",
    "    if s is not None:\n",
    "        try:\n",
    "            return unidecode(s)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "663ad514-b2f2-423e-a9ac-1ef40543f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Extracting Prefix and Numeric ID\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_prefix(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[0]\n",
    "    return None\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def extract_id(value):\n",
    "    if value and \":\" in value:\n",
    "        try:\n",
    "            return int(value.split(\":\")[1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4e547c3-3257-47a8-944e-2cd8f5faf2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Handle Stock-related Column\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_stock_market(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[0]\n",
    "    return None\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_stock_symbol(value):\n",
    "    if value and \":\" in value:\n",
    "        return value.split(\":\")[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b194ac-fa94-48c5-8e1d-78eb41b8d816",
   "metadata": {},
   "source": [
    "### Company Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "037320c8-bd0e-46b0-a0e8-dfa5d4c2e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, split\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_company(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"latitude\", col(\"latitude\").cast(\"decimal(9,6)\"))\n",
    "        df = df.withColumn(\"longitude\", col(\"longitude\").cast(\"decimal(9,6)\"))\n",
    "        \n",
    "        # Extract Extract prefix and ID \n",
    "        df = df.withColumn(\"entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "\n",
    "        # Step 3: Encoding\n",
    "        df = df.withColumn(\"description\", clean_text(\"description\"))\n",
    "        df = df.withColumn(\"address1\", clean_text(\"address1\"))\n",
    "        df = df.withColumn(\"zip_code\", clean_text(\"zip_code\"))\n",
    "        df = df.withColumn(\"region\", clean_text(\"region\"))\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"office_id\").alias(\"company_id\"),\n",
    "            col(\"entity_type\").alias(\"entity_type\"),\n",
    "            col(\"object_id\").alias(\"object_id\"),  # INT\n",
    "            col(\"description\").alias(\"description\"),\n",
    "            col(\"address1\").alias(\"address\"),\n",
    "            col(\"region\").alias(\"region\"),\n",
    "            col(\"city\").alias(\"city\"),\n",
    "            col(\"zip_code\").alias(\"zip_code\"),\n",
    "            col(\"state_code\").alias(\"state_code\"),\n",
    "            col(\"country_code\").alias(\"country_code\"),\n",
    "            col(\"latitude\").alias(\"latitude\"),\n",
    "            col(\"longitude\").alias(\"longitude\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 5: Data cleansing \n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"description\": \"Unknown\",\n",
    "            \"address\": \"Unknown\",\n",
    "            \"region\": \"Unknown\",\n",
    "            \"city\": \"Unknown\",\n",
    "            \"zip_code\": \"Unknown\",\n",
    "            \"state_code\": \"Unknown\",\n",
    "            \"country_code\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicate data  and latitude/longitude with value = 0\n",
    "        df_transformed = df_transformed.dropDuplicates([\"object_id\"])\n",
    "        df_transformed = df_transformed.filter((col(\"latitude\") != 0) & (col(\"longitude\") != 0))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        return df_transformed\n",
    "\n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"company\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef586144-4ba6-4091-a89c-8aa71be742e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "company = spark.read \\\n",
    "       .format(\"jdbc\") \\\n",
    "       .option(\"url\", DB_STAGING_URL) \\\n",
    "       .option(\"dbtable\", \"company\") \\\n",
    "       .option(\"user\", DB_STAGING_USER) \\\n",
    "       .option(\"password\", DB_STAGING_PASS) \\\n",
    "       .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "       .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dbdfdee-7547-48c9-9388-caa2e92802a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    }
   ],
   "source": [
    "transformed_df = transform_company(company)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3153e-6e62-4fb0-b4aa-ef1dadfcb9e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### People Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea816120-0f53-4fc8-b20d-cf668aa60a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, broadcast\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def transform_people(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace empty strings with NULL\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Create full_name from first_name + last_name\n",
    "        df = df.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "        # Step 3: Extract prefix and ID dari object_id\n",
    "        df = df.withColumn(\"entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping to target kolom\n",
    "        df_transformed = df.select(\n",
    "            col(\"people_id\").alias(\"people_id\"),\n",
    "            col(\"entity_type\").alias(\"entity_type\"), \n",
    "            col(\"object_id\").alias(\"object_id\"),\n",
    "            col(\"full_name\").alias(\"full_name\"),\n",
    "            col(\"birthplace\").alias(\"birthplace\"),\n",
    "            col(\"affiliation_name\").alias(\"affiliation_name\"),\n",
    "        )\n",
    "\n",
    "        df_transformed = df_transformed.withColumn(\"object_id\", clean_integer(col(\"object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"full_name\", clean_alpha_text(col(\"full_name\")))\n",
    "        df_transformed = df_transformed.withColumn(\"birthplace\", fix_encoding(col(\"birthplace\")))\n",
    "        df_transformed = df_transformed.withColumn(\"affiliation_name\", clean_alpha_text(col(\"affiliation_name\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 5: Cleaning data\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"full_name\": \"Unknown\",\n",
    "            \"birthplace\": \"Unknown\",\n",
    "            \"affiliation_name\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop some data\n",
    "        df_transformed = df_transformed.dropDuplicates([\"entity_type\", \"object_id\"])\n",
    "        df_transformed = df_transformed.filter(col(\"full_name\") != \"Unknown\")\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 7: Validate object_id in `dim_company`\n",
    "        dim_company = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"object_id\")\n",
    "\n",
    "        # Valid data (match with `dim_company`)\n",
    "        df_valid = df_transformed.join(\n",
    "            broadcast(dim_company),\n",
    "            on=\"object_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "        # Invalid data\n",
    "        df_invalid = df_transformed.join(\n",
    "            broadcast(dim_company),\n",
    "            on=\"object_id\",\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            # Convert to pandas and save to csv using `save_invalid_ids`\n",
    "            invalid_ids = df_invalid.select(\"entity_type\", \"object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"people\")\n",
    "\n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"people\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        return df_valid\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"people\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bf49fa4-4565-494d-8d64-2a8011816b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "people = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"people\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5575a930-b015-48f6-9472-1954aa3c7058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_205/2324708217.py:23: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  existing_df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204464 invalid IDs from table 'people' saved to logs/invalid_ids.csv\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Transform People Data\n",
    "transformed_people = transform_people(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cff3a7-3d19-4893-babd-43b34e2ea879",
   "metadata": {},
   "source": [
    "### Milestones Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d710e0-d0b3-474c-b694-57524dd9bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_milestones(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "        df = df.na.replace(\"NaN\", None)\n",
    "        \n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"milestone_date\", to_date(col(\"milestone_at\")))\n",
    "\n",
    "        # Step 3: Extract prefix and ID dari object_id\n",
    "        df = df.withColumn(\"entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"milestone_id\").alias(\"milestone_id\"),\n",
    "            col(\"entity_type\").alias(\"entity_type\"),\n",
    "            col(\"object_id\").alias(\"object_id\"),\n",
    "            col(\"milestone_date\").alias(\"milestone_date\"),\n",
    "            col(\"description\").alias(\"description\"),\n",
    "            col(\"source_url\").alias(\"source_url\"),\n",
    "            col(\"source_description\").alias(\"source_description\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "            \n",
    "        # Step 5: Handle strange values\n",
    "        df_transformed = df_transformed.withColumn(\"object_id\", clean_integer(col(\"object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"description\", clean_alpha_text(\"description\"))\n",
    "        df_transformed = df_transformed.withColumn(\"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\"))\n",
    "        df_transformed = df_transformed.withColumn(\"source_description\", clean_alpha_text(\"source_description\"))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 6: Cleaning data\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"description\": \"No Description\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "        \n",
    "        # Step 7: Drop duplicate data \n",
    "        df_transformed = df_transformed.dropDuplicates([\"milestone_id\"])\n",
    "        df_transformed = df_transformed.dropDuplicates([\"entity_type\", \"object_id\"])\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 8: Validation object_id in `dim_company`\n",
    "        dim_company = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"object_id\")\n",
    "\n",
    "        # Convert object_id to integer (if needed)\n",
    "        df_transformed = df_transformed.withColumn(\"object_id\", col(\"object_id\").cast(\"int\"))\n",
    "        \n",
    "        # Filter NULL object_id explicitly\n",
    "        df_transformed = df_transformed.filter(col(\"object_id\").isNotNull())\n",
    "        \n",
    "        # Valid data (match with `dim_company`)\n",
    "        df_valid = df_transformed.join(\n",
    "            broadcast(dim_company),\n",
    "            on=\"object_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        \n",
    "        # Invalid data\n",
    "        df_invalid = df_transformed.join(\n",
    "            broadcast(dim_company),\n",
    "            on=\"object_id\",\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        \n",
    "        if df_invalid.count() > 0:\n",
    "            # Convert to pandas and save to csv using `save_invalid_ids`\n",
    "            invalid_ids = df_invalid.select(\"entity_type\", \"object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"milestone\")\n",
    "        \n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"milestone\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        return df_valid\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"milestones\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86caff59-f7f6-4d4f-90c1-ab320d95d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "milestones = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"milestones\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f787b43-5541-4409-b0bd-5a8466d89ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Milestones Data\n",
    "transformed_milestones = transform_milestones(milestones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09597a83-09dd-46d4-89ee-13eea7d7763a",
   "metadata": {},
   "source": [
    "### Acquisition Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5c715-69cc-4f9c-9f3c-4e9ed2b09580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_acquisition(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"price_amount\", col(\"price_amount\").cast(\"decimal(15,2)\"))\n",
    "        df = df.withColumn(\"acquired_at\", to_date(col(\"acquired_at\")))\n",
    "\n",
    "        # Step 3: Extract prefix and ID\n",
    "        df = df.withColumn(\"acquiring_entity_type\", extract_prefix(col(\"acquiring_object_id\")))\n",
    "        df = df.withColumn(\"acquired_entity_type\", extract_prefix(col(\"acquired_object_id\")))\n",
    "        df = df.withColumn(\"acquiring_object_id\", extract_id(col(\"acquiring_object_id\")))\n",
    "        df = df.withColumn(\"acquired_object_id\", extract_id(col(\"acquired_object_id\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"acquisition_id\"),\n",
    "            col(\"acquiring_entity_type\"),\n",
    "            col(\"acquiring_object_id\"),\n",
    "            col(\"acquired_entity_type\"),\n",
    "            col(\"acquired_object_id\"),\n",
    "            col(\"price_amount\"),\n",
    "            col(\"price_currency_code\"),\n",
    "            col(\"acquired_at\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Step 5: Clean integer values\n",
    "        df_transformed = df_transformed.withColumn(\"acquiring_object_id\", clean_integer(col(\"acquiring_object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"acquired_object_id\", clean_integer(col(\"acquired_object_id\")))\n",
    "        df_transformed = df_transformed.dropna(subset=\"acquired_at\")\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 6: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"price_amount\": 0.0,\n",
    "            \"price_currency_code\": \"Unknown\",\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"term_code\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 7: Drop duplicates dan unknown data\n",
    "        df_transformed = df_transformed.dropDuplicates([\"acquisition_id\"])\n",
    "        df_transformed = df_transformed.filter(col(\"term_code\") != \"Unknown\")\n",
    "        df_transformed = df_transformed.filter(col(\"price_currency_code\") != \"Unknown\")\n",
    "        df_transformed = df_transformed.filter(col(\"price_amount\") != 0.0)\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 8: Validation object_id in `dim_company`\n",
    "        dim_company = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"object_id\", \"company_id\")\n",
    "\n",
    "        # Valid data\n",
    "        df_valid = (\n",
    "            df_transformed\n",
    "            .join(broadcast(dim_company.alias(\"acquiring\")), col(\"acquiring_object_id\") == col(\"acquiring.object_id\"), \"left\")\n",
    "            .join(broadcast(dim_company.alias(\"acquired\")), col(\"acquired_object_id\") == col(\"acquired.object_id\"), \"left\")\n",
    "            .select(df_transformed[\"*\"],  \n",
    "                    col(\"acquiring.company_id\").alias(\"acquiring_company_id\"),  \n",
    "                    col(\"acquired.company_id\").alias(\"acquired_company_id\"))\n",
    "        )\n",
    "        \n",
    "        # Invalid data\n",
    "        df_invalid = df_valid.filter(col(\"acquiring_company_id\").isNull() | col(\"acquired_company_id\").isNull())\n",
    "        \n",
    "        df_valid = df_valid.filter(col(\"acquiring_company_id\").isNotNull() & col(\"acquired_company_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"acquiring_entity_type\", \"acquiring_object_id\", \"acquired_entity_type\", \"acquired_object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"acquisition\")\n",
    "            \n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"acquisition\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        return df_valid\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"ETL Process\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"acquisition\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee608a3-37bd-4339-bf48-6fb3850ac8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "acquisition = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"acquisition\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7052b-1c4b-4c27-8828-2b392d46bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform acquisition data\n",
    "\n",
    "transformed_acquisition = transform_acquisition(acquisition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54cf6c4-5a50-47be-9731-6d7768dc379e",
   "metadata": {},
   "source": [
    "### Investments Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c31b4-f469-4d38-98fb-dac68bcb26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_investments(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Extract prefix and ID\n",
    "        df = df.withColumn(\"funded_entity_type\", extract_prefix(col(\"funded_object_id\")))\n",
    "        df = df.withColumn(\"investor_entity_type\", extract_prefix(col(\"investor_object_id\")))\n",
    "        df = df.withColumn(\"funded_object_id\", extract_id(col(\"funded_object_id\")))\n",
    "        df = df.withColumn(\"investor_object_id\", extract_id(col(\"investor_object_id\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 3: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"investment_id\").alias(\"investment_id\"),\n",
    "            col(\"funding_round_id\").alias(\"funding_round_id\"),\n",
    "            col(\"funded_entity_type\").alias(\"funded_entity_type\"),\n",
    "            col(\"funded_object_id\").alias(\"funded_object_id\"),\n",
    "            col(\"investor_entity_type\").alias(\"investor_entity_type\"),\n",
    "            col(\"investor_object_id\").alias(\"investor_object_id\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Step 4: Clean integer values\n",
    "        df_transformed = df_transformed.withColumn(\"funded_object_id\", clean_integer(col(\"funded_object_id\")))\n",
    "        df_transformed = df_transformed.withColumn(\"investor_object_id\", clean_integer(col(\"investor_object_id\")))\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 6: Drop duplicates\n",
    "        df_transformed = df_transformed.dropDuplicates([\"investment_id\"])\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 7: Validation object_id in `dim_company`\n",
    "        companies = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(col(\"object_id\"), col(\"company_id\"))\n",
    "        \n",
    "        people = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(col(\"object_id\"), col(\"people_id\"))\n",
    "\n",
    "        # Join with companies and people tables\n",
    "        df_transformed = df_transformed \\\n",
    "            .join(companies.alias(\"comp\"), df_transformed[\"investor_object_id\"] == col(\"comp.object_id\"), \"left\") \\\n",
    "            .join(people.alias(\"peop\"), df_transformed[\"investor_object_id\"] == col(\"peop.object_id\"), \"left\") \\\n",
    "            .withColumn(\"investor_object_id\", when(col(\"comp.company_id\").isNotNull(), col(\"comp.company_id\"))\n",
    "                        .otherwise(col(\"peop.people_id\"))) \\\n",
    "            .withColumn(\"investor_entity_type\", when(col(\"comp.company_id\").isNotNull(), lit(\"company\"))\n",
    "                        .otherwise(lit(\"people\")))\n",
    "\n",
    "        df_transformed = df_transformed \\\n",
    "            .join(companies.alias(\"funded\"), df_transformed[\"funded_object_id\"] == col(\"funded.object_id\"), \"left\") \\\n",
    "            .withColumn(\"mapped_funded_object_id\", col(\"funded.company_id\"))\n",
    "\n",
    "        df_transformed = df_transformed.select(\n",
    "            df_transformed[\"investment_id\"],\n",
    "            df_transformed[\"funding_round_id\"],\n",
    "            df_transformed[\"funded_entity_type\"],\n",
    "            df_transformed[\"investor_entity_type\"],\n",
    "            df_transformed[\"investor_object_id\"],\n",
    "            df_transformed[\"created_at\"],\n",
    "            df_transformed[\"updated_at\"],\n",
    "            col(\"mapped_funded_object_id\").alias(\"funded_object_id\")\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        df_valid = (\n",
    "            df_transformed\n",
    "            .join(broadcast(companies.alias(\"investor\")), col(\"investor_object_id\") == col(\"investor.object_id\"), \"left\")\n",
    "            .join(broadcast(companies.alias(\"funded\")), col(\"funded_object_id\") == col(\"funded.object_id\"), \"left\")\n",
    "            .select(df_transformed[\"*\"],  \n",
    "                    col(\"investor.company_id\").alias(\"investor_company_id\"),  \n",
    "                    col(\"funded.company_id\").alias(\"funded_company_id\"))\n",
    "        )\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"investor_object_id\").isNull() | col(\"funded_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"investor_object_id\").isNotNull() & col(\"funded_object_id\").isNotNull())\n",
    "        \n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.select(\"investor_entity_type\", \"investor_object_id\", \n",
    "                                            \"funded_entity_type\", \"funded_object_id\").toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"investments\")\n",
    "            \n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"investments\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        return df_valid\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"investments\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2fb2f-b1c0-4a2f-9dfe-b14ed294a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "investments = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"investments\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b87a7-79cf-4feb-be33-15ffcb2d48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform investments data\n",
    "\n",
    "transformed_investments = transform_investments(investments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca3ec2-187a-4d41-a70e-ae7b780910b6",
   "metadata": {},
   "source": [
    "### Funding Rounds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e4442-1ffc-496b-9beb-155febd47037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, when, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_funding_rounds(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Extract prefix and ID\n",
    "        df = df.withColumn(\"funding_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", extract_id(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", col(\"object_id\").cast(IntegerType()))\n",
    "\n",
    "        # Step 3: Format data type\n",
    "        df = df.withColumn(\"funding_date\", to_date(col(\"funded_at\")))\n",
    "        df = df.withColumn(\"funding_entity_type\", col(\"funding_entity_type\").cast(StringType()))\n",
    "        df = df.withColumn(\"participants\", col(\"participants\").cast(IntegerType()))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"funding_round_id\"),\n",
    "            col(\"funding_entity_type\"),\n",
    "            col(\"object_id\").alias(\"funding_object_id\"),\n",
    "            col(\"funding_round_type\").alias(\"round_type\"),\n",
    "            col(\"funding_date\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\"),\n",
    "            col(\"raised_amount_usd\"),\n",
    "            col(\"pre_money_currency_code\").alias(\"pre_money_currency\"),\n",
    "            col(\"pre_money_valuation\"),\n",
    "            col(\"pre_money_valuation_usd\"),\n",
    "            col(\"post_money_currency_code\").alias(\"post_money_currency\"),\n",
    "            col(\"post_money_valuation\"),\n",
    "            col(\"post_money_valuation_usd\"),\n",
    "            col(\"participants\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"source_description\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Step 5: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"round_type\": \"Unknown\",\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"pre_money_currency\": \"USD\",\n",
    "            \"post_money_currency\": \"USD\",\n",
    "            \"raised_amount_usd\": 0.0,\n",
    "            \"source_description\": \"Unknown\",\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicates dan invalid data\n",
    "        df_transformed = df_transformed.dropDuplicates([\"funding_round_id\"])\n",
    "        df_transformed = df_transformed.filter(col(\"round_type\") != \"Unknown\")\n",
    "\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\")\n",
    "        )\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_description\", clean_alpha_text(\"source_description\")\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 7: Load dim_company and dim_people for validation\n",
    "        companies = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"company_id\"))\n",
    "\n",
    "        people = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"people_id\"))\n",
    "\n",
    "        # Validation on funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(companies), df_transformed[\"funding_object_id\"] == companies[\"company_id\"], \"left\") \\\n",
    "            .join(broadcast(people), df_transformed[\"funding_object_id\"] == people[\"people_id\"], \"left\") \\\n",
    "            .withColumn(\"funding_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"funding_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"funding_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"funding_rounds\")\n",
    "            \n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"funding_rounds\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        return df_valid\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funding_rounds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aafda1-4af4-4373-9165-d0ef0daa0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "\n",
    "funding_rounds = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"funding_rounds\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa247fe5-1e9c-439a-b13f-94b96d4211d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform funding rounds\n",
    "\n",
    "transformed_funding_rounds = transform_funding_rounds(funding_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81202b76-a08c-4679-acbc-d62308dda2ce",
   "metadata": {},
   "source": [
    "### Relationship Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df68fbd-aa7c-4bdd-84f4-475f9bc940ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, to_timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_relationship(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"start_at\", to_date(col(\"start_at\")))\n",
    "        df = df.withColumn(\"end_at\", to_date(col(\"end_at\")))\n",
    "        df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\")))\n",
    "        df = df.withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "        # Step 3: Extract prefix and ID, normalize\n",
    "        df = df.withColumn(\"people_entity_type\", extract_prefix(col(\"person_object_id\")))\n",
    "        df = df.withColumn(\"relationship_entity_type\", extract_prefix(col(\"relationship_object_id\")))\n",
    "        df = df.withColumn(\"people_object_id\", extract_id(col(\"person_object_id\")))\n",
    "        df = df.withColumn(\"relationship_object_id\", extract_id(col(\"relationship_object_id\")))\n",
    "        df = df.withColumn(\"title\", normalize_text(col(\"title\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 3: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"relationship_id\"),\n",
    "            col(\"people_entity_type\").alias(\"people_entity_type\"),\n",
    "            col(\"people_object_id\").alias(\"people_object_id\"),\n",
    "            col(\"relationship_entity_type\").alias(\"relationship_entity_type\"),\n",
    "            col(\"relationship_object_id\").alias(\"relationship_object_id\"),\n",
    "            col(\"start_at\").alias(\"start_at\"),\n",
    "            col(\"end_at\").alias(\"end_at\"),\n",
    "            col(\"title\").alias(\"title\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"title\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 5: Drop duplicates and null values\n",
    "        df_transformed = df_transformed.dropDuplicates([\"relationship_id\"])\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 6: Load dim_company dan dim_people for validation\n",
    "        companies = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"company_id\"))\n",
    "\n",
    "        people = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"people_id\"))\n",
    "\n",
    "        # Validation on funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(companies), df_transformed[\"relationship_object_id\"] == companies[\"company_id\"], \"left\") \\\n",
    "            .join(broadcast(people), df_transformed[\"people_object_id\"] == people[\"people_id\"], \"left\") \\\n",
    "            .withColumn(\"relationship_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"people_object_id\").isNull() | col(\"relationship_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"people_object_id\").isNotNull() & col(\"relationship_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"relationship\")\n",
    "\n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"relationship\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        return df_valid\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"relationship\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfd543-2fe0-4fa4-9756-f57596313b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "relationship = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"relationship\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aac358-186d-43cb-b883-525e86198512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform relationship data\n",
    "\n",
    "transformed_relationship = transform_relationship(relationship)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bf71d-6769-4ebb-9e1f-e0dd631cfae9",
   "metadata": {},
   "source": [
    "### IPO Data - Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675efb3-d207-4122-af14-3b8d59527220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, to_timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_ipo(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Format data type\n",
    "        df = df.withColumn(\"public_at\", to_date(col(\"public_at\")))\n",
    "\n",
    "        # Step 3: Extract prefix and ID, normalize\n",
    "        df = df.withColumn(\"ipo_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"ipo_object_id\", extract_id(col(\"object_id\")))\n",
    "        df = df.withColumn(\"stock_market\", extract_stock_market(col(\"stock_symbol\")))\n",
    "        df = df.withColumn(\"stock_symbol\", extract_stock_symbol(col(\"stock_symbol\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"ipo_id\").alias(\"ipo_id\"),\n",
    "            col(\"ipo_entity_type\").alias(\"ipo_entity_type\"),\n",
    "            col(\"ipo_object_id\").alias(\"ipo_object_id\"),\n",
    "            col(\"valuation_currency_code\").alias(\"valuation_currency\"),\n",
    "            col(\"valuation_amount\").alias(\"valuation_amount\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\").alias(\"raised_amount\"),\n",
    "            col(\"public_at\").alias(\"public_at\"),\n",
    "            col(\"stock_market\").alias(\"stock_market\"),\n",
    "            col(\"stock_symbol\").alias(\"stock_symbol\"),\n",
    "            col(\"source_url\").alias(\"source_url\"),\n",
    "            col(\"source_description\").alias(\"source_description\"),\n",
    "            col(\"created_at\").alias(\"created_at\"),\n",
    "            col(\"updated_at\").alias(\"updated_at\")\n",
    "        )\n",
    "        \n",
    "        log_to_csv({\n",
    "            \"step\": \"Map Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 5: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"valuation_amount\": 0.0,\n",
    "            \"valuation_currency\": \"USD\",\n",
    "            \"raised_amount\": 0.0,\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"stock_market\": \"N/A\",\n",
    "            \"stock_symbol\": \"N/A\",\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicates \n",
    "        df_transformed = df_transformed.dropDuplicates([\"ipo_id\"])\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipos\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 7: Load dim_company and dim_people for validation\n",
    "        companies = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"company_id\"))\n",
    "\n",
    "        people = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"people_id\"))\n",
    "\n",
    "        # Validation on funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(companies), df_transformed[\"ipo_object_id\"] == companies[\"company_id\"], \"left\") \\\n",
    "            .join(broadcast(people), df_transformed[\"ipo_object_id\"] == people[\"people_id\"], \"left\") \\\n",
    "            .withColumn(\"ipo_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"ipo_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"ipo_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"ipo\")\n",
    "\n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"ipo\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        return df_valid\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"ipo\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293badc-afd4-4689-96bb-cf05e820c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "\n",
    "ipo = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"ipo\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ec69a-609a-4966-ad3e-f04227328f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform IPO Data\n",
    "\n",
    "transformed_ipo = transform_ipo(ipo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e2253-1d98-44c5-b241-a68fc8b5651b",
   "metadata": {},
   "source": [
    "### Funds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc775bb3-5740-422e-80a1-00f406b68ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, broadcast, when, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_funds(df):\n",
    "    try:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": \"STARTED\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 1: Replace \"\" to null\n",
    "        df = df.na.replace(\"\", None)\n",
    "\n",
    "        # Step 2: Extract prefix and ID \n",
    "        df = df.withColumn(\"fund_entity_type\", extract_prefix(col(\"object_id\")))\n",
    "        df = df.withColumn(\"fund_object_id\", extract_id(col(\"object_id\")))\n",
    "        df = df.withColumn(\"object_id\", col(\"object_id\").cast(IntegerType()))\n",
    "        \n",
    "        # Step 3: Format data type\n",
    "        df = df.withColumn(\"funding_date\", to_date(col(\"funded_at\")))\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Format Data\",\n",
    "            \"status\": f\"SUCCESS ({df.count()} rows)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 4: Mapping to target column\n",
    "        df_transformed = df.select(\n",
    "            col(\"fund_id\"),\n",
    "            col(\"fund_entity_type\"),\n",
    "            col(\"fund_object_id\").alias(\"fund_object_id\"),\n",
    "            col(\"name\").alias(\"fund_name\"),\n",
    "            col(\"funding_date\").alias(\"funding_date\"),\n",
    "            col(\"raised_currency_code\").alias(\"raised_currency\"),\n",
    "            col(\"raised_amount\"),\n",
    "            col(\"source_url\"),\n",
    "            col(\"source_description\"),\n",
    "            col(\"created_at\"),\n",
    "            col(\"updated_at\")\n",
    "        )\n",
    "\n",
    "        # Step 5: Handle null values\n",
    "        df_transformed = df_transformed.fillna({\n",
    "            \"raised_currency\": \"USD\",\n",
    "            \"raised_amount\": 0.0,\n",
    "            \"source_url\": \"Unknown\",\n",
    "            \"source_description\": \"Unknown\"\n",
    "        })\n",
    "\n",
    "        # Step 6: Drop duplicates dan invalid data\n",
    "        df_transformed = df_transformed.dropDuplicates([\"fund_id\"])\n",
    "        df_transformed = df_transformed.na.drop(subset=[\"funding_date\"])\n",
    "\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_url\", when(col(\"source_url\").rlike(r\"^(http|https)://.*\"), col(\"source_url\")).otherwise(\"Unknown\")\n",
    "        )\n",
    "        df_transformed = df_transformed.withColumn(\n",
    "            \"source_description\", clean_alpha_text(\"source_description\")\n",
    "        )\n",
    "\n",
    "        log_to_csv({\n",
    "            \"step\": \"Clean Data\",\n",
    "            \"status\": f\"SUCCESS ({df_transformed.count()} rows after cleansing)\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        # Step 7: Load dim_company and dim_people for validation\n",
    "        companies = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_company\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"company_id\"))\n",
    "\n",
    "        people = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", DWH_URL) \\\n",
    "            .option(\"dbtable\", \"dim_people\") \\\n",
    "            .option(\"user\", DWH_USER) \\\n",
    "            .option(\"password\", DWH_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load().select(col(\"people_id\"))\n",
    "\n",
    "        # Validation on funded_object_id\n",
    "        df_valid = df_transformed \\\n",
    "            .join(broadcast(companies), df_transformed[\"fund_object_id\"] == companies[\"company_id\"], \"left\") \\\n",
    "            .join(broadcast(people), df_transformed[\"fund_object_id\"] == people[\"people_id\"], \"left\") \\\n",
    "            .withColumn(\"fund_object_id\",\n",
    "                        when(col(\"company_id\").isNotNull(), col(\"company_id\"))\n",
    "                        .otherwise(col(\"people_id\"))) \\\n",
    "            .drop(\"company_id\", \"people_id\")\n",
    "\n",
    "        df_invalid = df_valid.filter(col(\"company_id\").isNull() | col(\"people_id\").isNull() | col(\"fund_object_id\").isNull())\n",
    "        df_valid = df_valid.filter(col(\"company_id\").isNotNull() & col(\"people_id\").isNotNull() & col(\"fund_object_id\").isNotNull())\n",
    "\n",
    "        if df_invalid.count() > 0:\n",
    "            invalid_ids = df_invalid.limit(10).toPandas().values.tolist()\n",
    "            save_invalid_ids(invalid_ids, table_name=\"funds\")\n",
    "\n",
    "            log_to_csv({\n",
    "                \"step\": \"Validation\",\n",
    "                \"status\": f\"{df_invalid.count()} rows with missing object_id\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"funds\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }, \"etl_log.csv\")\n",
    "\n",
    "        return df_valid\n",
    "        \n",
    "        print(\"The data is successfully transformed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_csv({\n",
    "            \"step\": \"Transform\",\n",
    "            \"status\": f\"FAILED - {str(e)}\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"funds\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe07f2-bef5-446c-8ef6-5f60a12ca2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from staging\n",
    "\n",
    "funds = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_STAGING_URL) \\\n",
    "    .option(\"dbtable\", \"funds\") \\\n",
    "    .option(\"user\", DB_STAGING_USER) \\\n",
    "    .option(\"password\", DB_STAGING_PASS) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c6a2c-03b2-4d91-b99e-b6675b19c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform funds Data\n",
    "\n",
    "transform_funds(funds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3af385-e24b-4111-a871-2a0d953ee0c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load to Data Warehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac5285b0-4e49-4316-82c9-ed0ec231d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_dwh(df, table_name, mode=\"overwrite\", use_upsert=False, idx_name=None, schema=None, source=None, log_path=\"etl_log.csv\"):\n",
    "    from pangres import upsert\n",
    "    from sqlalchemy import create_engine\n",
    "    import os\n",
    "\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        if not DWH_USER or not DWH_PASS:\n",
    "            raise EnvironmentError(\"DWH_USER or DWH_PASS is not set\")\n",
    "\n",
    "        if use_upsert:\n",
    "            data = df.toPandas()\n",
    "            if idx_name is None:\n",
    "                raise ValueError(\"Index name is required for upsert mode\")\n",
    "            data = data.set_index(idx_name)\n",
    "\n",
    "            conn = create_engine(f\"postgresql://{DWH_USER}:{DWH_PASS}@host.docker.internal:5432/pyspark_task_dwh\")\n",
    "            upsert(\n",
    "                con=conn,\n",
    "                df=data,\n",
    "                table_name=table_name,\n",
    "                schema=schema,\n",
    "                if_row_exists=\"update\"\n",
    "            )\n",
    "            print(f\"Data upserted to table '{table_name}' successfully!\")\n",
    "        else:\n",
    "            df.write \\\n",
    "              .format(\"jdbc\") \\\n",
    "              .option(\"url\", \"jdbc:postgresql://host.docker.internal:5432/pyspark_task_dwh\") \\\n",
    "              .option(\"dbtable\", table_name) \\\n",
    "              .option(\"user\", DWH_USER) \\\n",
    "              .option(\"password\", DWH_PASS) \\\n",
    "              .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "              .mode(mode) \\\n",
    "              .save()\n",
    "            print(f\"Data loaded to table '{table_name}' successfully!\")\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"transformed data\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to table '{table_name}': {e}\")\n",
    "        failed_data = data if use_upsert else df.toPandas()\n",
    "        failed_data['error_message'] = str(e)\n",
    "        failed_data['etl_date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\": \"Load to DWH\",\n",
    "            \"status\": \"Failed\",\n",
    "            \"source\": \"transformed data\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_message\": str(e)\n",
    "        }\n",
    "\n",
    "        failed_log_path = f'logs/failed_{table_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        failed_data.to_csv(failed_log_path, index=False)\n",
    "        print(f\"Failed data saved to: {failed_log_path}\")\n",
    "\n",
    "    finally:\n",
    "        log_msg.pop(\"error_message\", None)\n",
    "        log_to_csv(log_msg, log_path)\n",
    "\n",
    "    return df if not use_upsert else data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3b5bc13-5e7f-40d6-8e90-4447c2f00e34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_company' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m load_to_dwh(\u001b[43mtransformed_company\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim_company_test\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_upsert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformed_company' is not defined"
     ]
    }
   ],
   "source": [
    "# dim_company\n",
    "\n",
    "load_to_dwh(transformed_company, table_name=\"dim_company\", use_upsert=True, idx_name=\"company_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41c60359-1259-4beb-a787-27494cd29603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data upserted to table 'dim_people_test' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>full_name</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>affiliation_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>p</td>\n",
       "      <td>Raju Vegesna</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Zoho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>p</td>\n",
       "      <td>Alex Welch</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>C7 Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>p</td>\n",
       "      <td>Scott Penberthy</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Photobucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>p</td>\n",
       "      <td>Alice Lankester</td>\n",
       "      <td>Surrey, England</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>78</td>\n",
       "      <td>p</td>\n",
       "      <td>Rajiv Dutta</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>eBay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221830</th>\n",
       "      <td>262990</td>\n",
       "      <td>p</td>\n",
       "      <td>Barry Gavin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unaffiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221833</th>\n",
       "      <td>262993</td>\n",
       "      <td>p</td>\n",
       "      <td>Rostislav Raykov</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unaffiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221887</th>\n",
       "      <td>263050</td>\n",
       "      <td>p</td>\n",
       "      <td>Sandeep Gupta</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unaffiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222040</th>\n",
       "      <td>263212</td>\n",
       "      <td>p</td>\n",
       "      <td>Thomas A Kennedy</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unaffiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222121</th>\n",
       "      <td>263300</td>\n",
       "      <td>p</td>\n",
       "      <td>Garrett Potter</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unaffiliated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22244 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           object_id entity_type         full_name       birthplace  \\\n",
       "people_id                                                             \n",
       "3                  4           p      Raju Vegesna          Unknown   \n",
       "14                16           p        Alex Welch          Unknown   \n",
       "21                23           p   Scott Penberthy          Unknown   \n",
       "22                24           p   Alice Lankester  Surrey, England   \n",
       "68                78           p       Rajiv Dutta          Unknown   \n",
       "...              ...         ...               ...              ...   \n",
       "221830        262990           p       Barry Gavin          Unknown   \n",
       "221833        262993           p  Rostislav Raykov          Unknown   \n",
       "221887        263050           p     Sandeep Gupta          Unknown   \n",
       "222040        263212           p  Thomas A Kennedy          Unknown   \n",
       "222121        263300           p    Garrett Potter          Unknown   \n",
       "\n",
       "          affiliation_name  \n",
       "people_id                   \n",
       "3                     Zoho  \n",
       "14                C7 Group  \n",
       "21             Photobucket  \n",
       "22                   Apple  \n",
       "68                    eBay  \n",
       "...                    ...  \n",
       "221830        Unaffiliated  \n",
       "221833        Unaffiliated  \n",
       "221887        Unaffiliated  \n",
       "222040        Unaffiliated  \n",
       "222121        Unaffiliated  \n",
       "\n",
       "[22244 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim_people\n",
    "\n",
    "load_to_dwh(transformed_people, table_name=\"dim_people\", use_upsert=True, idx_name=\"people_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5351b-d4b4-4157-94c1-094f38e8e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_acquisition\n",
    "\n",
    "load_to_dwh(transformed_acquisition, table_name=\"fact_acquisition\", use_upsert=True, idx_name=\"acquisition_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3816c46-a6dd-491e-9191-e645709ed216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_investments\n",
    "\n",
    "load_to_dwh(transformed_investments, table_name=\"fact_investments\", use_upsert=True, idx_name=\"investment_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cc2f3-6542-4c35-be17-4e270f1db2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_funding_rounds\n",
    "\n",
    "load_to_dwh(transformed_funding_rounds, table_name=\"dim_funding_rounds\", use_upsert=True, idx_name=\"funding_round_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92958dfd-2950-466b-ab9c-ae70c8346667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_relationship\n",
    "\n",
    "load_to_dwh(transformed_relationship, table_name=\"fact_relationship\", use_upsert=True, idx_name=\"relationship_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930a65e-6eda-432e-95a6-88ed64092e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_ipo\n",
    "\n",
    "load_to_dwh(transformed_ipo, table_name=\"fact_ipo\", use_upsert=True, idx_name=\"ipo_id\", schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39522d4-f72a-48ca-8fc9-14559382f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_funds\n",
    "\n",
    "load_to_dwh(transformed_funds, table_name=\"dim_funds\", use_upsert=True, idx_name=\"fund_id\", schema=\"public\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
