{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f9b6cd-7c63-4d7c-a963-25952b0ca934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eace4aee-ea80-400b-af40-bbf677b5cb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check pyspark version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027ca5ff-3f8d-4fc2-b52a-7da84338ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1d6a71-4fac-4f99-9ea4-f2f7e2d73fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Final Project PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4d9054-9107-42d6-a0a0-e049b9319e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Final Project PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3eb43ae950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add16a43-963c-420d-8b5e-e8c18d934667",
   "metadata": {},
   "source": [
    "## Load and Handle Failure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d94b859-9d4d-483a-b6af-5dfcdf168376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54467c7-078d-466a-91d8-57af1aac710c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59477767-b87d-4a17-ab86-7500c2650141",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY_MINIO = os.getenv(\"ACCESS_KEY_MINIO\")\n",
    "SECRET_KEY_MINIO = os.getenv(\"SECRET_KEY_MINIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e88c81-918f-4d56-88fc-5c4ef92d923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /opt/conda/lib/python3.11/site-packages (7.2.15)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from minio) (2023.7.22)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from minio) (2.0.7)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from minio) (23.1.0)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.11/site-packages (from minio) (3.22.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from minio) (4.8.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a38f0744-e07b-4435-98e4-3df028e01c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Minio libray is used to interact with a MinIO server. \n",
    "from minio import Minio\n",
    "\n",
    "# BytesIO provides a way to work with binary data in memory as if it were a file.\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d03e931d-4a6e-4869-ae87-7a9a098b0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error(data, bucket_name:str, table_name:str):\n",
    "\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Initialize MinIO client\n",
    "    client = Minio('localhost:9000',\n",
    "                access_key=ACCESS_KEY_MINIO,\n",
    "                secret_key=SECRET_KEY_MINIO,\n",
    "                secure=False)\n",
    "\n",
    "    # Make a bucket if it doesn't exist\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "\n",
    "    # Convert DataFrame to CSV and then to bytes\n",
    "    csv_bytes = data.to_csv().encode('utf-8')\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    # Upload the CSV file to the bucket\n",
    "    client.put_object(\n",
    "        bucket_name=bucket_name,\n",
    "        object_name=f\"{table_name}_{current_date}.csv\", #name the fail source name and current etl date\n",
    "        data=csv_buffer,\n",
    "        length=len(csv_bytes),\n",
    "        content_type='application/csv'\n",
    "    )\n",
    "\n",
    "    # List objects in the bucket\n",
    "    objects = client.list_objects(bucket_name, recursive=True)\n",
    "    for obj in objects:\n",
    "        print(obj.object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d67139-17b1-4fdc-aaa1-a5f9a5fc15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b12c56d6-bb72-4a25-984a-b30b66d7ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT LOGS\n",
    "def log_to_csv(log_msg: dict, filename: str):\n",
    "    # Ensure the 'logs' directory exists\n",
    "    log_dir = os.path.join(os.getcwd(), 'logs')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Create the full file path inside 'logs'\n",
    "    file_path = os.path.join(log_dir, filename)\n",
    "\n",
    "    # Define the column headers\n",
    "    headers = [\"step\", \"status\", \"source\", \"table_name\", \"etl_date\"]\n",
    "\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        file_exists = os.path.isfile(file_path)\n",
    "\n",
    "        with open(file_path, mode='a', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=headers)\n",
    "\n",
    "            # Write the header only if the file doesn't exist\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Append the log message\n",
    "            writer.writerow(log_msg)\n",
    "\n",
    "        print(f\"Log written to {file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing log to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091d0a6-b0fc-4a07-96a0-5ea4288035fb",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8994a6-a66d-4dd2-87a7-1e29133144f9",
   "metadata": {},
   "source": [
    "### CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7124d8ff-228b-4ecf-b8b2-9f7adb03a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(file_path, table_name):\n",
    "    try:\n",
    "        # Read CSV using Spark\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n",
    "        # Show extracted data\n",
    "        df.show()\n",
    "\n",
    "        # Log success\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"CSV\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"CSV\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "\n",
    "        print(f\"Error extracting {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6396ca-a091-4d35-aa82-21826baf95e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|people_id|object_id|first_name| last_name|          birthplace|    affiliation_name|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|        1|      p:2|       Ben|   Elowitz|                NULL|           Blue Nile|\n",
      "|        2|      p:3|     Kevin|  Flaherty|                NULL|            Wetpaint|\n",
      "|        3|      p:4|      Raju|   Vegesna|                NULL|                Zoho|\n",
      "|        4|      p:5|       Ian|     Wenig|                NULL|                Zoho|\n",
      "|        5|      p:6|     Kevin|      Rose|         Redding, CA|        i/o Ventures|\n",
      "|        6|      p:7|       Jay|   Adelson|         Detroit, MI|                Digg|\n",
      "|        7|      p:8|      Owen|     Byrne|                NULL|                Digg|\n",
      "|        8|      p:9|       Ron|Gorodetzky|                NULL|                Digg|\n",
      "|        9|     p:10|      Mark|Zuckerberg|                NULL|            Facebook|\n",
      "|       10|     p:11|    Dustin| Moskovitz|     Gainesville, FL|            Facebook|\n",
      "|       11|     p:12|      Owen| Van Natta|                NULL|               Asana|\n",
      "|       12|     p:13|      Matt|    Cohler|                NULL|            LinkedIn|\n",
      "|       13|     p:14|     Chris|    Hughes|Hickery, North Ca...|General Catalyst ...|\n",
      "|       14|     p:16|      Alex|     Welch|                NULL|            C7 Group|\n",
      "|       15|     p:17|    Darren|   Crystal|                NULL|         Photobucket|\n",
      "|       16|     p:18|   Michael|     Clark|                NULL|   Photobucket (Old)|\n",
      "|       17|     p:19|      Greg|    Wimmer|                NULL|         Photobucket|\n",
      "|       18|     p:20|     Peter|    Foster|                NULL|         Photobucket|\n",
      "|       19|     p:21|   Heather|      Dana|                NULL|         Photobucket|\n",
      "|       20|     p:22|     Peter|      Pham|                NULL|   Photobucket, Inc.|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|people_id|object_id|first_name| last_name|          birthplace|    affiliation_name|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "|        1|      p:2|       Ben|   Elowitz|                NULL|           Blue Nile|\n",
      "|        2|      p:3|     Kevin|  Flaherty|                NULL|            Wetpaint|\n",
      "|        3|      p:4|      Raju|   Vegesna|                NULL|                Zoho|\n",
      "|        4|      p:5|       Ian|     Wenig|                NULL|                Zoho|\n",
      "|        5|      p:6|     Kevin|      Rose|         Redding, CA|        i/o Ventures|\n",
      "|        6|      p:7|       Jay|   Adelson|         Detroit, MI|                Digg|\n",
      "|        7|      p:8|      Owen|     Byrne|                NULL|                Digg|\n",
      "|        8|      p:9|       Ron|Gorodetzky|                NULL|                Digg|\n",
      "|        9|     p:10|      Mark|Zuckerberg|                NULL|            Facebook|\n",
      "|       10|     p:11|    Dustin| Moskovitz|     Gainesville, FL|            Facebook|\n",
      "|       11|     p:12|      Owen| Van Natta|                NULL|               Asana|\n",
      "|       12|     p:13|      Matt|    Cohler|                NULL|            LinkedIn|\n",
      "|       13|     p:14|     Chris|    Hughes|Hickery, North Ca...|General Catalyst ...|\n",
      "|       14|     p:16|      Alex|     Welch|                NULL|            C7 Group|\n",
      "|       15|     p:17|    Darren|   Crystal|                NULL|         Photobucket|\n",
      "|       16|     p:18|   Michael|     Clark|                NULL|   Photobucket (Old)|\n",
      "|       17|     p:19|      Greg|    Wimmer|                NULL|         Photobucket|\n",
      "|       18|     p:20|     Peter|    Foster|                NULL|         Photobucket|\n",
      "|       19|     p:21|   Heather|      Dana|                NULL|         Photobucket|\n",
      "|       20|     p:22|     Peter|      Pham|                NULL|   Photobucket, Inc.|\n",
      "+---------+---------+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the extract function\n",
    "df_people = extract_csv(\"data/people.csv\", \"people_data\")\n",
    "df_people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "683f3aa9-4a83-429a-96b3-9f6fe53d47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|            start_at|              end_at|is_past|sequence|               title|          created_at|          updated_at|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|              1|             p:2|                   c:1|                NULL|                NULL|  false|       8|Co-Founder/CEO/Bo...|2007-05-25 07:03:...|2013-06-03 09:58:...|\n",
      "|              2|             p:3|                   c:1|                NULL|                NULL|  false|  279242|        VP Marketing|2007-05-25 07:04:...|2010-05-21 16:31:...|\n",
      "|              3|             p:4|                   c:3|                NULL|                NULL|  false|       4|          Evangelist|2007-05-25 19:33:...|2013-06-29 13:36:...|\n",
      "|              4|             p:5|                   c:3|2006-03-01 00:00:...|2009-12-01 00:00:...|  false|       4|Senior Director S...|2007-05-25 19:34:...|2013-06-29 10:25:...|\n",
      "|              6|             p:7|                   c:4|2005-07-01 00:00:...|2010-04-05 00:00:...|  false|       1|Chief Executive O...|2007-05-25 20:05:...|2010-04-05 18:41:...|\n",
      "|              7|             p:8|                   c:4|                NULL|                NULL|  false|       1|Senior Software E...|2007-05-25 20:06:...|2010-01-12 01:13:...|\n",
      "|              8|             p:9|                   c:4|                NULL|                NULL|  false|       1|Systems Engineeri...|2007-05-25 20:07:...|2010-08-03 22:00:...|\n",
      "|              9|            p:10|                   c:5|                NULL|                NULL|  false|       1|Founder and CEO, ...|2007-05-25 21:51:...|2010-01-25 21:49:...|\n",
      "|             10|            p:11|                   c:5|                NULL|                NULL|  false|       1|          Co-Founder|2007-05-25 22:15:...|2011-08-11 23:48:...|\n",
      "|             11|            p:12|                   c:5|                NULL|                NULL|  false|       3|Chief Revenue Off...|2007-05-25 22:15:...|2010-01-25 21:49:...|\n",
      "|             12|            p:13|                   c:5|                NULL|                NULL|  false|       1|VP of Product Man...|2007-05-25 22:16:...|2010-01-25 21:49:...|\n",
      "|             13|            p:14|                   c:5|1993-07-04 00:00:...|1982-02-28 00:00:...|  false|       1|          Co-founder|2007-05-25 22:17:...|2010-09-01 18:15:...|\n",
      "|             14|            p:16|                c:7299|                NULL|                NULL|  false|       2|     Founder and CEO|2007-05-26 12:44:...|2011-07-19 00:05:...|\n",
      "|             15|            p:17|                c:7299|                NULL|                NULL|  false|       2|  Co-founder and CTO|2007-05-26 12:45:...|2011-07-19 00:05:...|\n",
      "|             16|            p:18|                c:7299|2005-09-01 00:00:...|2009-10-01 00:00:...|  false|       1|SVP, Technology a...|2007-05-26 12:47:...|2011-07-19 00:05:...|\n",
      "|             17|            p:19|                c:7299|2006-08-01 00:00:...|2009-08-01 00:00:...|  false|       8|VP, Finance & Adm...|2007-05-26 12:47:...|2011-10-28 03:08:...|\n",
      "|             18|            p:20|                c:7299|                NULL|                NULL|  false|       1|           VP, Sales|2007-05-26 12:48:...|2011-07-19 00:05:...|\n",
      "|             19|            p:21|                c:7299|                NULL|                NULL|  false|       1|VP, Customer Service|2007-05-26 12:49:...|2011-07-19 00:05:...|\n",
      "|             20|            p:22|                c:7299|2005-10-01 00:00:...|2008-03-01 00:00:...|  false|       1|VP, Business Deve...|2007-05-26 12:49:...|2013-08-07 08:51:...|\n",
      "|             21|            p:23|                c:7299|                NULL|                NULL|  false|       1|     VP, Engineering|2007-05-26 12:50:...|2011-07-19 00:05:...|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|relationship_id|person_object_id|relationship_object_id|            start_at|              end_at|is_past|sequence|               title|          created_at|          updated_at|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|              1|             p:2|                   c:1|                NULL|                NULL|  false|       8|Co-Founder/CEO/Bo...|2007-05-25 07:03:...|2013-06-03 09:58:...|\n",
      "|              2|             p:3|                   c:1|                NULL|                NULL|  false|  279242|        VP Marketing|2007-05-25 07:04:...|2010-05-21 16:31:...|\n",
      "|              3|             p:4|                   c:3|                NULL|                NULL|  false|       4|          Evangelist|2007-05-25 19:33:...|2013-06-29 13:36:...|\n",
      "|              4|             p:5|                   c:3|2006-03-01 00:00:...|2009-12-01 00:00:...|  false|       4|Senior Director S...|2007-05-25 19:34:...|2013-06-29 10:25:...|\n",
      "|              6|             p:7|                   c:4|2005-07-01 00:00:...|2010-04-05 00:00:...|  false|       1|Chief Executive O...|2007-05-25 20:05:...|2010-04-05 18:41:...|\n",
      "|              7|             p:8|                   c:4|                NULL|                NULL|  false|       1|Senior Software E...|2007-05-25 20:06:...|2010-01-12 01:13:...|\n",
      "|              8|             p:9|                   c:4|                NULL|                NULL|  false|       1|Systems Engineeri...|2007-05-25 20:07:...|2010-08-03 22:00:...|\n",
      "|              9|            p:10|                   c:5|                NULL|                NULL|  false|       1|Founder and CEO, ...|2007-05-25 21:51:...|2010-01-25 21:49:...|\n",
      "|             10|            p:11|                   c:5|                NULL|                NULL|  false|       1|          Co-Founder|2007-05-25 22:15:...|2011-08-11 23:48:...|\n",
      "|             11|            p:12|                   c:5|                NULL|                NULL|  false|       3|Chief Revenue Off...|2007-05-25 22:15:...|2010-01-25 21:49:...|\n",
      "|             12|            p:13|                   c:5|                NULL|                NULL|  false|       1|VP of Product Man...|2007-05-25 22:16:...|2010-01-25 21:49:...|\n",
      "|             13|            p:14|                   c:5|1993-07-04 00:00:...|1982-02-28 00:00:...|  false|       1|          Co-founder|2007-05-25 22:17:...|2010-09-01 18:15:...|\n",
      "|             14|            p:16|                c:7299|                NULL|                NULL|  false|       2|     Founder and CEO|2007-05-26 12:44:...|2011-07-19 00:05:...|\n",
      "|             15|            p:17|                c:7299|                NULL|                NULL|  false|       2|  Co-founder and CTO|2007-05-26 12:45:...|2011-07-19 00:05:...|\n",
      "|             16|            p:18|                c:7299|2005-09-01 00:00:...|2009-10-01 00:00:...|  false|       1|SVP, Technology a...|2007-05-26 12:47:...|2011-07-19 00:05:...|\n",
      "|             17|            p:19|                c:7299|2006-08-01 00:00:...|2009-08-01 00:00:...|  false|       8|VP, Finance & Adm...|2007-05-26 12:47:...|2011-10-28 03:08:...|\n",
      "|             18|            p:20|                c:7299|                NULL|                NULL|  false|       1|           VP, Sales|2007-05-26 12:48:...|2011-07-19 00:05:...|\n",
      "|             19|            p:21|                c:7299|                NULL|                NULL|  false|       1|VP, Customer Service|2007-05-26 12:49:...|2011-07-19 00:05:...|\n",
      "|             20|            p:22|                c:7299|2005-10-01 00:00:...|2008-03-01 00:00:...|  false|       1|VP, Business Deve...|2007-05-26 12:49:...|2013-08-07 08:51:...|\n",
      "|             21|            p:23|                c:7299|                NULL|                NULL|  false|       1|     VP, Engineering|2007-05-26 12:50:...|2011-07-19 00:05:...|\n",
      "+---------------+----------------+----------------------+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the extract function\n",
    "df_relations = extract_csv(\"data/relationships.csv\", \"relationships_data\")\n",
    "df_relations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73f9ea-4216-46a4-91e4-cd4bdb943e39",
   "metadata": {},
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58919bd4-a86e-46ed-a849-bf236d617ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for database\n",
    "\n",
    "DB_URL = os.getenv(\"DB_URL\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "029c35a1-6309-440c-a1d1-6427bfe1eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_db():\n",
    "    try:\n",
    "        # Get list of tables from the database\n",
    "        table_list = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DB_URL) \\\n",
    "            .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "            .option(\"user\", DB_USER) \\\n",
    "            .option(\"password\", DB_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .filter(\"table_schema = 'public'\") \\\n",
    "            .select(\"table_name\") \\\n",
    "            .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print(f\"Found tables: {table_list}\")\n",
    "\n",
    "        tables = {}\n",
    "        for table in table_list:\n",
    "            try:\n",
    "                # Read each table into a DataFrame\n",
    "                df = spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", DB_URL) \\\n",
    "                    .option(\"dbtable\", table) \\\n",
    "                    .option(\"user\", DB_USER) \\\n",
    "                    .option(\"password\", DB_PASS) \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()\n",
    "\n",
    "                tables[table] = df\n",
    "\n",
    "                # Log success for each table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": \"Success\",\n",
    "                    \"source\": \"PostgreSQL\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "\n",
    "                print(f\"Successfully extracted table: {table}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log failure for specific table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": f\"Failed: {e}\",\n",
    "                    \"source\": \"PostgreSQL\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "                print(f\"Failed to extract table: {table} - Error: {e}\")\n",
    "\n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure for the whole extraction process\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"PostgreSQL\",\n",
    "            \"table_name\": \"N/A\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        print(f\"Failed to extract tables: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7557a91-4ad6-4d9e-ba84-3482d3b77027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables: ['company', 'acquisition', 'funding_rounds', 'funds', 'investments', 'ipos']\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: company\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: acquisition\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funding_rounds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: investments\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: ipos\n",
      "Extracted tables: ['company', 'acquisition', 'funding_rounds', 'funds', 'investments', 'ipos']\n"
     ]
    }
   ],
   "source": [
    "# Extract all tables\n",
    "tables = extract_from_db()\n",
    "print(f\"Extracted tables: {list(tables.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fe3be2a-e7cf-4d7d-978d-b9135c7bf937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|office_id|object_id|      description|              region|            address1|  address2|          city|  zip_code|state_code|country_code| latitude|  longitude|         created_at|         updated_at|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|        8|      c:8|                 |              SF Bay|959 Skyway Road, ...|          |    San Carlos|     94070|        CA|         USA|37.506885|-122.247573|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|        9|      c:9|     Headquarters|         Los Angeles|9229 W. Sunset Blvd.|          |West Hollywood|     90069|        CA|         USA|34.090368|-118.393064|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       10|     c:10|                 |         Los Angeles|8536 National Blv...|          |   Culver City|     90232|        CA|         USA|34.025958|-118.379768|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       11|     c:11|                 |         Los Angeles|      407 N Maple Dr|          | Beverly Hills|     90210|        CA|         USA|34.076179|-118.394170|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       12|     c:12|                 |              SF Bay|     1355 Market St.|          | San Francisco|     94103|        CA|         USA|37.776805|-122.416924|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       14|     c:14|                 |              SF Bay|                    |          |    Menlo Park|          |        CA|         USA|37.484130|-122.169472|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       15|     c:15|               HQ|              SF Bay|   539 Bryant Street|          | San Francisco|     94107|        CA|         USA|37.789634|-122.404052|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       16|     c:16|                 |           San Diego|16935 W. Bernardo...|          |     San Diego|     92127|        CA|         USA|33.022176|-117.081406|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       17|     c:18|Lala Headquarters|              SF Bay|    209 Hamilton Ave|Suite #200|     Palo Alto|     94301|        CA|         USA|37.451151|-122.154369|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       18|     c:19|                 |         Los Angeles|10960 Wilshire Blvd.| Suite 700|   Los Angeles|     90024|        CA|         USA|34.057498|-118.446596|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       19|     c:20|     Headquarters|              SF Bay|2145 Hamilton Avenue|          |      San Jose|     95125|        CA|         USA|37.295005|-121.930035|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       20|     c:21|                 |United States - O...|                    |          |              |          |          |         USA|37.090240| -95.712891|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       21|     c:22|                 |            New York|                    |          | New York City|          |        NY|         USA|40.757929| -73.985506|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       22|     c:23|                 |            New York|    100 5th Ave Fl 6|          |      New York|10011-6903|        NY|         USA|40.746497| -74.009447|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       23|     c:24|                 |  California - Other|                    |          |              |          |        CA|         USA|37.269175|-119.306607|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       24|     c:25|               HQ|            New York|       1515 Broadway|          |      New York|     10036|        NY|         USA|40.757725| -73.986011|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       25|     c:26|                 |              London|                    |          |        London|          |          |         GBR|53.344104|  -6.267494|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       26|     c:27|               HQ|              SF Bay|1050 Enterprise W...|          |     Sunnyvale|     94089|        CA|         USA|37.387845|-122.055197|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       27|     c:28|     Headquarters|              SF Bay| 170 West Tasman Dr.|          |      San Jose|     95134|        CA|         USA|37.408802|-121.953770|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       28|     c:29|                 |              SF Bay|    701 First Avenue|          |     Sunnyvale|     94089|        CA|         USA|37.418531|-122.025485|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read each table\n",
    "df_acquisition = tables[\"acquisition\"]\n",
    "df_company = tables[\"company\"]\n",
    "df_funding_rounds = tables[\"funding_rounds\"]\n",
    "df_funds = tables[\"funds\"]\n",
    "df_investments = tables[\"investments\"]\n",
    "df_ipos = tables[\"ipos\"]\n",
    "\n",
    "# check\n",
    "df_company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd8521-1d76-4405-858b-3fbf4a99f1d2",
   "metadata": {},
   "source": [
    "### From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e095748-b6b8-4e62-b2f4-980a2db88053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba254169-f554-44c9-ad58-88b2a6b19ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(link_api: str, list_parameter: dict, data_name: str):\n",
    "    try:\n",
    "        # Establish connection to API\n",
    "        resp = requests.get(link_api, params=list_parameter)\n",
    "        resp.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the response JSON\n",
    "        raw_response = resp.json()\n",
    "\n",
    "        # Convert JSON data to pandas DataFrame\n",
    "        df_api = pd.DataFrame(raw_response)\n",
    "\n",
    "        if df_api.empty:\n",
    "            raise ValueError(\"Empty response from API\")\n",
    "\n",
    "        # Convert pandas DataFrame to PySpark DataFrame\n",
    "        spark_df = spark.createDataFrame(df_api)\n",
    "\n",
    "        # Log success\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "\n",
    "        print(f\"Successfully extracted data from API: {data_name}\")\n",
    "        return spark_df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Log request failure\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "        print(f\"Request failed: {e}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        # Log parsing failure\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "        print(f\"Parsing error: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other errors\n",
    "        log_msg = {\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"API\",\n",
    "            \"table_name\": data_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c186f5f1-1954-4c9d-b19a-9f6e97e1a860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted data from API: milestones\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "|          created_at|         description|milestone_at|milestone_code|milestone_id|object_id|  source_description|          source_url|          updated_at|\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "|2008-06-18 08:14:...|Survives iPhone 3...|  2008-06-09|         other|           1|     c:12|Twitter Fails To ...|http://www.techcr...|2008-06-18 08:14:...|\n",
      "|2008-06-18 08:50:...|More than 4 Billi...|  2008-06-18|         other|           3|     c:59|11 Billion Videos...|http://www.comsco...|2008-06-18 08:50:...|\n",
      "|2008-06-19 04:14:...|Reddit goes Open ...|  2008-06-18|         other|           4|    c:314|reddit goes open ...|http://blog.reddi...|2008-06-19 04:14:...|\n",
      "|2008-06-19 04:15:...|Adds the ability ...|  2008-01-22|         other|           5|    c:314|Reddit Adds Abili...|http://www.techcr...|2008-06-19 04:15:...|\n",
      "|2008-06-19 04:39:...|Adobe announced n...|  2008-06-18|         other|           6|    c:283|                 NaN|                 NaN|2008-06-19 04:39:...|\n",
      "|2008-06-19 08:29:...|Closed release of...|  2008-02-01|         other|           7|   c:6816|                 NaN|                 NaN|2008-06-19 16:55:...|\n",
      "|2008-06-19 08:29:...|Closed release of...|  2008-05-01|         other|           8|   c:6816|Diary of a Mobile...|http://mobverge.b...|2008-06-19 16:55:...|\n",
      "|2008-06-19 15:29:...|Scoofers, a new s...|  2008-06-19|         other|           9|   c:5874|Official launch o...|http://scoofers.b...|2008-06-19 19:34:...|\n",
      "|2008-06-19 17:48:...|Jeremy Zawodny le...|  2008-07-01|         other|          10|   c:2034|I'm Joining Craig...|http://jeremy.zaw...|2008-06-19 17:48:...|\n",
      "|2008-06-19 18:54:...|SnapLogic release...|  2008-04-23|         other|          11|   c:5085|SnapLogic release...|http://venturebea...|2008-06-19 18:54:...|\n",
      "|2008-06-20 02:06:...|Joshua Schachter ...|  2008-06-19|         other|          13|     c:75|It Gets Worse: Jo...|http://www.techcr...|2008-06-20 02:06:...|\n",
      "|2008-06-20 21:19:...|       Bevy Launches|  2008-05-15|         other|          14|   c:6947|            Facebook|http://apps.faceb...|2008-07-18 01:29:...|\n",
      "|2008-06-22 19:10:...|Twitter partners ...|  2008-06-09|         other|          15|     c:12|Twitter Partners ...|http://www.techcr...|2008-06-22 19:10:...|\n",
      "|2008-06-22 19:15:...|Launches a status...|  2008-05-28|         other|          16|     c:12|    What's Going On?|http://blog.twitt...|2008-06-22 19:15:...|\n",
      "|2008-06-22 19:15:...|Twitter Starts Bl...|  2008-05-07|         other|          17|     c:12|Twitter Starts Bl...|http://www.techcr...|2008-06-22 19:15:...|\n",
      "|2008-06-22 19:17:...|VP Lee Mighdoll L...|  2008-04-24|         other|          18|     c:12|VP Lee Mighdoll O...|http://www.techcr...|2008-06-22 19:17:...|\n",
      "|2008-06-22 19:19:...|Chief Architect B...|  2008-04-23|         other|          19|     c:12|Amateur Hour Over...|http://www.techcr...|2008-06-22 19:19:...|\n",
      "|2008-06-22 19:20:...|Twitter Japan Lau...|  2008-04-22|         other|          20|     c:12|Twitter! Japan! Ads!|http://www.techcr...|2008-06-22 19:20:...|\n",
      "|2008-06-22 19:21:...|Twitter helps bus...|  2008-04-16|         other|          21|     c:12|Twitter Saves Man...|http://www.techcr...|2008-06-23 19:40:...|\n",
      "|2008-06-23 01:49:...|Sometrics has int...|  2008-06-03|         other|          24|   c:5411|Sometrics Launche...|http://publicatio...|2008-06-23 19:16:...|\n",
      "+--------------------+--------------------+------------+--------------+------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract data from 2008 - 2010\n",
    "link_api = \"https://api-milestones.vercel.app/api/data\"\n",
    "list_parameter = {\n",
    "    \"start_date\": \"2008-01-01\",\n",
    "    \"end_date\": \"2010-12-31\"\n",
    "}\n",
    "\n",
    "df_milestones = extract_api(link_api, list_parameter, \"milestones\")\n",
    "df_milestones.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35082a-3994-4b7f-bcf2-37ec93ff032c",
   "metadata": {},
   "source": [
    "## Load - Staging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61ac65e0-9c28-4a03-a51e-96fe5b481438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84df23db-7cc2-4706-9c7e-467bf86376b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pangres in /opt/conda/lib/python3.11/site-packages (4.2.1)\n",
      "Requirement already satisfied: pandas>=0.25.3 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.12 in /opt/conda/lib/python3.11/site-packages (from pangres) (2.0.22)\n",
      "Requirement already satisfied: alembic>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from pangres) (1.12.0)\n",
      "Requirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.11/site-packages (from pangres) (23.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.3.1->pangres) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25.3->pangres) (1.24.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.3.12->pangres) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->pangres) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.3.1->pangres) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pangres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45b33ca6-54a8-45e6-96ea-241d6f62e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for database\n",
    "\n",
    "DB_STAGING_URL = os.getenv(\"DB_STAGING_URL\")\n",
    "DB_STAGING_USER = os.getenv(\"DB_STAGING_USER\")\n",
    "DB_STAGING_PASS = os.getenv(\"DB_STAGING_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "968be5ca-e796-4b3b-b539-8939450416f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "from pangres import upsert\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_staging2(df, table_name, mode=\"overwrite\", use_upsert=False, idx_name=None, schema=None, source=None):\n",
    "    try:\n",
    "        if use_upsert:\n",
    "            # Convert Spark DataFrame ke Pandas DataFrame\n",
    "            data = df.toPandas()\n",
    "\n",
    "            # Create connection ke PostgreSQL\n",
    "            conn = create_engine(f\"postgresql://{DB_STAGING_USER}:{DB_STAGING_PASS}@host.docker.internal:5432/pyspark_task_staging\")\n",
    "\n",
    "            # Set index untuk upsert (kalau nggak ada, kasih warning)\n",
    "            if idx_name is None:\n",
    "                raise ValueError(\"Index name is required for upsert mode\")\n",
    "\n",
    "            data = data.set_index(idx_name)\n",
    "\n",
    "            # Lakukan upsert (insert kalau belum ada, update kalau sudah ada)\n",
    "            upsert(\n",
    "                con=conn,\n",
    "                df=data,\n",
    "                table_name=table_name,\n",
    "                schema=schema,\n",
    "                if_row_exists=\"update\"\n",
    "            )\n",
    "            print(f\"Data upserted to table '{table_name}' successfully!\")\n",
    "        else:\n",
    "            # Load dengan Spark\n",
    "            df.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", \"jdbc:postgresql://host.docker.internal:5432/pyspark_task_staging\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", DB_STAGING_USER) \\\n",
    "                .option(\"password\", DB_STAGING_PASS) \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .mode(mode) \\\n",
    "                .save()\n",
    "\n",
    "            print(f\"Data loaded to table '{table_name}' successfully!\")\n",
    "\n",
    "        # Buat success log\n",
    "        log_msg = {\n",
    "            \"step\": \"Load Staging\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to table '{table_name}': {e}\")\n",
    "\n",
    "        # Buat DataFrame gagal untuk logging\n",
    "        failed_data = df.toPandas() if not use_upsert else data\n",
    "        failed_data['error_message'] = str(e)\n",
    "        failed_data['etl_date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Buat log gagal\n",
    "        log_msg = {\n",
    "            \"step\": \"Load Staging\",\n",
    "            \"status\": \"Failed\",\n",
    "            \"source\": \"Staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_message\": str(e)\n",
    "        }\n",
    "\n",
    "        # Save failed data ke CSV\n",
    "        failed_log_path = f'logs/failed_{table_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        failed_data.to_csv(failed_log_path, index=False)\n",
    "        print(f\"Failed data saved to: {failed_log_path}\")\n",
    "\n",
    "    finally:\n",
    "        # Hapus error_message sebelum simpan ke log\n",
    "        if 'error_message' in log_msg:\n",
    "            del log_msg['error_message']\n",
    "\n",
    "        # Simpan log ke CSV\n",
    "        log_to_csv(log_msg, 'etl_log.csv')\n",
    "\n",
    "    return df if not use_upsert else data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d6a2305-57a6-49b1-b57f-cf37f11a2a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'milestones' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: string, description: string, milestone_at: string, milestone_code: string, milestone_id: bigint, object_id: string, source_description: string, source_url: string, updated_at: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from API\n",
    "load_staging2(df_milestones, \"milestones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03e436bf-6909-4dbe-8a4f-9ad66ee5e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'relationship' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'people' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[people_id: string, object_id: string, first_name: string, last_name: string, birthplace: string, affiliation_name: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from csv\n",
    "load_staging2(df_relations, \"relationship\")\n",
    "load_staging2(df_people, \"people\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "121a8a94-b4c4-4957-aaa4-2fb5dba19e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to table 'acquisition' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'funding_rounds' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'funds' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'investments' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'ipo' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Data loaded to table 'company' successfully!\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[office_id: int, object_id: string, description: string, region: string, address1: string, address2: string, city: string, zip_code: string, state_code: string, country_code: string, latitude: decimal(9,6), longitude: decimal(9,6), created_at: timestamp, updated_at: timestamp]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from database\n",
    "\n",
    "load_staging2(df_acquisition, \"acquisition\") \n",
    "load_staging2(df_funding_rounds, \"funding_rounds\") \n",
    "load_staging2(df_funds, \"funds\")\n",
    "load_staging2(df_investments, \"investments\")\n",
    "load_staging2(df_ipos, \"ipo\")\n",
    "load_staging2(df_company, \"company\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771dc6c-3890-4437-8cbb-215f4e0aab11",
   "metadata": {},
   "source": [
    "## Extract Data from Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f576786a-b8bb-43b0-9740-3c34d1311910",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4f7905f-e399-4aa4-ae59-7a0b1713abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for staging\n",
    "\n",
    "DB_STAGING_URL = os.getenv(\"DB_STAGING_URL\")\n",
    "DB_STAGING_USER = os.getenv(\"DB_STAGING_USER\")\n",
    "DB_STAGING_PASS = os.getenv(\"DB_STAGING_PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebcd5088-00c6-40a8-b4f2-5ddedf46c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_staging():\n",
    "    try:\n",
    "        # Get list of tables from staging\n",
    "        table_list = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", DB_STAGING_URL) \\\n",
    "            .option(\"dbtable\", \"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public') AS tbl\") \\\n",
    "            .option(\"user\", DB_STAGING_USER) \\\n",
    "            .option(\"password\", DB_STAGING_PASS) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load() \\\n",
    "            .select(\"table_name\") \\\n",
    "            .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print(f\"Found tables in staging: {table_list}\")\n",
    "\n",
    "        tables = {}\n",
    "        for table in table_list:\n",
    "            try:\n",
    "                # Read each table into a DataFrame\n",
    "                df = spark.read \\\n",
    "                    .format(\"jdbc\") \\\n",
    "                    .option(\"url\", DB_STAGING_URL) \\\n",
    "                    .option(\"dbtable\", table) \\\n",
    "                    .option(\"user\", DB_STAGING_USER) \\\n",
    "                    .option(\"password\", DB_STAGING_PASS) \\\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                    .load()\n",
    "\n",
    "                tables[table] = df\n",
    "\n",
    "                # Log success for each table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": \"Success\",\n",
    "                    \"source\": \"PostgreSQL (Staging)\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "\n",
    "                print(f\"Successfully extracted table: {table}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log failure for specific table\n",
    "                log_to_csv({\n",
    "                    \"step\": \"Extract\",\n",
    "                    \"status\": f\"Failed: {e}\",\n",
    "                    \"source\": \"PostgreSQL (Staging)\",\n",
    "                    \"table_name\": table,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, \"etl_log.csv\")\n",
    "                print(f\"Failed to extract table: {table} - Error: {e}\")\n",
    "\n",
    "        return tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure for the whole extraction process\n",
    "        log_to_csv({\n",
    "            \"step\": \"Extract\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": \"PostgreSQL (Staging)\",\n",
    "            \"table_name\": \"N/A\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, \"etl_log.csv\")\n",
    "        print(f\"Failed to extract tables: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e0eee4a-955a-482b-bec9-eb7fa9ab5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables in staging: ['investments', 'relationship', 'people', 'ipo', 'company', 'acquisition', 'funding_rounds', 'milestones', 'funds']\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: investments\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: relationship\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: people\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: ipo\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: company\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: acquisition\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funding_rounds\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: milestones\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Successfully extracted table: funds\n",
      "Extracted tables: ['investments', 'relationship', 'people', 'ipo', 'company', 'acquisition', 'funding_rounds', 'milestones', 'funds']\n"
     ]
    }
   ],
   "source": [
    "# Extract all tables from staging\n",
    "data = extract_from_staging()\n",
    "print(f\"Extracted tables: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "21bb0c38-f907-470a-8242-80a13d02ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|office_id|object_id|      description|              region|            address1|  address2|          city|  zip_code|state_code|country_code| latitude|  longitude|         created_at|         updated_at|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "|        8|      c:8|                 |              SF Bay|959 Skyway Road, ...|          |    San Carlos|     94070|        CA|         USA|37.506885|-122.247573|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|        9|      c:9|     Headquarters|         Los Angeles|9229 W. Sunset Blvd.|          |West Hollywood|     90069|        CA|         USA|34.090368|-118.393064|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       10|     c:10|                 |         Los Angeles|8536 National Blv...|          |   Culver City|     90232|        CA|         USA|34.025958|-118.379768|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       11|     c:11|                 |         Los Angeles|      407 N Maple Dr|          | Beverly Hills|     90210|        CA|         USA|34.076179|-118.394170|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       12|     c:12|                 |              SF Bay|     1355 Market St.|          | San Francisco|     94103|        CA|         USA|37.776805|-122.416924|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       14|     c:14|                 |              SF Bay|                    |          |    Menlo Park|          |        CA|         USA|37.484130|-122.169472|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       15|     c:15|               HQ|              SF Bay|   539 Bryant Street|          | San Francisco|     94107|        CA|         USA|37.789634|-122.404052|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       16|     c:16|                 |           San Diego|16935 W. Bernardo...|          |     San Diego|     92127|        CA|         USA|33.022176|-117.081406|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       17|     c:18|Lala Headquarters|              SF Bay|    209 Hamilton Ave|Suite #200|     Palo Alto|     94301|        CA|         USA|37.451151|-122.154369|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       18|     c:19|                 |         Los Angeles|10960 Wilshire Blvd.| Suite 700|   Los Angeles|     90024|        CA|         USA|34.057498|-118.446596|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       19|     c:20|     Headquarters|              SF Bay|2145 Hamilton Avenue|          |      San Jose|     95125|        CA|         USA|37.295005|-121.930035|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       20|     c:21|                 |United States - O...|                    |          |              |          |          |         USA|37.090240| -95.712891|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       21|     c:22|                 |            New York|                    |          | New York City|          |        NY|         USA|40.757929| -73.985506|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       22|     c:23|                 |            New York|    100 5th Ave Fl 6|          |      New York|10011-6903|        NY|         USA|40.746497| -74.009447|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       23|     c:24|                 |  California - Other|                    |          |              |          |        CA|         USA|37.269175|-119.306607|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       24|     c:25|               HQ|            New York|       1515 Broadway|          |      New York|     10036|        NY|         USA|40.757725| -73.986011|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       25|     c:26|                 |              London|                    |          |        London|          |          |         GBR|53.344104|  -6.267494|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       26|     c:27|               HQ|              SF Bay|1050 Enterprise W...|          |     Sunnyvale|     94089|        CA|         USA|37.387845|-122.055197|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       27|     c:28|     Headquarters|              SF Bay| 170 West Tasman Dr.|          |      San Jose|     95134|        CA|         USA|37.408802|-121.953770|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "|       28|     c:29|                 |              SF Bay|    701 First Avenue|          |     Sunnyvale|     94089|        CA|         USA|37.418531|-122.025485|2007-01-01 22:19:54|2007-01-01 22:19:54|\n",
      "+---------+---------+-----------------+--------------------+--------------------+----------+--------------+----------+----------+------------+---------+-----------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read All data from staging\n",
    "acquisition = data[\"acquisition\"]\n",
    "company = data[\"company\"]\n",
    "funding_rounds = data[\"funding_rounds\"]\n",
    "funds = data[\"funds\"]\n",
    "investments = data[\"investments\"]\n",
    "ipos = data[\"ipo\"]\n",
    "milestones = data[\"milestones\"]\n",
    "people = data[\"people\"]\n",
    "relationship = data[\"relationship\"]\n",
    "\n",
    "# check\n",
    "company.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d6181-6ec3-4c5d-8dd7-83623e0ebd1a",
   "metadata": {},
   "source": [
    "## Data Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845db9a-96db-420a-b2e0-cbdbd30533a9",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0e38fdd-8ff5-4ca5-9d28-385f85826723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "from decimal import Decimal\n",
    "\n",
    "# Helper function buat konversi tipe data ke JSON-compatible\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        return obj.isoformat()\n",
    "    return obj\n",
    "\n",
    "def profile_data(person, df, table_name, format_file):\n",
    "    try:\n",
    "        n_rows = df.count()\n",
    "        n_cols = len(df.columns)\n",
    "        \n",
    "        column_info = {}\n",
    "        for col in df.columns:\n",
    "            data_type = df.schema[col].dataType.simpleString()\n",
    "            sample_values = df.select(col).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "            null_count = df.filter(df[col].isNull()).count()\n",
    "            unique_count = df.select(col).distinct().count()\n",
    "            \n",
    "            # Min and max values (if numeric or date type)\n",
    "            try:\n",
    "                min_value = df.agg({col: \"min\"}).collect()[0][0]\n",
    "                max_value = df.agg({col: \"max\"}).collect()[0][0]\n",
    "            except:\n",
    "                min_value = None\n",
    "                max_value = None\n",
    "            \n",
    "            # Persentase missing value\n",
    "            percentage_missing = round((null_count / n_rows) * 100, 2) if n_rows > 0 else 0.0\n",
    "            \n",
    "            # Ambil 5 nilai unik sebagai sampel\n",
    "            unique_values = df.select(col).distinct().limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "            \n",
    "            # Persentase valid date (khusus untuk tipe date dan datetime)\n",
    "            percentage_valid_date = None\n",
    "            if data_type in ['date', 'timestamp']:\n",
    "                valid_date_count = df.filter(df[col].isNotNull()).count()\n",
    "                percentage_valid_date = round((valid_date_count / n_rows) * 100, 2) if n_rows > 0 else 0.0\n",
    "\n",
    "            column_info[col] = {\n",
    "                \"data_type\": data_type,\n",
    "                \"sample_values\": [convert_to_serializable(v) for v in sample_values] if sample_values else None,\n",
    "                \"unique_count\": unique_count,\n",
    "                \"unique_value\": [convert_to_serializable(v) for v in unique_values] if unique_values else None,\n",
    "                \"null_count\": null_count,\n",
    "                \"percentage_missing_value\": percentage_missing,\n",
    "                \"min_value\": convert_to_serializable(min_value),\n",
    "                \"max_value\": convert_to_serializable(max_value),\n",
    "                \"percentage_valid_date\": percentage_valid_date\n",
    "            }\n",
    "        \n",
    "        dict_profiling = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"person_in_charge\": person,\n",
    "            \"profiling_result\": {\n",
    "                \"table_name\": table_name,\n",
    "                \"format_file\": format_file,\n",
    "                \"n_rows\": n_rows,\n",
    "                \"n_cols\": n_cols,\n",
    "                \"report\": column_info\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save profiling result to JSON\n",
    "        folder_path = \"data_profiling\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        file_path = os.path.join(folder_path, f\"{table_name}_profiling.json\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(dict_profiling, f, indent=4, default=convert_to_serializable)\n",
    "\n",
    "        print(f\"Profiling saved to: {file_path}\")\n",
    "\n",
    "        # Create success log message\n",
    "        log_msg = {\n",
    "            \"step\": \"Profiling\",\n",
    "            \"status\": \"Success\",\n",
    "            \"source\": format_file,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error profiling table {table_name}: {e}\")\n",
    "\n",
    "        # Create fail log message\n",
    "        log_msg = {\n",
    "            \"step\": \"Profiling\",\n",
    "            \"status\": f\"Failed: {e}\",\n",
    "            \"source\": format_file,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        # Save log to CSV\n",
    "        log_to_csv(log_msg, \"etl_log.csv\")\n",
    "\n",
    "    return dict_profiling if 'dict_profiling' in locals() else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc1ec5e0-6c82-4871-9bd0-1e89098e1663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling saved to: data_profiling/people_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "{\n",
      "  \"created_at\": \"2025-03-22T09:50:39.336805\",\n",
      "  \"person_in_charge\": \"Mr. A\",\n",
      "  \"profiling_result\": {\n",
      "    \"table_name\": \"people_data\",\n",
      "    \"format_file\": \"from Staging\",\n",
      "    \"n_rows\": 226709,\n",
      "    \"n_cols\": 6,\n",
      "    \"report\": {\n",
      "      \"people_id\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"177264\",\n",
      "          \"296\",\n",
      "          \"91421\",\n",
      "          \"467\",\n",
      "          \"177595\"\n",
      "        ],\n",
      "        \"unique_count\": 226709,\n",
      "        \"unique_value\": [\n",
      "          \"177264\",\n",
      "          \"296\",\n",
      "          \"91421\",\n",
      "          \"467\",\n",
      "          \"177595\"\n",
      "        ],\n",
      "        \"null_count\": 0,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"1\",\n",
      "        \"max_value\": \"99999\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"object_id\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"p:105829\",\n",
      "          \"p:73\",\n",
      "          \"p:171\",\n",
      "          \"p:214\",\n",
      "          \"p:214744\"\n",
      "        ],\n",
      "        \"unique_count\": 226709,\n",
      "        \"unique_value\": [\n",
      "          \"p:105829\",\n",
      "          \"p:73\",\n",
      "          \"p:171\",\n",
      "          \"p:214\",\n",
      "          \"p:214744\"\n",
      "        ],\n",
      "        \"null_count\": 0,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"p:10\",\n",
      "        \"max_value\": \"p:99999\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"first_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Hideki\",\n",
      "          \"Tyler\",\n",
      "          \"Nell\",\n",
      "          \"K\",\n",
      "          \"Rony\"\n",
      "        ],\n",
      "        \"unique_count\": 28423,\n",
      "        \"unique_value\": [\n",
      "          \"Hideki\",\n",
      "          \"Tyler\",\n",
      "          \"Nell\",\n",
      "          \"K\",\n",
      "          \"Rony\"\n",
      "        ],\n",
      "        \"null_count\": 5,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"\\\"\\\"\\\"DJ\\\"\",\n",
      "        \"max_value\": \"\\u00de\\u00f3rlindur\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"last_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Broukhim\",\n",
      "          \"Francois\",\n",
      "          \"Nerst\",\n",
      "          \"Banerjee\",\n",
      "          \"Cancel\"\n",
      "        ],\n",
      "        \"unique_count\": 107774,\n",
      "        \"unique_value\": [\n",
      "          \"Broukhim\",\n",
      "          \"Francois\",\n",
      "          \"Nerst\",\n",
      "          \"Banerjee\",\n",
      "          \"Cancel\"\n",
      "        ],\n",
      "        \"null_count\": 1,\n",
      "        \"percentage_missing_value\": 0.0,\n",
      "        \"min_value\": \"!llmind\",\n",
      "        \"max_value\": \"\\u00fcz\\u00fcm\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"birthplace\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Gainesville, FL\",\n",
      "          \"Bangalore\",\n",
      "          \"jacksonville , FL\",\n",
      "          \"Brisbane, Australia\",\n",
      "          \"North Adams, MA\"\n",
      "        ],\n",
      "        \"unique_count\": 8273,\n",
      "        \"unique_value\": [\n",
      "          \"Gainesville, FL\",\n",
      "          \"Bangalore\",\n",
      "          \"jacksonville , FL\",\n",
      "          \"Brisbane, Australia\",\n",
      "          \"North Adams, MA\"\n",
      "        ],\n",
      "        \"null_count\": 198622,\n",
      "        \"percentage_missing_value\": 87.61,\n",
      "        \"min_value\": \" Jr.\\\"\",\n",
      "        \"max_value\": \"\\u00e4\\u00b8\\u00ad\\u00e5\\u009b\\u00bd\",\n",
      "        \"percentage_valid_date\": null\n",
      "      },\n",
      "      \"affiliation_name\": {\n",
      "        \"data_type\": \"string\",\n",
      "        \"sample_values\": [\n",
      "          \"Blue Nile\",\n",
      "          \"Pandora Media\",\n",
      "          \"YouSendIt\",\n",
      "          \"Diet TV\",\n",
      "          \"SomethingSimpler\"\n",
      "        ],\n",
      "        \"unique_count\": 27250,\n",
      "        \"unique_value\": [\n",
      "          \"Blue Nile\",\n",
      "          \"Pandora Media\",\n",
      "          \"YouSendIt\",\n",
      "          \"Diet TV\",\n",
      "          \"SomethingSimpler\"\n",
      "        ],\n",
      "        \"null_count\": 21,\n",
      "        \"percentage_missing_value\": 0.01,\n",
      "        \"min_value\": \"! Haz Life\",\n",
      "        \"max_value\": \"\\u00d6resundswebb\",\n",
      "        \"percentage_valid_date\": null\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "profiling_result = profile_data(\"Mr. A\", people, \"people_data\", \"from Staging\")\n",
    "print(json.dumps(profiling_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc775e70-e9da-46d1-8042-68d886be2794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling saved to: data_profiling/relationship_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/acquisition_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/company_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/funding_rounds_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/funds_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/investments_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/ipos_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n",
      "Profiling saved to: data_profiling/milestones_data_profiling.json\n",
      "Log written to /home/jovyan/work/logs/etl_log.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-03-22T09:53:41.820808',\n",
       " 'person_in_charge': 'Mrs. OP',\n",
       " 'profiling_result': {'table_name': 'milestones_data',\n",
       "  'format_file': 'from Staging',\n",
       "  'n_rows': 8152,\n",
       "  'n_cols': 9,\n",
       "  'report': {'created_at': {'data_type': 'string',\n",
       "    'sample_values': ['2010-09-30 04:46:05.000',\n",
       "     '2010-10-04 23:53:31.000',\n",
       "     '2010-05-26 23:08:38.000',\n",
       "     '2010-07-09 05:01:33.000',\n",
       "     '2010-07-10 12:53:28.000'],\n",
       "    'unique_count': 7504,\n",
       "    'unique_value': ['2010-09-30 04:46:05.000',\n",
       "     '2010-10-04 23:53:31.000',\n",
       "     '2010-05-26 23:08:38.000',\n",
       "     '2010-07-09 05:01:33.000',\n",
       "     '2010-07-10 12:53:28.000'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-06-18 08:14:06.000',\n",
       "    'max_value': '2013-12-10 20:15:30.000',\n",
       "    'percentage_valid_date': None},\n",
       "   'description': {'data_type': 'string',\n",
       "    'sample_values': [\"Viewfinity named in 'Hottest Boston Companies' List\",\n",
       "     'Centralway invested in \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n LLC \\x97 the largest US-based mail order portal in the Czech Republic.',\n",
       "     'Eyeblaster files for IPO',\n",
       "     'Arsago Launches UCITS LatAm Fund.',\n",
       "     'Sonicbids launches new venue database.'],\n",
       "    'unique_count': 8012,\n",
       "    'unique_value': [\"Viewfinity named in 'Hottest Boston Companies' List\",\n",
       "     'Centralway invested in \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n LLC \\x97 the largest US-based mail order portal in the Czech Republic.',\n",
       "     'Eyeblaster files for IPO',\n",
       "     'Arsago Launches UCITS LatAm Fund.',\n",
       "     'Sonicbids launches new venue database.'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompleted the raise of StarVest Partners II, a $245 million fund in January 2009',\n",
       "    'max_value': 'â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£ â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_at': {'data_type': 'string',\n",
       "    'sample_values': ['2009-12-04',\n",
       "     '2009-06-23',\n",
       "     '2010-09-24',\n",
       "     '2010-02-12',\n",
       "     '2008-12-03'],\n",
       "    'unique_count': 924,\n",
       "    'unique_value': ['2009-12-04',\n",
       "     '2009-06-23',\n",
       "     '2010-09-24',\n",
       "     '2010-02-12',\n",
       "     '2008-12-03'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-01-01',\n",
       "    'max_value': '2010-12-31',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_code': {'data_type': 'string',\n",
       "    'sample_values': ['other'],\n",
       "    'unique_count': 1,\n",
       "    'unique_value': ['other'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'other',\n",
       "    'max_value': 'other',\n",
       "    'percentage_valid_date': None},\n",
       "   'milestone_id': {'data_type': 'bigint',\n",
       "    'sample_values': [2453, 5556, 4823, 2509, 2529],\n",
       "    'unique_count': 8152,\n",
       "    'unique_value': [2453, 5556, 4823, 2509, 2529],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 1,\n",
       "    'max_value': 39356,\n",
       "    'percentage_valid_date': None},\n",
       "   'object_id': {'data_type': 'string',\n",
       "    'sample_values': ['c:41922', 'c:58230', 'c:23012', 'c:57411', 'c:44410'],\n",
       "    'unique_count': 4065,\n",
       "    'unique_value': ['c:41922', 'c:58230', 'c:23012', 'c:57411', 'c:44410'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'c:1',\n",
       "    'max_value': 'f:9985',\n",
       "    'percentage_valid_date': None},\n",
       "   'source_description': {'data_type': 'string',\n",
       "    'sample_values': ['Zedira Raises Additional Funding',\n",
       "     'Muecs Ltd. Launches an Online Collaboration Tool for Enterprises With a Social Media Analyzer',\n",
       "     'Mugasha Launches Online Electronic Music Service',\n",
       "     'Datapipe Named Growth Company of the Year',\n",
       "     'Towerstream Launches Wireless Broadband Network in Philadelphia, PA'],\n",
       "    'unique_count': 5452,\n",
       "    'unique_value': ['Zedira Raises Additional Funding',\n",
       "     'Muecs Ltd. Launches an Online Collaboration Tool for Enterprises With a Social Media Analyzer',\n",
       "     'Mugasha Launches Online Electronic Music Service',\n",
       "     'Datapipe Named Growth Company of the Year',\n",
       "     'Towerstream Launches Wireless Broadband Network in Philadelphia, PA'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '\\t  After Whirlwind Year, Everyday Health To Tap Public Markets For $100M',\n",
       "    'max_value': 'â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£ â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90£â\\x90',\n",
       "    'percentage_valid_date': None},\n",
       "   'source_url': {'data_type': 'string',\n",
       "    'sample_values': ['http://techcrunch.com/2010/07/10/google-secretly-invested-100-million-in-zynga-preparing-to-launch-google-games/',\n",
       "     'http://www.tokyocamp.net/',\n",
       "     'http://www.pr.com/press-release/173572',\n",
       "     'http://blog.zooners.com/',\n",
       "     'http://paidcontent.org/article/419-ipg-shifts-initiatives-breen-to-reprise-media-as-global-ceo/'],\n",
       "    'unique_count': 6064,\n",
       "    'unique_value': ['http://techcrunch.com/2010/07/10/google-secretly-invested-100-million-in-zynga-preparing-to-launch-google-games/',\n",
       "     'http://www.tokyocamp.net/',\n",
       "     'http://www.pr.com/press-release/173572',\n",
       "     'http://blog.zooners.com/',\n",
       "     'http://paidcontent.org/article/419-ipg-shifts-initiatives-breen-to-reprise-media-as-global-ceo/'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': 'NaN',\n",
       "    'max_value': 'www://connectwithcoaches.com/',\n",
       "    'percentage_valid_date': None},\n",
       "   'updated_at': {'data_type': 'string',\n",
       "    'sample_values': ['2010-05-31 01:20:43.000',\n",
       "     '2010-07-10 12:53:28.000',\n",
       "     '2010-07-12 02:15:04.000',\n",
       "     '2010-03-10 18:47:20.000',\n",
       "     '2010-10-12 21:16:38.000'],\n",
       "    'unique_count': 7263,\n",
       "    'unique_value': ['2010-05-31 01:20:43.000',\n",
       "     '2010-07-10 12:53:28.000',\n",
       "     '2010-07-12 02:15:04.000',\n",
       "     '2010-03-10 18:47:20.000',\n",
       "     '2010-10-12 21:16:38.000'],\n",
       "    'null_count': 0,\n",
       "    'percentage_missing_value': 0.0,\n",
       "    'min_value': '2008-06-18 08:14:06.000',\n",
       "    'max_value': '2013-12-11 03:24:18.000',\n",
       "    'percentage_valid_date': None}}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_data(\"Mr. CCC\", relationship, \"relationship_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. H\", acquisition, \"acquisition_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. OP\", company, \"company_data\", \"from Staging\")\n",
    "profile_data(\"Mr. CCC\", funding_rounds, \"funding_rounds_data\", \"from Staging\")\n",
    "profile_data(\"Mr. A\", funds, \"funds_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. H\", investments, \"investments_data\", \"from Staging\")\n",
    "profile_data(\"Mr. A\", ipos, \"ipos_data\", \"from Staging\")\n",
    "profile_data(\"Mrs. OP\", milestones, \"milestones_data\", \"from Staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7512f9-be97-46e7-8796-34b7b258906f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
